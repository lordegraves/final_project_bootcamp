# Create a variable and set it as a list
info_list = ["Samar", 25, "Kyra", 20]
print(info_list)

# Methods for accessing parts of a list

# Return the value of a list at a given index
print(info_list[0])
print(info_list[-1])

# Return the index of the first object with a matching value
print(info_list.index("Kyra"))

# Return a list slice [index_start:index_end]
print(info_list[2:4])
print(info_list[1:])
print(info_list[:2])

# Methods for modifying a list

# Add an element onto the end of a list
info_list.append("Kamau")
print(info_list)

# Change a specified element within a list at the given index
info_list[3] = 85
print(info_list)

# Remove a specified object from a list
info_list.remove("Kamau")
print(info_list)

# Remove the object at the index specified
info_list.pop(0)
info_list.pop(0)
print(info_list)

# Functions for accessing information about a list
# Define a list named scores
scores = [92, 87, 68, 75, 96]

# Return the max (or highest value) item in a list
print(f"Max score: {max(scores)}")

# Return the min (or lowest) item in a list
print(f"Min score: {min(scores)}")

# Return the sum of the items in a list
print(f"Sum score: {sum(scores)}")

# Return the length of the list
print(f"Length of score list: {len(scores)}")

# Use sum and len to calculate average
average_score = sum(scores) / len(scores)
print(f"Average score: {average_score}")

# Create a tuple, a sequence of immutable Python objects that cannot be changed
info_tuple = ("Python", 100, 4.65, False)
print(info_tuple)

# Information functions also work on tuples, provided they contain valid data
# types
names_tuple = ("Melanie", "Jacinta", "Yindi", "Li")

print(len(names_tuple))
print(max(names_tuple))
print(min(names_tuple))

guests_tuple = (3, 5, 2, 4, 3)
print(sum(guests_tuple))


# Create a variable and set it as a list


# Methods for accessing parts of a list

# Return the value of a list at a given index


# Return the index of the first object with a matching value


# Return a list slice [index_start:index_end]


# Methods for modifying a list

# Add an element onto the end of a list


# Change a specified element within a list at the given index


# Remove a specified object from a list


# Remove the object at the index specified


# Functions for accessing information about a list
# Define a list named scores
scores = [92, 87, 68, 75, 96]

# Return the max (or highest value) item in a list


# Return the min (or lowest) item in a list


# Return the sum of the items in a list


# Return the length of the list


# Use sum and len to calculate average


# Create a tuple, a sequence of immutable Python objects that cannot be changed


# Information functions also work on tuples, provided they contain valid data
# types


# Create a tuple containing the names of menu sections:
# snacks, meals, drinks, and dessert.
menu_sections = ("snacks", "meals", "drinks", "dessert")

# Print the tuple.
print(menu_sections)

# Create a list of items for one of the menu sections.
desserts = ["Black Forest Cake", "Pavlova", "Lemon Meringue Pie", "Cherry Tart"]

# Create a list of prices for each of the menu items in the previous list.
dessert_prices = [4.99, 6.99, 4.25, 3.50]

# Ask a user to input a new item and append it to the relevant list.
desserts.append(input("What dessert would you like to add? "))

# Ask a user to input the price of the new item, referenced using list indexing
# and append it to the relevant list.
dessert_prices.append(float(input(f"How much does {desserts[-1]} cost? ")))

# Print the menu and prices.
print(desserts)
print(dessert_prices)

# Ask the user which item to remove from the menu.
remove_item = input("Which item do you want to remove? ")

# Find the index of the item and save it as a variable.
index = desserts.index(remove_item)

# Remove the item from the menu list using remove().
desserts.remove(remove_item)

# Remove the item from the prices list using pop().
dessert_prices.pop(index)

# Print the menu and prices again to confirm removal.
print(desserts)
print(dessert_prices)

# Find the most expensive price on the menu.
print(f"Most expensive: {max(dessert_prices)}")

# Find the cheapest price on the menu.
print(f"Cheapest: {min(dessert_prices)}")

# Find the cost if someone bought every item on the menu.
print(f"Total cost: {sum(dessert_prices)}")

# Confirm that the menu and prices lists are the same length.
print(
    f"The menu length is {len(desserts)} and the prices length is "
    + str(len(dessert_prices))
)


# Create a tuple containing the names of menu sections:
# snacks, meals, drinks, and dessert.


# Print the tuple.


# Create a list of items for one of the menu sections.


# Create a list of prices for each of the menu items in the previous list.


# Ask a user to input a new item and append it to the relevant list.


# Ask a user to input the price of the new item, referenced using list indexing
# and append it to the relevant list.


# Print the menu and prices.


# Ask the user which item to remove from the menu.


# Find the index of the item and save it as a variable.


# Remove the item from the menu list using remove().


# Remove the item from the prices list using pop().


# Print the menu and prices again to confirm removal.


# Find the most expensive price on the menu.


# Find the cheapest price on the menu.


# Find the cost if someone bought every item on the menu.


# Confirm that the menu and prices lists are the same length.


# Declare variables
x = 1
y = 10

# Check if one value is equal to another
if x == 1:
    print("x is equal to 1")

# Check if one value is NOT equal to another
if y != 1:
    print("y is not equal to 1")

# Check if one value is less than another
if x < y:
    print("x is less than y")

# Check if one value is greater than another
if y > x:
    print("y is greater than x")

# Check if a value is greater than or equal to another
if x >= 1:
    print("x is greater than or equal to 1")

# Use a Boolean to check a condition
congratulations = True
#congratulations = False

if congratulations:
    print("Congratulations!")
else:
    print("How are you feeling?")

# Get an input from a user
user_input = input("What number would you like to check? ")

# Check if the user input is a number
if user_input.isdigit():
    number = int(user_input)
    print(f"Input '{number}' is a number!")
else:
    print(f"{user_input} is not a number.")

# Declare variables


# Check if one value is equal to another


# Check if one value is NOT equal to another


# Check if one value is less than another


# Check if one value is greater than another


# Check if a value is greater than or equal to another


# Use a Boolean to check a condition


# Get an input from a user


# Check if the user input is a number


# 1. oooo needs some work
x = 5
if 2 * x > 10:
    print("Question 1 works!")
else:
    print("oooo needs some work")

# 2. Question 2 works!
x = 5
if len("Dog") < x:
    print("Question 2 works!")
else:
    print("Still missing out")

# 3. GOT QUESTION 3!
x = 2
y = 5
if (x ** 3 >= y):
    print("GOT QUESTION 3!")
else:
    print("This one didn't work")

# 4. Madagascar is in Africa
country = "Madagascar"
if country == "Madagascar":
    print(f"{country} is in Africa")
else:
    print(f"{country} is not in Africa")

# 5. You're going places!
going_places = True
if going_places:
    print("You're going places!")
else:
    print("You prefer to stay at home.")

# 6. 66,000 cannot be converted to a number
altitude = "66,000"
if altitude.isdigit():
    print(f"The plane flew at {altitude} feet")
else:
    print(f"{altitude} cannot be converted to a number")


# 1. 
x = 5
if 2 * x > 10:
    print("Question 1 works!")
else:
    print("oooo needs some work")

# 2. 
x = 5
if len("Dog") < x:
    print("Question 2 works!")
else:
    print("Still missing out")

# 3. 
x = 2
y = 5
if (x ** 3 >= y):
    print("GOT QUESTION 3!")
else:
    print("This one didn't work")

# 4. 
country = "Madagascar"
if country == "Madagascar":
    print(f"{country} is in Africa")
else:
    print(f"{country} is not in Africa")

# 5. 
going_places = True
if going_places:
    print("You're going places!")
else:
    print("You prefer to stay at home.")

# 6. 
altitude = "66,000"
if altitude.isdigit():
    print(f"The plane flew at {altitude} feet")
else:
    print(f"{altitude} cannot be converted to a number")


# Declare variables
x = 1
y = 10

# Logical operators: "and" and "or"

# Check for two conditions to be met using "and"
if x == 1 and y == 10:
    print("Both values returned true")

# Check if either of two conditions is met using "or"
if x < 45 or y < 5:
    print("One or more of the statements were true")

# Check if a condition is not true
    if not (x > y):
        print("x is not greater than y")

# Check multiple conditions
plate = "fancy"
if plate == "cracked":
    print("Throw the dish away")
elif plate == "fancy":
    print("Put the plate on the top shelf")
else:
    print("Put the plate on the bottom shelf")

# Conditionals with membership operators: "in" and "not in"

# Check if a variable is in a list
name = "Amidah"
group_one = ["Jorge", "Joon", "Susan"]
group_two = ["Gerald", "Paola", "Ryder"]
group_three = ["Farah", "Amidah", "Koen"]

if name in group_one:
    print(name + " is in the first group")
elif name in group_two:
    print(name + " is in group two")
elif name in group_three:
    print(name + " is in group three")
else:
    print(name + " does not have a group")

# Check if a variable is not in a list
countries = ["Fiji", "Australia", "New Zealand", "Papua New Guinea", "Palau"
             "Solomon Islands", "Micronesia", "Vanuatu", "Samoa", "Kiribati",
             "Tonga", "Marshall Islands", "Tuvalu", "Nauru"]
country = "Kenya"

if country not in countries:
    print (country + " is not in Oceania")

# Conditionals with identity operators: "is" and "is not"

# Check if a variable is a list
if type(countries) is list:
    print("countries is a list")

# Check if a variable is not a list
if country is not list:
    print("country is not a list")

# Check if a variable is a float or integer
if type(x) is float:
    print("x is a float")
elif type(x) is int:
    print("x is an integer")
else:
    print("x is not a number")

# Check multiple conditions with comparison and logical operators
height = 66
age = 16
adult_permission = True

if (height > 70) and (age >= 18):
    print("Can ride all the roller coasters")
elif (height > 65) and (age >= 18):
    print("Can ride moderate roller coasters")
elif (height > 60) and (age >= 18):
    print("Can ride light roller coasters")
elif ((height > 50) and (age >= 18)) or ((adult_permission) and (height > 50)):
    print("Can ride bumper cars")
else:
    print("Stick to lazy river")

# Declare variables
x = 1
y = 10

# Logical operators: "and" and "or"

# Check for two conditions to be met using "and"


# Check if either of two conditions is met using "or"


# Check if a condition is not true


# Check multiple conditions


# Conditionals with membership operators: "in" and "not in"

# Check if a variable is in a list
name = "Amidah"
group_one = ["Jorge", "Joon", "Susan"]
group_two = ["Gerald", "Paola", "Ryder"]
group_three = ["Farah", "Amidah", "Koen"]



# Check if a variable is not in a list
countries = ["Fiji", "Australia", "New Zealand", "Papua New Guinea", "Palau"
             "Solomon Islands", "Micronesia", "Vanuatu", "Samoa", "Kiribati",
             "Tonga", "Marshall Islands", "Tuvalu", "Nauru"]
country = "Kenya"



# Conditionals with identity operators: "is" and "is not"

# Check if a variable is a list


# Check if a variable is not a list


# Check if a variable is a float or integer


# Check multiple conditions with comparison and logical operators
height = 66
age = 16
adult_permission = True



# Declare variables
budget = 2000
cities = ["Rome", "Nairobi", "Phnom Penh", "Santiago", "Toronto", "Rotorua"]
cities_daily_cost = [150, 70, 60, 80, 110, 125]
days = input("How many days can you travel? ")
city_to_visit = input("What city would you like to visit? ")

# Check if days is a number, and convert it to an integer if it is
if days.isdigit():
    days = int(days)
# Else print an error
else:
    print("Input was not a number.")

# Check if budget and days are integers, and if so, calculate the daily budget
if type(days) is int and type(budget) is int:
    daily_budget = budget / days
    print(f"The daily budget is ${daily_budget}")
# Else print an error
else:
    print("There was an error. Unable to calculate daily budget.")

# Check if the city_to_visit is in the cities list
if city_to_visit in cities:
    # Get the daily cost for the city
    city_index = cities.index(city_to_visit)
    city_daily_cost = cities_daily_cost[city_index]
# Else set the city_daily_cost to 0 to be used for error checking
else:
    city_daily_cost = 0

# Check if the city_daily_cost is greater than 0 and equal to or less than
# the daily budget
if city_daily_cost > 0 and city_daily_cost <= daily_budget:
    # Tell the traveler they can afford the vacation
    print(
        f"You can afford the trip to {city_to_visit} for {days} days with "
        + f"your daily budget of ${daily_budget} because the daily cost is $"
        + str(city_daily_cost)
    )
# Else if the city_daily_cost is greater than 0 and greater than the daily
# budget
elif city_daily_cost > 0 and city_daily_cost > daily_budget:
    # Calculate and print out how much more per day the traveler needs
    need_daily = city_daily_cost - daily_budget
    print(
        f"You'll have to wait to travel when you can afford ${need_daily} "
        + "more per day."
    )
# Else print an error
else:
    print("City was not in list. We cannot make any recommendations.")


# Declare variables
budget = 2000
cities = ["Rome", "Nairobi", "Phnom Penh", "Santiago", "Toronto", "Rotorua"]
cities_daily_cost = [150, 70, 60, 80, 110, 125]
days = input("How many days can you travel? ")
city_to_visit = input("What city would you like to visit? ")

# Check if days is a number, and convert it to an integer if it is

# Else print an error


# Check if budget and days are integers, and if so, calculate the daily budget

# Else print an error


# Check if the city_to_visit is in the cities list, and if so,
# get the daily cost for the city

# Else set the city_daily_cost to 0 to be used for error checking


# Check if the city_daily_cost is greater than 0 and equal to or less than the
# daily budget, and if so, tell the traveler they can afford the vacation

# Else if the city_daily_cost is greater than 0 and greater than the daily budget,
# calculate and print out how much more per day the traveler needs

# Else print an error


# Loop through a range of numbers (0 through 4)
for x in range(5):
    print(x)

print("-" * 40)

# Loop through a range of numbers (2 through 6 - yes 6! Up to, but not
# including, 7)
for x in range(2, 7):
    print(x)

print("-" * 40)

# Loop through a range of numbers (0 through 4) without using the value in the
# range
y = 2
for _ in range(5):
    # Add 2 to the value of y
    y += 2
    print(y)

print("-" * 40)

# Iterate through letters in a string
word = "Peace"
for letter in word:
    print(letter)

print("-" * 40)

# Iterate through a list
zoo = ["cow", "dog", "bee", "zebra"]
for animal in zoo:
    print(animal.upper())

print("-" * 40)

# Make changes to each item in a list
numbers = [9, 6, 4, 9]
for i in range(len(numbers)):
    # Add 1 to the list item
    numbers[i] += 1
    print(numbers[i])

print("-" * 40)

# Loop while a condition is being met
run = "y"

# run.lower() means the condition will be met if run = "y" or run = "Y"
while run.lower() == "y":
    print("Hi!")
    run = input("To run again. Enter 'y': ")

# While loop with a Boolean variable
keep_looping = True

while keep_looping:
    print("Hi!")
    keep_looping = bool(input("Press enter to exit and anything else to continue. "))


# Loop through a range of numbers (0 through 4)


print("-" * 40)

# Loop through a range of numbers (2 through 6 - yes 6! Up to, but not
# including, 7)


print("-" * 40)

# Loop through a range of numbers (0 through 4) without using the value in the
# range

    # Add 2 to the value of y


print("-" * 40)

# Iterate through letters in a string


print("-" * 40)

# Iterate through a list


print("-" * 40)

# Make changes to each item in a list


print("-" * 40)

# Loop while a condition is being met
run = "y"

# run.lower() means the condition will be met if run = "y" or run = "Y"


# While loop with a Boolean variable


# Declare lists
cities = ["Rome", "Nairobi", "Phnom Penh", "Santiago", "Toronto", "Rotorua"]
cities_daily_cost = [150, 70, 60, 80, 110, 125]

# Loop 3 times through the cities list
for _ in range(3):
    # Ask the user for a city and convert it to title case
    city = input("What city should be added to the list? ").title()
    # Append the city to the cities list
    cities.append(city)

# Use a while loop to append to the cities_daily_cost list as long as that list
# is shorter than the cities list
while len(cities_daily_cost) < len(cities):
    # Use the length of cities_daily_cost for the index of cities to print its
    # value when asking the cost to append to the cities_daily_cost list
    index = len(cities_daily_cost)
    cost = int(input(f"What is the daily cost for {cities[index]} "))
    cities_daily_cost.append(cost)

# Loop through the cities_daily_cost list and add 10 to each item
for i in range(len(cities_daily_cost)):
    cities_daily_cost[i] += 10

# Use a for loop to loop through both of the lists at the same time, since
# they're the same length, and print out the city and daily cost on the same
# line
for i in range(len(cities)):
    print(f"Daily cost of {cities[i]} is ${cities_daily_cost[i]}")


# Declare lists
cities = ["Rome", "Nairobi", "Phnom Penh", "Santiago", "Toronto", "Rotorua"]
cities_daily_cost = [150, 70, 60, 80, 110, 125]

# Loop 3 times through the cities list

# Ask the user for a city and convert it to title case

# Append the city to the cities list


# Use a while loop to append to the cities_daily_cost list as long as that list
# is shorter than the cities list

# Use the length of cities_daily_cost for the index of cities to print its
# value when asking the cost to append to the cities_daily_cost list


# Loop through the cities_daily_cost list and add 10 to each item


# Use a for loop to loop through both of the lists at the same time, since
# they're the same length, and print out the city and daily cost on the same
# line


# Nested if-else statements

issue_currency = "EUR"
price = 30.0

# Check if price is not negative (greater than equal to 0)
if price >= 0:
    # If price is not negative and currency is 'USD' (Dollar).
    if issue_currency == "USD":
        print("The currency is $", price)
    # If price is not negative and currency is 'EUR' (Euro).
    elif issue_currency == "EUR":
        print("The currency is â‚¬", price)
    # If anything other than the above.
    else:
        print("The currency is not in USD or EUR.")
# Else price is negative
else:
    print("Error, the price listed is a negative number")


# Nested loops

# Keep looping until we exit the loop
while True:

    # Declare user_number
    user_number = ""

    # Keep looping while user_number is not an integer
    while type(user_number) is not int:
        # Ask the user how many numbers to loop through
        user_number = input("How many numbers? ")

        # Validate the input is a number
        if user_number.isdigit():
            user_number = int(user_number)
        else:
            print("You didn't input a valid number")

    # Loop through the numbers. (Be sure to cast the string into an integer.)
    for x in range(user_number):

        # Print each number in the range
        print(x)

        # Limit the range to 20
        if x >= 20:
            print("Your input number is too high, breaking loop.")
            break


    # Once complete, ask the user if they want to quit
    user_play = input("Type 'q' to exit and anything else to continue. ")
    if user_play == 'q':
        break


# Nested if-else statements

issue_currency = "EUR"
price = 30.0

# Check if price is not negative (greater than equal to 0)

    # If price is not negative and currency is 'USD' (Dollar).

    # If price is not negative and currency is 'EUR' (Euro).

    # If anything other than the above.


# Else price is negative.


# Nested loops

# Keep looping until we exit the loop


    # Declare user_number


    # Keep looping while user_number is not an integer

        # Ask the user how many numbers to loop through


        # Validate the input is a number


    # Loop through the numbers. (Be sure to cast the string into an integer.)


        # Print each number in the range


        # Limit the range to 20


    # Once complete, ask the user if they want to quit


# Declare lists
rides = []
prices = []

while True:
    # Keep adding rides until exiting loop
    rides.append(input("What ride should be added to the amusement park? "))

    # Continuous loop
    while True:
        # Prompt user for the cost of the ride
        cost = input("How much does it cost to ride this ride? ")

        # Check if the input is a number
        if cost.replace(".","",1).isdigit():
            # Check if the input is less than or equal to 15
            if float(cost) <= 15:
                # Convert the input to a float and append it to the prices list
                prices.append(float(cost))
                # Exit the loop
                break
            else:
                print("Please choose a price that is $15 or less.")
        else:
            print("You didn't input a valid price")

    # Ask the user if they wish to quit, and break the loop if they type 'q'
    keep_going = input("Type 'q' to exit and anything else to add another ride. ")
    if keep_going == 'q':
        break

# Loop through the rides and prices lists by finding the length of rides
for i in range(len(rides)):
    # Set the discount variable to False
    discount = False
    # Check if the price is greater than $5
    if prices[i] > 5:
        # Update the price to include a 10% discount
        prices[i] *= 0.9
        # Set the discount variable to true
        discount = True
    # Print the ride name and price, with the price formatted to 2 decimal places
    print(f"{rides[i]} costs ${prices[i]:.2f} to ride.")
    # If a discount was applied, print a message that says this
    if discount:
        print("A discount of 10% is applied.")
        # Print a dash 40 times
    print("-" * 40)



# Declare lists
rides = []
prices = []

# Continuous loop

    # Keep adding rides until exiting loop


    # Continuous loop

        # Prompt user for the cost of the ride


        # Check if the input is a number

            # Check if the input is less than or equal to 15

                # Convert the input to a float and append it to the prices list

                # Exit the loop



    # Ask the user if they wish to quit, and break the loop if they type 'q'

# Loop through the rides and prices lists by finding the length of rides

    # Set the discount variable to False

    # Check if the price is greater than $5

        # Update the price to include a 10% discount

        # Set the discount variable to true

    # Print the ride name and price, with the price formatted to 2 decimal places

    # If a discount was applied, print a message that says this


    # Print a dash 40 times


# Incorporate the random library
import random

# Print Title
print("Let's Play Rock Paper Scissors!")

# Specify the three options
options = ["r", "p", "s"]



# Create a continuous loop so the user can play multiple rounds
while True:
    # User Selection
    user_choice = input("Make your Choice: (r)ock, (p)aper, (s)cissors? ")

    # Check if the user selected a valid choice from the options list
    if user_choice in options:
        # Generate the computer selection
        computer_choice = random.choice(options)

        # Create a variable called user_full_choice to hold the text of the 
        # full word for the user's choice by using a conditional
        if user_choice == 'r':
            user_full_choice = 'rock'
        elif user_choice == 'p':
            user_full_choice = 'paper'
        else:
            user_full_choice = 'scissors'

        # Run Conditionals

        # First check if there is a tie
        if user_choice == computer_choice:
            print(f"You both chose {user_full_choice}!")
            print("A smashing tie!")
        
        # Check if the user picked rock and computer picked paper
        elif (user_choice == "r" and computer_choice == "p"):
            print("You chose rock. The computer chose paper.")
            print("Sorry. You lose.")

        # Check if the user picked rock and computer picked scissors
        elif (user_choice == "r" and computer_choice == "s"):
            print("You chose rock. The computer chose scissors.")
            print("Yay! You won.")

        # Check if the user picked paper and computer picked scissors
        elif (user_choice == "p" and computer_choice == "s"):
            print("You chose paper. The computer chose scissors.")
            print("Sorry. You lose.")

        # Check if the user picked paper and computer picked rock
        elif (user_choice == "p" and computer_choice == "r"):
            print("You chose paper. The computer chose rock.")
            print("Yay! You won.")

        # Check if the user picked scissors and computer picked paper
        elif (user_choice == "s" and computer_choice == "p"):
            print("You chose scissors. The computer chose paper.")
            print("Yay! You won.")

        # Check if the user picked scissors and computer picked rock
        elif (user_choice == "s" and computer_choice == "r"):
            print("You chose scissors. The computer chose rock.")
            print("Sorry. You lose.")

        # Ask the user if they would like to play again and save the answer as 
        # a variable
        print("Would you like to play again?")
        play_again = input("Type (y) to continue or anything else to exit. ")

        # If the user's answer is not "y" or "Y", break from the loop
        if play_again.lower() != "y":
            break
    # Print an error if the user didn't select a valid choice
    else:
        print("I don't understand that!")
        print("Next time, choose from 'r', 'p', or 's'.")

# Say goodbye if the loop has been exited
print("Thank you for playing Rock Paper Scissors. See you next time!")



# Incorporate the random library
import random

# Print Title
print("Let's Play Rock Paper Scissors!")

# Specify the three options
options = ["r", "p", "s"]

# Create a continuous loop so the user can play multiple rounds
while True:
    
    # User Selection
    user_choice = input("Make your Choice: (r)ock, (p)aper, (s)cissors? ")
    # Check if the user selected a valid choice from the options list
    if user_choice in options:
        
        # Generate the computer selection
        computer_choice = random.choice(options)
        
        if user_choice=='r':
            user_full_choice='rock'
        elif user_choice=='p':
            user_full_choice='paper'
        else:
            user_full_choice='scissors'
            
        if user_choice==computer_choice:
            print(f"You both chose {user_full_choice}")
        elif (user_choice=='r' and computer_choice=='p'):
            print("You chose rock, the computer chose paper")
            print("sorry. you lose")
        elif (user_choice=='r' and computer_choice=='s'):
            print("You chose rock, the computer chose Scissors")
            print("woot, you win")  
        # Check if the user picked paper and computer picked rock
        elif (user_choice == "p" and computer_choice == "r"):
            print("You chose paper. The computer chose rock.")
            print("Yay! You won.")

        # Check if the user picked scissors and computer picked paper
        elif (user_choice == "s" and computer_choice == "p"):
            print("You chose scissors. The computer chose paper.")
            print("Yay! You won.")

        # Check if the user picked scissors and computer picked rock
        elif (user_choice == "s" and computer_choice == "r"):
            print("You chose scissors. The computer chose rock.")
            print("Sorry. You lose.")     
            
        print("would like to play again?") 
        play_again=input("Type (y) if yes")
        
        if play_again.lower()!='y':
            break
    else:
        print("I dont unserstand.  Next time choose r p or s")

        # Create a variable called user_full_choice to hold the text of the 
        # full word for the user's choice by using a conditional
        

        # Run Conditionals to determine the result

        # First check if there is a tie
        
            print(f"You both chose {user_full_choice}!")
            print("A smashing tie!")
        
        # Check if the user picked rock and computer picked paper
        
            print("You chose rock. The computer chose paper.")
            print("Sorry. You lose.")

        # Check if the user picked rock and computer picked scissors
        
            print("You chose rock. The computer chose scissors.")
            print("Yay! You won.")

        # Check if the user picked paper and computer picked scissors
        
            print("You chose paper. The computer chose scissors.")
            print("Sorry. You lose.")

        # Check if the user picked paper and computer picked rock
        
            print("You chose paper. The computer chose rock.")
            print("Yay! You won.")

        # Check if the user picked scissors and computer picked paper
        
            print("You chose scissors. The computer chose paper.")
            print("Yay! You won.")

        # Check if the user picked scissors and computer picked rock
        
            print("You chose scissors. The computer chose rock.")
            print("Sorry. You lose.")

        # Ask the user if they would like to play again and save the answer as 
        # a variable
        

        # If the user's answer is not "y" or "Y", break from the loop
        
    # Print an error if the user didn't select a valid choice
    else:
        print("I don't understand that!")
        print("Next time, choose from 'r', 'p', or 's'.")

# Say goodbye if the loop has been exited
print("Thank you for playing Rock Paper Scissors. See you next time!")

# Boolean to place the order
place_order = True

# List to track pie purchases
pie_purchases = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

# Pie List
pie_list = ["Pecan", 
            "Apple", 
            "Lemon Meringue", 
            "Banoffee", 
            "Black Bun",
            "Bean", 
            "Buko", 
            "Burek", 
            "Tamale", 
            "Steak"]

# Display initial message
print("Welcome to the House of Pies! Here are our pies:")

# While we are still shopping...
while place_order:

    # Show pie selection prompt
    print("-" * 50)
    pie_number = 1
    for pie in pie_list:
        print(f"({pie_number}) {pie}")
        pie_number += 1

    pie_choice = input("Which would you like? ")

    # Get index of the pie from the selected number
    choice_index = int(pie_choice) - 1

    # Add pie to the pie list by finding the matching index and adding one to its value
    pie_purchases[choice_index] += 1

    print("-" * 50)

    # Inform the customer of the pie purchase
    print("Great! We'll have that " + pie_list[choice_index] 
          + " Pie right out for you.")

    # Provide exit option
    while True:
		# Ask the customer if they would like to order anything else
        keep_ordering = input("Would you like to keep ordering? (Y)es or (N)o ")

        # Check the customer's input
        match keep_ordering.lower():
            # Customer chose yes
            case 'y':
                # Keep ordering
                place_order = True
                # Exit the keep ordering question loop
                break
            # Customer chose no
            case 'n':
                # Complete the order
                place_order = False
                # Since the customer decided to stop ordering, thank them for their order
                print("Thank you for your order.")
                # Exit the keep ordering question loop
                break
            # Customer typed an invalid input
            case _:
                # Tell the customer to try again
                print("I didn't understand your response. Please try again.")

# Once the pie list is complete
print("-" * 50)

# Count instances of each pie
print("You purchased: ")

# Loop through the full pie list
for pie_index in range(len(pie_list)):
    pie_count = pie_purchases[pie_index]
    pie_name = pie_list[pie_index]

    # If the pie count is greater than or equal to 1:
    if pie_count >= 1:
        # Gather the count of each pie in the pie list and print them alongside the pies
        print(f"{pie_count} {pie_name} Pie")

# Boolean to place the order
place_order = True

# List to track pie purchases
pie_purchases = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

# Pie List
pie_list = ["Pecan", 
            "Apple", 
            "Lemon Meringue", 
            "Banoffee", 
            "Black Bun",
            "Bean", 
            "Buko", 
            "Burek", 
            "Tamale", 
            "Steak"]

# Display initial message
print("Welcome to the House of Pies! Here are our pies:")

# While we are still shopping...
while place_order:

    # Show pie selection prompt
    print("-" * 50)
    pie_number = 1
    for pie in pie_list:
        print(f"({pie_number}) {pie}")
        pie_number += 1

    pie_choice = input("Which would you like? ")

    # Get index of the pie from the selected number
    choice_index = int(pie_choice) - 1

    # Add pie to the pie list by finding the matching index and adding one to its value
    pie_purchases[choice_index] += 1

    print("-" * 50)

    # Inform the customer of the pie purchase
    print("Great! We'll have that " + pie_list[choice_index] 
          + " Pie right out for you.")

    # Provide exit option
    while True:
		# Ask the customer if they would like to order anything else
        keep_ordering = input("Would you like to keep ordering? (Y)es or (N)o ")

        # Check the customer's input

            # Customer chose yes

                # Keep ordering

                # Exit the keep ordering question loop

            # Customer chose no

                # Complete the order

                # Since the customer decided to stop ordering, thank them for their order

                # Exit the keep ordering question loop

            # Customer typed an invalid input

                # Tell the customer to try again


# Once the pie list is complete
print("-" * 50)

# Count instances of each pie
print("You purchased: ")

# Loop through the full pie list
for pie_index in range(len(pie_list)):
    pie_count = pie_purchases[pie_index]
    pie_name = pie_list[pie_index]

    # If the pie count is greater than or equal to 1:
    if pie_count >= 1:
        # Gather the count of each pie in the pie list and print them alongside the pies
        print(f"{pie_count} {pie_name} Pie")


# Incorporate the random library
import random

# Print Title
print("Let's Play Rock Paper Scissors!")

# Specify the three options
options = ["r", "p", "s"]

# Create a continuous loop so the user can play multiple rounds
while True:
    # User Selection
    user_choice = input("Make your Choice: (r)ock, (p)aper, (s)cissors? ")

    # Check if the user selected a valid choice
    if user_choice in options:
        # Generate the computer selection
        computer_choice = random.choice(options)

        # Create a variable called user_full_choice to hold the text of the 
        # full word for the user's choice by using a match case statement
        match user_choice: 
            case 'r':
                user_full_choice = 'rock'
            case 'p':
                user_full_choice = 'paper'
            case _:
                user_full_choice = 'scissors'

        # Run Conditionals
        # First check if there is a tie
        if user_choice == computer_choice:
            print(f"You both chose {user_full_choice}!")
            print("A smashing tie!")
        else:
            # Tell the user what they chose
            print(f"You chose {user_full_choice}")

            # Refactor the conditionals into a nested match statement.
            # Check the user_choice first, then inside each case, check the 
            # computer_choice
            match user_choice:
                case "r":
                    match computer_choice:
                        case "p":
                            print("The computer chose paper.")
                            print("Sorry. You lose.")
                        case "s":
                            print("The computer chose scissors.")
                            print("Yay! You won.")
                case "p":
                    match computer_choice:
                        case "r":
                            print("The computer chose rock.")
                            print("Yay! You won.")
                        case "s":
                            print("The computer chose scissors.")
                            print("Sorry. You lose.")
                case "s":
                    match computer_choice:
                        case "p":
                            print("The computer chose paper.")
                            print("Yay! You won.")
                        case "r":
                            print("The computer chose rock.")
                            print("Sorry. You lose.")

        # Ask the user if they would like to play again and save the answer as 
        # a variable
        print("Would you like to play again?")
        play_again = input("Type (y) to continue or anything else to exit. ")

        # If the user's answer is not "y" or "Y", break from the loop
        if play_again.lower() != "y":
            break
    else:
        print("I don't understand that!")
        print("Next time, choose from 'r', 'p', or 's'.")

print("Thank you for playing Rock Paper Scissors. See you next time!")

# Incorporate the random library
import random

# Print Title
print("Let's Play Rock Paper Scissors!")

# Specify the three options
options = ["r", "p", "s"]

# Create a continuous loop so the user can play multiple rounds
while True:
    # User Selection
    user_choice = input("Make your Choice: (r)ock, (p)aper, (s)cissors? ")

    # Check if the user selected a valid choice
    if user_choice in options:
        # Generate the computer selection
        computer_choice = random.choice(options)

        # Create a variable called user_full_choice to hold the text of the 
        # full word for the user's choice by using a match case statement


        # Run Conditionals
        # First check if there is a tie
        if user_choice == computer_choice:
            print(f"You both chose {user_full_choice}!")
            print("A smashing tie!")
        else:
            # Tell the user what they chose
            print(f"You chose {user_full_choice}")

            # Refactor the conditionals into a nested match statement.
            # Check the user_choice first, then inside each case, check the 
            # computer_choice


        # Ask the user if they would like to play again and save the answer as 
        # a variable
        print("Would you like to play again?")
        play_again = input("Type (y) to continue or anything else to exit. ")

        # If the user's answer is not "y" or "Y", break from the loop
        if play_again.lower() != "y":
            break
    else:
        print("I don't understand that!")
        print("Next time, choose from 'r', 'p', or 's'.")

print("Thank you for playing Rock Paper Scissors. See you next time!")

# Unlike lists, dictionaries store information in pairs
# ---------------------------------------------------------------

# Create a dictionary to hold the actor's names.
actors = {}

# Create a dictionary using the built-in function.
actors = dict()

# A dictionary of an actor.
actors = {"name": "Tom Cruise"}
print(f'{actors["name"]}')

# Add an actor to the dictionary with the key "name"
# and the value "Denzel Washington".
actors["name"] = "Denzel Washington"

# Print the actors dictionary.
print(actors)

# Print only the actor.
print(f'{actors["name"]}')

# A list of actors
actors_list = [
    "Tom Cruise",
    "Angelina Jolie",
    "Kristen Stewart",
    "Denzel Washington"]

# Overwrite the value, "Denzel Washington", with the list of actors.
actors["name"] = actors_list

# Print the first actor
print(f'{actors["name"][0]}')

# ---------------------------------------------------------------

# A dictionary can contain multiple pairs of information
actress = {
    "name": "Angelina Jolie",
    "genre": "Action",
    "nationality": "United States"
}

# ---------------------------------------------------------------

# A dictionary can contain multiple types of information
another_actor = {
    "name": "Sylvester Stallone",
    "age": 62,
    "married": True,
    "best movies": [
        "Rocky",
        "Rocky 2",
        "Rocky 3"]}
print(f'{another_actor["name"]} was in {another_actor["best movies"][0]}')
# ---------------------------------------------------------------

# A dictionary can even contain another dictionary
film = {
    "title": "Interstellar",
    "revenues": {
        "United States": 360,
        "China": 250,
        "United Kingdom": 73
    }
}
print(f'{film["title"]} made {film["revenues"]["United States"]}'" million dollars in the US.")

# ---------------------------------------------------------------


# Create our film dictionary
film = {
    "title": "Everything Everywhere All At Once",
    "director": ["Daniel Kwan", "Daniel Scheinert"],
    "cast": [
        "Michelle Yeoh",
        "Ke Huy Quan",
        "Jamie Lee Curtis",
        "Stephanie Hsu",
        "James Hong",
        "Jenny Slate",
        "Harry Shum Jr.",
        "Tallie Medel"
    ],
    "distributor": "A24",
    "box_office_in_millions": {
        "us": 72.1,
        "uk": 6.2,
        "canada": 5.1,
        "australia": 4.5
    },
    "release_date": {
        "us": "April 8, 2022",
        "uk": "May 13, 2022",
        "canada": "March 25, 2022",
        "australia": "April 14, 2022"
    }
}

# If we don't know all of the keys in the dictionary that we
# may want to access, we can use keys() to list them for us
print(film.keys())

# We may also just want to access a dictionary's values

print(film.values())

# And we can access each item in the dictionary as a key-value pair
# as a list of tuples in the format [(key, value)]

print(film.items())

# ---------------------------------------------------------------

# Using these dictionary methods, we can iterate through different
# parts of the dictionary.

# There are two ways to loop through the keys
print("\nUsing for key in film:")
for key in film:
    print(key)

print("\nUsing for key in film.keys():")
for key in film.keys():
    print(key)

# Looping through a dictionary's values must use the values() method
print("\nfor value in film.values():")
for value in film.values():
    print(value)

# When looping through each item in a dictionary, we unpack the (key, value)
# tuple so we can use them separately
print("\nfor key, value in film.items():")
for key, value in film.items():
    print("-" * 50)
    print(f"Key: {key}\nValue: {value}" )


# Unlike lists, dictionaries store information in pairs
# ---------------------------------------------------------------

# Create a dictionary to hold the actor's names.


# Create a dictionary using the built-in function.


# A dictionary of an actor.


# Add an actor to the dictionary with the key "name"
# and the value "Denzel Washington".


# Print the actors dictionary.


# Print only the actor.


# A list of actors


# Overwrite the value, "Denzel Washington", with the list of actors.


# Print the first actor


# ---------------------------------------------------------------

# A dictionary can contain multiple pairs of information


# ---------------------------------------------------------------

# A dictionary can contain multiple types of information


# ---------------------------------------------------------------

# A dictionary can even contain another dictionary


# ---------------------------------------------------------------


# Create our film dictionary
film = {
    "title": "Everything Everywhere All At Once",
    "director": ["Daniel Kwan", "Daniel Scheinert"],
    "cast": [
        "Michelle Yeoh", 
        "Ke Huy Quan", 
        "Jamie Lee Curtis", 
        "Stephanie Hsu", 
        "James Hong",
        "Jenny Slate",
        "Harry Shum Jr.",
        "Tallie Medel"
    ],
    "distributor": "A24",
    "box_office_in_millions": {
        "us": 72.1,
        "uk": 6.2,
        "canada": 5.1,
        "australia": 4.5
    },
    "release_date": {
        "us": "April 8, 2022",
        "uk": "May 13, 2022",
        "canada": "March 25, 2022",
        "australia": "April 14, 2022"
    }
}

# If we don't know all of the keys in the dictionary that we
# may want to access, we can use keys() to list them for us


# We may also just want to access a dictionary's values


# And we can access each item in the dictionary as a key-value pair 
# as a list of tuples in the format [(key, value)]


# ---------------------------------------------------------------

# Using these dictionary methods, we can iterate through different 
# parts of the dictionary.

# There are two ways to loop through the keys


# Looping through a dictionary's values must use the values() method


# When looping through each item in a dictionary, we unpack the (key, value)
# tuple so we can use them separately


# Dictionary full of info
my_info = {"name": "Zendaya",
           "age": 26,
           "hobbies": ["philanthropy", 
                       "playing with animals", 
                       "fashion", 
                       "social media"],
           "wake-up": {
               "Monday": 5, 
               "Friday": 5, 
               "Saturday": 10, 
               "Sunday": 9
            }
}

# Print out results stored in the dictionary
print(f'Hello I am {my_info["name"]} and I am {my_info["age"]} years old.')
print(f'My first hobby is {my_info["hobbies"][0]}')
print(f'On the weekend I get up at {my_info["wake-up"]["Saturday"]}')

# Use a loop to print out the key-value pairs in the dictionary
for key, value in my_info.items():
    print(f"{key}: {value}")

# Use a loop to print out the keys of the wake-up dictionary
for day in my_info["wake-up"].keys():
    print(day)

# Use a loop to print out the values of the wake-up dictionary
for time in my_info["wake-up"].values():
    print(time)

# Dictionary full of info


# Print out results stored in the dictionary


# Use a loop to print out the key-value pairs in the dictionary


# Use a loop to print out the keys of the wake-up dictionary


# Use a loop to print out the values of the wake-up dictionary


# List of lists example
table_data = [
    ["Ticker", "Open", "Close"],
    ["AAPL", 363.25, 363.4],
    ["AMZN", 2756.0, 2757.99],
    ["GOOG", 1409.1, 1408.2]
]

# Access the Amazon data
amazon_data = table_data[2]
print(amazon_data)

# Get the Amazon opening price
amazon_opening_price = amazon_data[1]
print(amazon_opening_price)

# Combine the previous 2 steps
amazon_opening_price = table_data[2][1]
print(amazon_opening_price)

# Print out the ticker data by row
for row in table_data:
    ticker = row[0]
    print(ticker)

# List of dictionaries example
table_data = [
    {
        "Ticker": "AAPL",
        "Open": 363.25,
        "Close": 363.4
    },
    {
        "Ticker": "AMZN",
        "Open": 2756.0,
        "Close": 2757.99
    },
    {
        "Ticker": "GOOG",
        "Open": 1409.1,
        "Close": 1408.2
    }
]

# Print out each dictionary in the list
for item in table_data:
    print(item)

# Print out just the ticker value in each dictionary in the list
for item in table_data:
    ticker = item["Ticker"]
    print(ticker)



# List of lists example
table_data = [
    ["Ticker", "Open", "Close"],
    ["AAPL", 363.25, 363.4],
    ["AMZN", 2756.0, 2757.99],
    ["GOOG", 1409.1, 1408.2]
]

# Access the Amazon data


# Get the Amazon opening price


# Combine the previous 2 steps


# Print out the ticker data by row


# List of dictionaries example
table_data = [
    {
        "Ticker": "AAPL",
        "Open": 363.25,
        "Close": 363.4
    },
    {
        "Ticker": "AMZN",
        "Open": 2756.0,
        "Close": 2757.99
    },
    {
        "Ticker": "GOOG",
        "Open": 1409.1,
        "Close": 1408.2
    }
]

# Print out each dictionary in the list


# Print out just the ticker value in each dictionary in the list



# List of lists of birds
birds_list = [
    ["Magpie", "Cockatoo", "Hummingbird", "Ostrich", "Bald Eagle", 
     "Emperor Penguin", "Lyrebird", "Peacock", "Toucan", "Helmeted Hornbill"],
    [60, 70, 10, 270, 100, 129, 90, 105, 60, 120],
    [210, 900, 5, 136000, 26000, 112000, 5200, 28600, 4180, 2900],
    [3.5, 45, 5, 40, 30, 20, 30, 15, 20, 30]
]

# List of bird dictionaries
birds_dictionaries = [
    {
        "name": "Magpie",
        "size (cm)": 60,
        "weight (g)": 210,
        "lifespan": 3.5
    },
    {
        "name": "Cockatoo",
        "size (cm)": 70,
        "weight (g)": 900,
        "lifespan": 45
    },
    {
        "name": "Hummingbird",
        "size (cm)": 10,
        "weight (g)": 5,
        "lifespan": 5
    },
    {
        "name": "Ostrich",
        "size (cm)": 270,
        "weight (g)": 136000,
        "lifespan": 40
    },
    {
        "name": "Bald Eagle",
        "size (cm)": 100,
        "weight (g)": 26000,
        "lifespan": 30
    },
    {
        "name": "Emperor Penguin",
        "size (cm)": 129,
        "weight (g)": 112000,
        "lifespan": 20
    },
    {
        "name": "Lyrebird",
        "size (cm)": 90,
        "weight (g)": 5200,
        "lifespan": 30
    },
    {
        "name": "Peacock",
        "size (cm)": 105,
        "weight (g)": 28600,
        "lifespan": 15
    },
    {
        "name": "Toucan",
        "size (cm)": 60,
        "weight (g)": 4180,
        "lifespan": 20
    },
    {
        "name": "Helmeted Hornbill",
        "size (cm)": 120,
        "weight (g)": 2900,
        "lifespan": 30
    }
]

# Print out the data about the 4th bird in birds_list
for row in birds_list:
    print(row[3])

# Calculate the total weight (kg) of all the birds in the birds list
print(f"Total bird weights: {sum(birds_list[2])/1000:.3f}kg")

# Loop through the birds_dictionaries list
for item in birds_dictionaries:
    # Print the names of the birds and their lifespans from the birds_dictionary
    print(f'{item["name"]} can live for {item["lifespan"]} years')
    # Calculate and print out the size to weight ratio
    size_to_weight_ratio = item["size (cm)"] / item["weight (g)"]
    print(f"Its size to weight ratio is {size_to_weight_ratio}")
    print("-" * 50)

# Highest size to weight ratio: hummingbird with 2.0
# Lowest size to weight ratio: emperor penguin with 0.00115

# List of lists of birds
birds_list = [
    ["Magpie", "Cockatoo", "Hummingbird", "Ostrich", "Bald Eagle", 
     "Emperor Penguin", "Lyrebird", "Peacock", "Toucan", "Helmeted Hornbill"],
    [60, 70, 10, 270, 100, 129, 90, 105, 60, 120],
    [210, 900, 5, 136000, 26000, 112000, 5200, 28600, 4180, 2900],
    [3.5, 45, 5, 40, 30, 20, 30, 15, 20, 30]
]

# List of bird dictionaries
birds_dictionaries = [
    {
        "name": "Magpie",
        "size (cm)": 60,
        "weight (g)": 210,
        "lifespan": 3.5
    },
    {
        "name": "Cockatoo",
        "size (cm)": 70,
        "weight (g)": 900,
        "lifespan": 45
    },
    {
        "name": "Hummingbird",
        "size (cm)": 10,
        "weight (g)": 5,
        "lifespan": 5
    },
    {
        "name": "Ostrich",
        "size (cm)": 270,
        "weight (g)": 136000,
        "lifespan": 40
    },
    {
        "name": "Bald Eagle",
        "size (cm)": 100,
        "weight (g)": 26000,
        "lifespan": 30
    },
    {
        "name": "Emperor Penguin",
        "size (cm)": 129,
        "weight (g)": 112000,
        "lifespan": 20
    },
    {
        "name": "Lyrebird",
        "size (cm)": 90,
        "weight (g)": 5200,
        "lifespan": 30
    },
    {
        "name": "Peacock",
        "size (cm)": 105,
        "weight (g)": 28600,
        "lifespan": 15
    },
    {
        "name": "Toucan",
        "size (cm)": 60,
        "weight (g)": 4180,
        "lifespan": 20
    },
    {
        "name": "Helmeted Hornbill",
        "size (cm)": 120,
        "weight (g)": 2900,
        "lifespan": 30
    }
]

# Print out the data about the 4th bird in birds_list


# Calculate the total weight (kg) of all the birds in the birds list


# Loop through the birds_dictionaries list

    # Print the names of the birds and their lifespans from the birds_dictionary


    # Calculate and print out the size to weight ratio


# Highest size to weight ratio: 
# Lowest size to weight ratio: 

# Declare films dictionary
films = {
    "Everything Everywhere All At Once": {
        "director": ["Daniel Kwan", "Daniel Scheinert"],
        "cast": [
            "Michelle Yeoh", 
            "Ke Huy Quan", 
            "Jamie Lee Curtis", 
            "Stephanie Hsu", 
            "James Hong",
            "Jenny Slate",
            "Harry Shum Jr.",
            "Tallie Medel"
        ],
        "distributor": "A24",
        "box_office_in_millions": {
            "us": 72.1,
            "uk": 6.2,
            "canada": 5.1,
            "australia": 4.5
        },
        "release_date": {
            "us": "April 8, 2022",
            "uk": "May 13, 2022",
            "canada": "March 25, 2022",
            "australia": "April 14, 2022"
        }
    },
    "Hidden Figures": {
        "director": "Theodore Melfi",
        "writer": ["Allison Schroeder", "Theodore Melfi"],
        "cast": [
            "Taraji P. Henson", 
            "Octavia Spencer", 
            "Janelle MonÃ¡e",
            "Kevin Costner",
            "Kirsten Dunst", 
            "Jim Parsons",
            "Mahershala Ali",
            "Aldis Hodge",
            "Glen Powell"
        ],
        "distributor": "Twentieth Century Fox",
        "box_office_in_millions": {
            "us": 169.6,
            "japan": 14.1,
            "uk": 7.9,
            "australia": 13.6,
            "france": 5.6
        },
        "release_date": {
            "us": "December 25, 2016",
            "japan": "September 29, 2017",
            "uk": "February 17, 2017",
            "australia": "February 16, 2017",
            "france": "March 7, 2017"
        }
    },
    "Elemental": {
        "director": "Peter Sohn",
        "writer": ["John Hoberg", "Kat Likkel", "Brenda Hsueh"],
        "cast": [
            "Leah Lewis", 
            "Mamoudou Athie", 
            "Ronnie Del Carmen",
            "Shila Ommi",
            "Catherine O'Hara", 
            "Wendi McLendon-Covey",
            "Joe Pera"
        ],
        "distributor": "Walt Disney Studios Motion Pictures",
        "box_office_in_millions": {
            "us": 109.6,
            "south_korea": 25.9,
            "mexico": 13.5,
            "australia": 8.0,
            "france": 9.2
        },
        "release_date": {
            "us": "June 16, 2023",
            "south_korea": "June 14, 2023",
            "mexico": "June 23, 2023",
            "australia": "June 15, 2023",
            "france": "June 21, 2023"
        }
    }
}

menu_dashes = "-" * 60

# Launch the program and present a greeting to the customer
print("Welcome to the film repository.")

films.keys()

# Users may want to view information about different films, so let's create
# a continuous loop
while True:
    # Ask the user which film they want to view
    print("Which film would you like to view information about? ")

    # Create a variable for the menu item number
    i = 1
    # Create a dictionary to store the menu for later retrieval 
    menu_items = {}

    # Print the options to choose from film titles (all the first level 
    # dictionary items in films).
    for key in films.keys():
        print(f"{i}: {key}")
        # Store the film title associated with its menu item number
        menu_items[i] = key
        # Add 1 to the menu item number
        i += 1

    # Get the user's input
    film_selection = input("Type menu number to view or q to quit: ")

    # Exit the loop if user typed 'q'
    if film_selection == 'q':
        break
    # Check if the user's input is a number
    elif film_selection.isdigit():
        # Check if the user's input is a valid option
        if int(film_selection) in menu_items.keys():
            # Save the film name to a variable
            film_selection_name = menu_items[int(film_selection)]
            # Print out the film they selected
            print(f"You selected {film_selection_name}")

            # Display the heading for the data
            print(menu_dashes)
            print(f"Information about {film_selection_name}")
            print(menu_dashes)
            print("Dictionary Reference                   | Information")
            print("---------------------------------------|--------------------")

            # Print out the data from the selected film
            for key, value in films[film_selection_name].items():
                # Check if the value is a dictionary to handle differently
                if type(value) is dict:
                    # Iterate through the dictionary items
                    for key2, value2 in value.items():
                        # Print the film data
                        num_item_spaces = 38 - len(key + key2) - 4
                        item_spaces = " " * num_item_spaces
                        print(f"{key}['{key2}']{item_spaces} | "
                              + f"{value2}")
                # Check if the value is a list to handle differently
                elif type(value) is list:
                    # Iterate through the list items
                    for i in range(len(value)):
                        # Print the film data
                        num_item_spaces = 38 - len(key) - 3
                        item_spaces = " " * num_item_spaces
                        print(f"{key}[{i}]{item_spaces} | "
                              + f"{value[i]}")
                else:
                    # Print the film data
                    num_item_spaces = 38 - len(key)
                    item_spaces = " " * num_item_spaces
                    print(f"{key}{item_spaces} | {value}")
            
            print(menu_dashes)
            input("Press enter to return to the main menu.")

        else:
            # Tell the customer they didn't select a valid option
            print(f"{film_selection} was not an option.")
    else:
        # Tell the customer they didn't select a number
        print("You didn't select a number.")

# Declare films dictionary
films = {
    "Everything Everywhere All At Once": {
        "director": ["Daniel Kwan", "Daniel Scheinert"],
        "cast": [
            "Michelle Yeoh", 
            "Ke Huy Quan", 
            "Jamie Lee Curtis", 
            "Stephanie Hsu", 
            "James Hong",
            "Jenny Slate",
            "Harry Shum Jr.",
            "Tallie Medel"
        ],
        "distributor": "A24",
        "box_office_in_millions": {
            "us": 72.1,
            "uk": 6.2,
            "canada": 5.1,
            "australia": 4.5
        },
        "release_date": {
            "us": "April 8, 2022",
            "uk": "May 13, 2022",
            "canada": "March 25, 2022",
            "australia": "April 14, 2022"
        }
    },
    "Hidden Figures": {
        "director": "Theodore Melfi",
        "writer": ["Allison Schroeder", "Theodore Melfi"],
        "cast": [
            "Taraji P. Henson", 
            "Octavia Spencer", 
            "Janelle MonÃ¡e",
            "Kevin Costner",
            "Kirsten Dunst", 
            "Jim Parsons",
            "Mahershala Ali",
            "Aldis Hodge",
            "Glen Powell"
        ],
        "distributor": "Twentieth Century Fox",
        "box_office_in_millions": {
            "us": 169.6,
            "japan": 14.1,
            "uk": 7.9,
            "australia": 13.6,
            "france": 5.6
        },
        "release_date": {
            "us": "December 25, 2016",
            "japan": "September 29, 2017",
            "uk": "February 17, 2017",
            "australia": "February 16, 2017",
            "france": "March 7, 2017"
        }
    },
    "Elemental": {
        "director": "Peter Sohn",
        "writer": ["John Hoberg", "Kat Likkel", "Brenda Hsueh"],
        "cast": [
            "Leah Lewis", 
            "Mamoudou Athie", 
            "Ronnie Del Carmen",
            "Shila Ommi",
            "Catherine O'Hara", 
            "Wendi McLendon-Covey",
            "Joe Pera"
        ],
        "distributor": "Walt Disney Studios Motion Pictures",
        "box_office_in_millions": {
            "us": 109.6,
            "south_korea": 25.9,
            "mexico": 13.5,
            "australia": 8.0,
            "france": 9.2
        },
        "release_date": {
            "us": "June 16, 2023",
            "south_korea": "June 14, 2023",
            "mexico": "June 23, 2023",
            "australia": "June 15, 2023",
            "france": "June 21, 2023"
        }
    }
}

menu_dashes = "-" * 60

# Launch the program and present a greeting to the customer
print("Welcome to the film repository.")

# Users may want to view information about different films, so let's create
# a continuous loop
while True:
    # Ask the user which film they want to view
    print("Which film would you like to view information about? ")

    # Create a variable for the menu item number
    i = 1
    # Create a dictionary to store the menu for later retrieval 
    menu_items = {}

    # Print the options to choose from film titles (all the first level 
    # dictionary items in films).
    for key in films.keys():
        print(f"{i}: {key}")
        # Store the film title associated with its menu item number
        menu_items[i] = key
        # Add 1 to the menu item number
        i += 1

    # Get the user's input
    film_selection = input("Type menu number to view or q to quit: ")

    # Exit the loop if user typed 'q'
    if film_selection == 'q':
        break
    # Check if the user's input is a number
    elif film_selection.isdigit():
        # Check if the user's input is a valid option

            # Save the film name to a variable

            # Print out the film they selected


            # Display the heading for the data


            # Print out the data from the selected film

                # Check if the value is a dictionary to handle differently

                    # Iterate through the dictionary items

                        # Print the film data

                # Check if the value is a list to handle differently

                    # Iterate through the list items

                        # Print the film data


                    # Print the film data

            

             # Tell the customer they didn't select a valid option
 
    else:
        # Tell the customer they didn't select a number
        print("You didn't select a number.")


# Menu dictionary
menu = {
    "Snacks": {
        "Cookie": .99,
        "Banana": .69,
        "Apple": .49,
        "Granola bar": 1.99
    },
    "Meals": {
        "Burrito": 4.49,
        "Teriyaki Chicken": 9.99,
        "Sushi": 7.49,
        "Pad Thai": 6.99,
        "Pizza": {
            "Cheese": 8.99,
            "Pepperoni": 10.99,
            "Vegetarian": 9.99
        },
        "Burger": {
            "Chicken": 7.49,
            "Beef": 8.49
        }
    },
    "Drinks": {
        "Soda": {
            "Small": 1.99,
            "Medium": 2.49,
            "Large": 2.99
        },
        "Tea": {
            "Green": 2.49,
            "Thai iced": 3.99,
            "Irish breakfast": 2.49
        },
        "Coffee": {
            "Espresso": 2.99,
            "Flat white": 2.99,
            "Iced": 3.49
        }
    },
    "Dessert": {
        "Chocolate lava cake": 10.99,
        "Cheesecake": {
            "New York": 4.99,
            "Strawberry": 6.49
        },
        "Australian Pavlova": 9.99,
        "Rice pudding": 4.99,
        "Fried banana": 4.49
    }
}

menu_dashes = "-" * 42

# Launch the store and present a greeting to the customer
print("Welcome to the variety food truck.")

# Customers may want to view different sections of the menu, so let's create a 
# continuous loop
while True:
    # Ask the customer which menu category they want to view
    print("Which menu would you like to view? ")

    # Create a variable for the menu item number
    i = 1
    # Create a dictionary to store the menu for later retrieval 
    menu_items = {}

    # Print the options to choose from menu headings (all the first level 
    # dictionary items in menu).
    for key in menu.keys():
        print(f"{i}: {key}")
        # Store the menu category associated with its menu item number
        menu_items[i] = key
        # Add 1 to the menu item number
        i += 1

    # Get the customer's input
    menu_category = input("Type menu number to view or q to quit: ")

    # Exit the loop if user typed 'q'
    if menu_category == 'q':
        break
    # Check if the customer's input is a number
    elif menu_category.isdigit():
        # Check if the customer's input is a valid option
        if int(menu_category) in menu_items.keys():
            # Save the menu category name to a variable
            menu_category_name = menu_items[int(menu_category)]
            # Print out the menu category name they selected
            print(f"You selected {menu_category_name}")

            # Display the heading for the sub-menu
            print(menu_dashes)
            print(f"This is the {menu_category_name} menu.")
            print(menu_dashes)
            print("Item # | Item name                | Price")
            print("-------|--------------------------|-------")

            # Initialize a menu item counter
            item_counter = 1
            # Print out the menu options from the menu_category_name
            for key, value in menu[menu_category_name].items():
                # Check if the menu item is a dictionary to handle differently
                if type(value) is dict:
                    # Iterate through the dictionary items
                    for key2, value2 in value.items():
                        # Print the menu item
                        num_item_spaces = 24 - len(key + key2) - 3
                        item_spaces = " " * num_item_spaces
                        print(f"{item_counter}      | "
                              + f"{key} - {key2}{item_spaces} | "
                              + f"${value2}")
                        # Add 1 to the item_counter
                        item_counter += 1
                else:
                    # Print the menu item
                    num_item_spaces = 24 - len(key)
                    item_spaces = " " * num_item_spaces
                    print(f"{item_counter}      | "
                          + f"{key}{item_spaces} | ${value}")
                    # Add 1 to the item_counter
                    item_counter += 1
            
            print(menu_dashes)
            input("Press enter to return to the main menu.")

        else:
            # Tell the customer they didn't select a menu option
            print(f"{menu_category} was not a menu option.")
    else:
        # Tell the customer they didn't select a number
        print("You didn't select a number.")


# Menu dictionary
menu = {
    "Snacks": {
        "Cookie": .99,
        "Banana": .69,
        "Apple": .49,
        "Granola bar": 1.99
    },
    "Meals": {
        "Burrito": 4.49,
        "Teriyaki Chicken": 9.99,
        "Sushi": 7.49,
        "Pad Thai": 6.99,
        "Pizza": {
            "Cheese": 8.99,
            "Pepperoni": 10.99,
            "Vegetarian": 9.99
        },
        "Burger": {
            "Chicken": 7.49,
            "Beef": 8.49
        }
    },
    "Drinks": {
        "Soda": {
            "Small": 1.99,
            "Medium": 2.49,
            "Large": 2.99
        },
        "Tea": {
            "Green": 2.49,
            "Thai iced": 3.99,
            "Irish breakfast": 2.49
        },
        "Coffee": {
            "Espresso": 2.99,
            "Flat white": 2.99,
            "Iced": 3.49
        }
    },
    "Dessert": {
        "Chocolate lava cake": 10.99,
        "Cheesecake": {
            "New York": 4.99,
            "Strawberry": 6.49
        },
        "Australian Pavlova": 9.99,
        "Rice pudding": 4.99,
        "Fried banana": 4.49
    }
}

menu_dashes = "-" * 42

# Launch the store and present a greeting to the customer
print("Welcome to the variety food truck.")

# Customers may want to view different sections of the menu, so let's create a 
# continuous loop
while True:
    # Ask the customer which menu category they want to view
    print("Which menu would you like to view? ")

    # Create a variable for the menu item number
    i = 1
    # Create a dictionary to store the menu for later retrieval 
    menu_items = {}

    # Print the options to choose from menu headings (all the first level 
    # dictionary items in menu).


        # Store the menu category associated with its menu item number

        # Add 1 to the menu item number


    # Get the customer's input
    menu_category = input("Type menu number to view or q to quit: ")

    # Exit the loop if user typed 'q'
    if menu_category == 'q':
        break
    # Check if the customer's input is a number
    elif menu_category.isdigit():
        # Check if the customer's input is a valid option
        if int(menu_category) in menu_items.keys():
            # Save the menu category name to a variable

            # Print out the menu category name they selected
            print(f"You selected {menu_category_name}")

            # Display the heading for the sub-menu
            print(menu_dashes)
            print(f"This is the {menu_category_name} menu.")
            print(menu_dashes)
            print("Item # | Item name                | Price")
            print("-------|--------------------------|-------")

            # Initialize a menu item counter
            item_counter = 1
            # Print out the menu options from the menu_category_name

                # Check if the menu item is a dictionary to handle differently

                    # Iterate through the dictionary items

                        # Print the menu item

                        # Add 1 to the item_counter

                # Else the menu item is not a dictionary

                    # Print the menu item

                    # Add 1 to the item_counter

            
            print(menu_dashes)
            input("Press enter to return to the main menu.")

        else:
            # Tell the customer they didn't select a menu option
            print(f"{menu_category} was not a menu option.")
    else:
        # Tell the customer they didn't select a number
        print("You didn't select a number.")


fish = "halibut"

# Loop through each letter in the string and push to an array
letters = []
for letter in fish:
    letters.append(letter)

print(letters)

# List comprehensions provide concise syntax for creating lists
letters = [letter for letter in fish]

print(letters)

# We can manipulate each element as we go
capital_letters = []
for letter in fish:
    capital_letters.append(letter.upper())

print(capital_letters)

# List comprehension for the above
capital_letters = [letter.upper() for letter in fish]

print(capital_letters)

# We can also add conditional logic (if statements) to a list comprehension
july_temperatures = [87, 85, 92, 79, 106]
hot_days = []
for temperature in july_temperatures:
    if temperature > 90:
        hot_days.append(temperature)
print(hot_days)

# List comprehension with conditional
hot_days = [temperature for temperature in july_temperatures if temperature > 90]

print(hot_days)

# We can also perform calculations in a list comprehension
circle_radii = [2.4, 4.5, 6.2, 7.6, 10.5]
diameters = []
pi = 3.14159265358979323846
for radius in circle_radii:
    diameters.append(2 * pi * radius)
print(diameters)

# List comprehension for the calculation
diameters = [2 * pi * radius for radius in circle_radii]
print(diameters)

# It's even possible to perform list functions on a list comprehension
# Let's find the maximum diameter from our list of radii
max_diameter = max([2 * pi * radius for radius in circle_radii])
print(max_diameter)


fish = "halibut"

# Loop through each letter in the string and push to an array
letters = []
for letter in fish:
    letters.append(letter)

print(letters)

# List comprehensions provide concise syntax for creating lists


# We can manipulate each element as we go
capital_letters = []
for letter in fish:
    capital_letters.append(letter.upper())

print(capital_letters)

# List comprehension for the above


# We can also add conditional logic (if statements) to a list comprehension
july_temperatures = [87, 85, 92, 79, 106]
hot_days = []
for temperature in july_temperatures:
    if temperature > 90:
        hot_days.append(temperature)
print(hot_days)

# List comprehension with conditional


# We can also perform calculations in a list comprehension
circle_radii = [2.4, 4.5, 6.2, 7.6, 10.5]
diameters = []
pi = 3.14159265358979323846
for radius in circle_radii:
    diameters.append(2 * pi * radius)
print(diameters)

# List comprehension for the calculation


# It's even possible to perform list functions on a list comprehension
# Let's find the maximum diameter from our list of radii



# initialize list of guests for user input
guests = []

# Begin a continuous loop that should only be exited when the user says they're 
# done with entering guests.
while True:

    # Ask the user for the guest's name ans save it to a variable
    guest_name = input("Please enter the name of the guest. ")

    # Begin a continuous loop to ask for number of adult guests and check for 
    # input error. Exit the loop when the input is a number and that number 
    # is converted to an integer.
    while True:
        party_number_adults = input(f"How many adult guests in {guest_name}'s party? ")
        if party_number_adults.isdigit():
            party_number_adults = int(party_number_adults)
            break
        else:
            print("You didn't enter a valid response. Please enter a number.")

    # Begin a continuous loop to ask for number of children guests and check for 
    # input error. Exit the loop when the input is a number and that number 
    # is converted to an integer.
    while True:
        party_number_children = input(f"How many child guests in {guest_name}'s party? ")
        if party_number_children.isdigit():
            party_number_children = int(party_number_children)
            break
        else:
            print("You didn't enter a valid response. Please enter a number.")

    # Append the guest list with a dictionary that includes the guest name,
    # number of adults and number of children in the party
    guests.append({
        "guest_name": guest_name,
        "party_number_adults": party_number_adults,
        "party_number_children": party_number_children
    })

    more_guests = input("Do you have more guests to register? Type (n) to exit. ")
    if more_guests == 'n':
        break

# Use a list comprehension to create a list of tuples with the guests for each
# party of guests in the format (adults, children)
tuple_guests = [(guest["party_number_adults"], guest["party_number_children"]) for guest in guests]

print(tuple_guests)

# Use a list comprehension to calculate the total number of adult guests
total_adult_guests = sum([guest["party_number_adults"] for guest in guests])

print(f"Total adult guests entered: {total_adult_guests}")

# Use a list comprehension to calculate the total number of child guests
total_child_guests = sum([guest["party_number_children"] for guest in guests])

print(f"Total children guests entered: {total_child_guests}")

# Use a list comprehension to calculate the total number of guests
total_guests = sum([guest["party_number_children"] + guest["party_number_adults"] for guest in guests])

print(f"Total guests entered: {total_guests}")

# Bonus: Use a list comprehension to create a string of the guest's name in 
# title case and the total number of guests in the party
guests_titled = [f'{guest["guest_name"].title()}: party of {guest["party_number_adults"] + guest["party_number_children"]}' for guest in guests]
print("Guest list:")
for guest in guests_titled:
    print(guest)


# initialize list of guests for user input
guests = []

# Begin a continuous loop that should only be exited when the user says they're 
# done with entering guests.
while True:

    # Ask the user for the guest's name ans save it to a variable
    guest_name = input("Please enter the name of the guest. ")

    # Begin a continuous loop to ask for number of adult guests and check for 
    # input error. Exit the loop when the input is a number and that number 
    # is converted to an integer.
    while True:
        party_number_adults = input(f"How many adult guests in {guest_name}'s party? ")
        if party_number_adults.isdigit():
            party_number_adults = int(party_number_adults)
            break
        else:
            print("You didn't enter a valid response. Please enter a number.")

    # Begin a continuous loop to ask for number of children guests and check for 
    # input error. Exit the loop when the input is a number and that number 
    # is converted to an integer.
    while True:
        party_number_children = input(f"How many child guests in {guest_name}'s party? ")
        if party_number_children.isdigit():
            party_number_children = int(party_number_children)
            break
        else:
            print("You didn't enter a valid response. Please enter a number.")

    # Append the guest list with a dictionary that includes the guest name,
    # number of adults and number of children in the party
    guests.append({
        "guest_name": guest_name,
        "party_number_adults": party_number_adults,
        "party_number_children": party_number_children
    })

    more_guests = input("Do you have more guests to register? Type (n) to exit. ")
    if more_guests == 'n':
        break

# Print the guests dictionary to observe the structure
# Comment out the following line when it is no longer needed
print(guests)

# Use a list comprehension to create a list of tuples with the guests for each
# party of guests in the format (adults, children)


# Use a list comprehension to calculate the total number of adult guests


# Use a list comprehension to calculate the total number of child guests


# Use a list comprehension to calculate the total number of guests


# Bonus: Use a list comprehension to create a string of the guest's name in 
# title case and the total number of guests in the party



# Define the hello function.
def hello():
    print("Hi! This is the hello function.")

# Call the hello function
hello()

# Define the scream function.
def scream():
    print("AAAAH!")
scream()
scream()
scream()
print("Okay, I feel much better.")


# Define the hello function.
def hello():
    print("Hi! This is the hello function.")

# Call the hello function
hello()

# Define the scream function.


"""A Definitive Buy."""


def process_payment():
    print("The total cost of this transaction will be 75 cents.")
    print("Ka-ching! Payment has been processed.")

if __name__ == "__main__":
    process_payment()


"""A Definitive Buy."""


# Create a function that prints two messages.
# The first message should state the cost of the transaction.
# The second message should state that the payment has been processed.




if __name__ == "__main__":
    # Call the function to run the code in the function.
    process_payment()


""" Passing Arguments"""

# 1a. Positional arguments.
def positional(x, y):
    """This function adds two positional arguments,
    adds them and prints the total"""
    total = x + y
    print(f"The total is: {total}")

if __name__ == "__main__":
    positional(5,12)

# 1b. Positional arguments.
def savings(balance, apr, days):
    """This function adds two positional arguments,
    adds them and prints the total"""
    interest_rate = (apr/100) * (days/365)
    interest_earned = balance * interest_rate
    balance += interest_earned
    print(f"The new balance is: {balance}")

if __name__ == "__main__":
    # Incorrect order.
    savings(5, 31, 50000)
    # Correct order
    # savings(50000, 3, 31)

# 2. Keyword arguments.
def keyword(a, b, c):
    """This function takes three keyword arguments,
    adds them, and prints the total"""
    total = a + b + c
    print(f"The total is: {total}")

if __name__ == "__main__":
    keyword(a=-3, c=10, b=5)

# 3. Keyword and positional arguments
def pos_key_args(a, b, c):
    """This function takes one positional argument
    and two keyword arguments, adds them, and prints the total"""
    total = a + b + c
    print(f"The total is: {total}")

if __name__ == "__main__":
    pos_key_args(42, b=-10, c=5)
    # pos_key_args(b=-10, c=5, 42) uncomment this line and run again.

# 4. Iterable unpacking arguments
def iterable(a, b, c):
    """This function takes an iterable (list of tuple)
    and adds the values in the iterable, and prints the total"""
    total = a + b + c
    print(f"The total is: {total}")

if __name__ == "__main__":
    tuple_values = (5, -10, 7)
    list_values = [7, 23, -11]
    iterable(*tuple_values)
    iterable(*list_values)

# 5. Dictionary unpacking arguments.
def dictionary(a, b, c):
    """This function takes an iterable (list of tuple)
    and adds the values in the iterable, and prints the total"""
    total = a + b + c
    print(f"The total is: {total}")

if __name__ == "__main__":
    dict_values = {'b': -4, 'c': 100, 'a':-42 }
    dictionary(*dict_values) # returns the keys
    dictionary(**dict_values) # returns the values


"""Passing Arguments."""

# Define a function that will add two numbers.
def add():
    """This function takes two numbers and adds them and then returns the total."""
    first_number = 1 # This is a local scope of the function.
    second_number = 2 # This is a local scope of the function.
    total = first_number + second_number
    print(f"Your total is: {total}")


if __name__ == "__main__":
    add()
    print(first_number)


# Global variables for first_number and second_number.
first_number = 2
second_number = 3

# Define a function that will add two numbers.
def addition():
    """This function takes two numbers and adds them and then returns the total."""
    first_number = 1 # This is a local variable of the function.
    second_number = 2 # This is a local variable of the function.
    total = first_number + second_number
    print(f"Your total is: {total}")

if __name__ == "__main__":
    addition()
    print(f"The global variables for the 'first_number' and 'second_number` are {first_number} and {second_number}")


""" Passing Arguments"""

# 1a. Positional arguments.
def positional(x, y):
    """This function adds two positional arguments,
    adds them and prints the total"""
    total = x + y
    print(f"The total is: {total}")

if __name__ == "__main__":
    positional(5,12)

# 1b. Positional arguments.
def savings(balance, apr, days):
    """This function adds two positional arguments,
    adds them and prints the total"""
    interest_rate = (apr/100) * (days/365)
    interest_earned = balance * interest_rate
    balance += interest_earned
    print(f"The new balance is: {balance}")

if __name__ == "__main__":
    # Incorrect order.
    savings(5, 31, 50000)
    # Correct order
    # savings(50000, 3, 31)

# 2. Keyword arguments.
def keyword(a, b, c):
    """This function takes three keyword arguments,
    adds them, and prints the total"""
    total = a + b + c
    print(f"The total is: {total}")

if __name__ == "__main__":
    keyword(a=-3, c=10, b=5)

# 3. Keyword and positional arguments
def pos_key_args(a, b, c):
    """This function takes one positional argument
    and two keyword arguments, adds them, and prints the total"""
    total = a + b + c
    print(f"The total is: {total}")

if __name__ == "__main__":
    pos_key_args(42, b=-10, c=5)
    # pos_key_args(b=-10, c=5, 42) uncomment this line and run again.

# 4. Iterable unpacking arguments
def iterable(a, b, c):
    """This function takes an iterable (list of tuple)
    and adds the values in the iterable, and prints the total"""
    total = a + b + c
    print(f"The total is: {total}")

if __name__ == "__main__":
    tuple_values = (5, -10, 7)
    list_values = [7, 23, -11]
    iterable(*tuple_values)
    iterable(*list_values)

# 5. Dictionary unpacking arguments.
def dictionary(a, b, c):
    """This function takes an iterable (list of tuple)
    and adds the values in the iterable, and prints the total"""
    total = a + b + c
    print(f"The total is: {total}")

if __name__ == "__main__":
    dict_values = {'b': -4, 'c': 100, 'a':-42 }
    dictionary(*dict_values) # returns the keys
    dictionary(**dict_values) # returns the values


"""Passing Arguments."""

# Define a function that will add two numbers.
def add():
    """This function takes two numbers and adds them and then returns the total."""
    first_number = 1 # This is a local scope of the function.
    second_number = 2 # This is a local scope of the function.
    total = first_number + second_number
    print(f"Your total is: {total}")


if __name__ == "__main__":
    add()
    print(first_number)


# Global variables for first_number and second_number.
first_number = 2
second_number = 3

# Define a function that will add two numbers.


""" Calculating the cost of a new car"""

def calculate_future_value(current_loan_value, annual_interest_rate, months_remaining):
    """
    Create a function called calculate_future_value
    Args:
        current_loan_value (float): The current loan value
        the annual_interest_rate (float): The APR
        the months_remaining (int): The number of months remaining on the loan

    Returns:
        Prints the future value of the loan as a float.
    """
    future_value = current_loan_value * (1 + (annual_interest_rate / 12)) ** months_remaining
    # Print the future value of the car to 2 decimal places and thousandths.
    print(f"The future value of the new car is ${future_value: ,.2f}.")


if __name__ == "__main__":
    # The new_car_loan dictionary.
    new_car_loan = {
        "current_loan_value": 25000,
        "months_remaining": 6,
        "annual_interest_rate": 0.085
        }

    # Set the function call equal to a variable called cost_of_new_car.
    # Pass the relevant information from the dictionary as arguments to the function.
    calculate_future_value(
        new_car_loan["current_loan_value"],
        new_car_loan["annual_interest_rate"],
        new_car_loan["months_remaining"]
        )


""" Calculating the cost of a new car"""

def calculate_future_value():
    """
    Create a function called calculate_future_value
    Args:
        current_loan_value (float): The current loan value
        the annual_interest_rate (float): The APR
        the months_remaining (int): The number of months remaining on the loan

    Returns:
        Prints the future value of the loan as a float.
    """

    # Print the future value of the car to 2 decimal places and thousandths.



if __name__ == "__main__":
    # The new_car_loan dictionary.
    new_car_loan = {
        "current_loan_value": 25000,
        "months_remaining": 6,
        "annual_interest_rate": 0.085
        }

    # Set the function call equal to a variable called cost_of_new_car.
    # Pass the relevant information from the dictionary as arguments to the function.
    calculate_future_value()


""" Returning Values Demonstration"""

def average_numbers(numbers):
    """ Calculates the average of an array of numbers"""
    average = sum(numbers) / len(numbers)
    print("The average is: ", average)

if __name__ == "__main__":
    average_numbers([1, 2, 3])


# We can return values from inside a function and use those in other parts of the code.
def average_numbers(numbers):
    """ Calculates the average of an array of numbers"""
    average = sum(numbers) / len(numbers)
    return average

if __name__ == "__main__":
    first_average = average_numbers([1, 2, 3])
    second_average = average_numbers([4, 5, 6])
    print(f'The average of the first list is {first_average}')
    print(f'The average of the second list is {second_average}')


""" Returning Values Demonstration"""

def average_numbers(numbers):
    """ Calculates the average of an array of numbers"""
    average = sum(numbers) / len(numbers)
    print("The average is: ", average)

if __name__ == "__main__":
    average_numbers([1, 2, 3])


# We can return values from inside a function and use those in other parts of the code.
# def average_numbers(numbers):
#     """ Calculates the average of an array of numbers"""
#     average = sum(numbers) / len(numbers)
#     return average

# if __name__ == "__main__":
#     first_average = average_numbers([1, 2, 3])
#     second_average = average_numbers([4, 5, 6])
#     print(f'The average of the first list is {first_average}')
#     print(f'The average of the second list is {second_average}')


"""Returned Goods."""

# Define a new function called process_claims
def process_claims(claims):
    """
    Calculate the total insurance payout based on a list of claims.
    Args:
        claims (list): A list of claim amounts.
    Returns:
        float: The total insurance payout, which is 30% of the sum of all claims.
    """
    # Create a variable called `total_claims`, that is the sum of all claims
    total_claims = sum(claims)
    # Calculate a total payout, which is 30% of total_claims:
    total_payout = total_claims * 0.30
    # Return only the total_payout amount
    return total_payout

if __name__ == "__main__":
    # Add the weekly claims
    weekly_claims = [5000, 1000, 8000, 10000, 3000, 3500]
    # Create a variable that passes the weekly claims to the function.
    total_insurance_payout = process_claims(weekly_claims)
    # Print the total insurance payout to 2 decimal places.
    print(f"The total insurance payout is: ${total_insurance_payout: ,.2f}.")


"""Returned Goods."""

# Define a new function called process_claims
def process_claims():
    """
    Calculate the total insurance payout based on a list of claims.
    Args:
        claims (list): A list of claim amounts.
    Returns:
        float: The total insurance payout, which is 30% of the sum of all claims.
    """
    # Create a variable called `total_claims`, that is the sum of all claims

    # Calculate a total payout, which is 30% of total_claims:

    # Return only the total_payout amount


if __name__ == "__main__":
    # Add the weekly claims
    weekly_claims = [5000, 1000, 8000, 10000, 3000, 3500]
    # Create a variable that passes the weekly claims to the function.

    # Print the total insurance payout to 2 decimal places.


# Anonymous Functions

# A function that divides by 7 and rounds to the nearest hundredth
def divide_by_seven(num):
    """A function that divides by 7 and rounds to the nearest hundredth"""
    return round(num / 7,3)
print(f"The result of using a Python function: {[divide_by_seven(number) for number in [24,654,3,961,21]]}")


# Create a list comprehension that divides by 7 and rounds to the nearest hundredth
list_comprehension = [divide_by_seven(number) for number in [24,654,3,961,21]]

# Print the results
print(f"An example of a list comprehension that uses a function: {list_comprehension}")

# Instead of using a list comprehension we can use the map function to do the same thing.
map_function = map(divide_by_seven,(24,654,3,961,21))

# Print the results
print(f"An example of a map function: {list(map_function)}")

# 4. Demonstrate how to implement the previous example as a lambda function within the map function
map_function = map(lambda x: round(x/7,3),(24,654,3,961,21))
print(f"An example of a using the lambda function: {list(map_function)}")

# 5. Use the filter and lambda functions to get only the numbers divided by 3.
numbers = [12, 7, 9, 18, 25, 36, 42, 55, 63]
divisible_by_3 = list(filter(lambda x: x % 3 == 0, numbers))
print(divisible_by_3)


# Anonymous Functions

# 1. A function that divides by 7 and rounds to the nearest hundredth

# 2. Create a list comprehension that divides by 7 and rounds to the nearest hundredth

# 3. Instead of using a list comprehension we can use the map function to do the same thing.

# 4. Demonstrate how to implement the previous example as a lambda function within the map function

# 5. Use the filter and lambda functions to get only the numbers divided by 3.


""" Anonymous Functions"""

# 1. Get the even numbers from a list using the filter and lambda functions.
numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
even_numbers = list(filter(lambda x: x % 2 == 0, numbers))
print(even_numbers)

# 2. Use the map and lambda functions to add the numbers from both lists.
numbers1 = [1, 2, 3, 4, 5]
numbers2 = [10, 20, 30, 40, 50]
result = list(map(lambda x, y: x + y, numbers1, numbers2))
print(result)

# 3. Use the map and lambda functions to split the following sentence into words.
sentence = "My favorite hobby is coding in Python"
words = list(map(lambda x: x.strip(), sentence.split()))
print(words)


""" Anonymous Functions"""

# 1. Get the even numbers from a list using the filter and lambda functions.
numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]


# 2. Use the map and lambda functions to add the numbers from both lists.
numbers1 = [1, 2, 3, 4, 5]
numbers2 = [10, 20, 30, 40, 50]


# 3. Use the map and lambda functions to split the following sentence into words.
sentence = "My favorite hobby is coding in Python"


def my_function(parameter1, parameter2):
    """
    Brief description of the function.

   Optional:
     More detailed description of what the function does.

    Args:
        arg1 (type): Description of arg1.
        arg2 (type): Description of arg2.

    Returns:
        type: Description of the return value or print statement.

    Raises:
        ErrorType: Description of the exception raised, if any.

    Examples:
        Provide some usage examples of the function.

    Note:
        Any additional notes about the function.
    """
    # Function code here

if __name__ == "__main__":
    my_function(arg1=parameter1, arg2=parameter2)


def my_function(parameter1, parameter2):
    """
    Brief description of the function.

   Optional:
     More detailed description of what the function does.

    Args:
        arg1 (type): Description of arg1.
        arg2 (type): Description of arg2.

    Returns:
        type: Description of the return value or print statement.

    Raises:
        ErrorType: Description of the exception raised, if any.

    Examples:
        Provide some usage examples of the function.

    Note:
        Any additional notes about the function.
    """
    # Function code here


"""Pizza Order"""

def create_pizza_order():
    """
    The function prompts the user to choose three toppings
    from a list of five toppings and creates a pizza order.
    Args:
        None
    Returns:
        None. But it prints out the toppings on the pizza.
    Raises:
        ErrorType: If the user enters a number less than 1 or greater than 5
        then asks the user to try again.
    """
    # Create a list of toppings.
    toppings = ['pepperoni', 'mushrooms', 'onions', 'sausage', 'bell peppers']

    # Create a statement about the program.
    print("Welcome to the Sal's Famous Pizza!")
    print("Please choose three toppings for your pizza from the following options:")

    # Create a for loop the iterates through the toppings and displays them to the user.
    for i, topping in enumerate(toppings, start=1):
        print(f"{i}. {topping}")

    # Create an empty list for the toppings the user chooses.
    selected_toppings = []

    # Iterate through the a range of 3 for the number of toppings
    # Prompt the user to input a number for a topping.
    for _ in range(3):
        topping_number = int(input("Enter the number of your chosen topping: "))

        # Create a while loop that selects a number between 1 and 5.
        # If an number less than 1 or greater than 5 is select prompt the user to try again.
        while topping_number < 1 or topping_number > 5:
            print("Invalid topping number. Please try again.")
            topping_number = int(input("Enter the number of your chosen topping: "))

        # Retrieve the selected topping from the list using toppings[topping_number - 1]
        # Then add the topping to the list.
        selected_toppings.append(toppings[topping_number - 1])

    # Print out the order.
    print("Here is your order:")
    print("Your pizza comes with:", ", ".join(selected_toppings))

if __name__ == "__main__":
    # Call the function.
    create_pizza_order()


    # Create a statement about the program.
    print("Welcome to the Sal's Famous Pizza!")
    print("Please choose three toppings for your pizza from the following options:")

    # Create a for loop the iterates through the toppings and displays them to the user.
    for i, topping in enumerate(toppings, start=1):
        print(f"{i}. {topping}")

    # Create an empty list for the toppings the user chooses.
    selected_toppings = []

    # Iterate through the a range of 3 for the number of toppings
    # Prompt the user to input a number for a topping.
    for _ in range(3):
        topping_number = int(input("Enter the number of your chosen topping: "))

        # Create a while loop that selects a number between 1 and 5.
        # If an number less than 1 or greater than 5 is select prompt the user to try again.
        while topping_number < 1 or topping_number > 5:
            print("Invalid topping number. Please try again.")
            topping_number = int(input("Enter the number of your chosen topping: "))

        # Retrieve the selected topping from the list using toppings[topping_number - 1]
        # Then add the topping to the list.
        selected_toppings.append(toppings[topping_number - 1])

    # Print out the order.
    print("Here is your order:")
    print("Your pizza comes with:", ", ".join(selected_toppings))

if __name__ == "__main__":
    # Call the function.
    create_pizza_order()


"""Pizza Order"""

def create_pizza_order():
    toppings = ['pepperoni', 'mushrooms', 'onions', 'sausage', 'bell peppers']

    print("Welcome to the Sal's Famous Pizza!")
    print("Please choose three toppings for your pizza from the following options:")

    for i, topping in enumerate(toppings, start=1):
        print(f"{i}. {topping}")

    selected_toppings = []
    for _ in range(3):
        topping_number = int(input("Enter the number of your chosen topping: "))
        while topping_number < 1 or topping_number > 5:
            print("Invalid topping number. Please try again.")
            topping_number = int(input("Enter the number of your chosen topping: "))

        selected_toppings.append(toppings[topping_number - 1])

    print("Here is your order:")
    print("Your pizza comes with:", ", ".join(selected_toppings))

if __name__ == "__main__":
    create_pizza_order()


"""Importing  modules, functions and methods"""

# Import the sqrt function from the math module.
from math import sqrt

# Calculate the square root of a number.
number = 16
result = sqrt(number)
print(f"The square root of {number} is {result}")


# Import the randint and choice methods from the random module.
from random import randint, choice

# Generate a random number between 1 and 10 and selecting a random element from a list.
random_number = randint(1, 10)
print(f"The random number is: {random_number}")

my_list = ['apple', 'banana', 'orange', 'grape', 'mango']

# Use the choice method to randomly select an element from the list.
random_element = choice(my_list)
print(f"A random element from the list is: {random_element}")

# Import the datetime and date classes from the datetime module.
from datetime import datetime, date

# Get the current datetime using the now function.
current_datetime = datetime.now()
# Get the current time using the strftime function.
current_time = current_datetime.strftime("%H:%M:%S")
# Get the current date using the today function.
current_date = date.today()

print(f"The current datetime is: {current_datetime}")
print(f"The current time is: {current_time}")
print(f"The current date is: {current_date}")


"""Importing  modules, functions and methods"""

# Import the sqrt function from the math module.


# Calculate the square root of a number.


# Import the randint and choice methods from the random module.

# Generate a random number between 1 and 10 and selecting a random element from a list.


# Use the choice method to randomly select an element from the list.

# Import the datetime and date classes from the datetime module.

# Get the current datetime using the now function.


# Get the current time using the strftime function.

# Get the current date using the today function.




""" Calculating the cost of a new car"""

def calculate_future_value(current_loan_value, annual_interest_rate, months_remaining):
    """
    Create a function called calculate_future_value
    Args:
        current_loan_value (float): The current loan value
        the annual_interest_rate (float): The APR
        the months_remaining (int): The number of months remaining on the loan
            Returns:
        Prints the future value of the loan as a float.
    """
    future_value = current_loan_value * (1 + (annual_interest_rate / 12)) ** months_remaining
    return future_value




# Import the calculate_future_value function from the CarLoan file.
from CarLoan import calculate_future_value

# Create the new_car_loan dictionary.
new_car_loan = {
    "current_loan_value": 25000,
    "months_remaining": 12,
    "annual_interest_rate": 0.0315
    }

# Set the function call equal to a variable called car_value.
# Pass the relevant information from the dictionary as parameters to the function call.
car_value = calculate_future_value(
    new_car_loan["current_loan_value"],
    new_car_loan["annual_interest_rate"],
    new_car_loan["months_remaining"]
    )

# Print the future value of the car to 2 decimal places.
print(f"The future value of the car is ${car_value: ,.2f}.")


""" Calculating the cost of a new car"""

def calculate_future_value(current_loan_value, annual_interest_rate, months_remaining):
    """
    Create a function called calculate_future_value
    Args:
        current_loan_value (float): The current loan value
        the annual_interest_rate (float): The APR
        the months_remaining (int): The number of months remaining on the loan
            Returns:
        Prints the future value of the loan as a float.
    """
    future_value = current_loan_value * (1 + (annual_interest_rate / 12)) ** months_remaining
    # Print the future value of the car to 2 decimal places.
    return future_value
s



# Import the calculate_future_value function from the CarLoan file.


# Create the new_car_loan dictionary.
new_car_loan = {
    "current_loan_value": 25000,
    "months_remaining": 12,
    "annual_interest_rate": 0.0315
    }

# Set the function call equal to a variable called car_value.
# Pass the relevant information from the dictionary as parameters to the function call.


# Print the future value of the car to 2 decimal places.


"""Refactoring Examples"""

# Code using for loop.
numbers = [1, 2, 3, 4, 5]
squared_numbers = []
for num in numbers:
    squared_numbers.append(num ** 2)

print(squared_numbers)

# Refactored the code to use a list comprehension
numbers = [1, 2, 3, 4, 5]
squared_numbers = [num ** 2 for num in numbers]

print(squared_numbers)


# Code that uses the range() function.
numbers = [10, 20, 30, 40, 50]
for i in range(len(numbers)):
    print(f"Index: {i}, Value: {numbers[i]}")

# Refactored the code to use enumerate()
numbers = [10, 20, 30, 40, 50]
for i, num in enumerate(numbers):
    print(f"Index: {i}, Value: {num}")


# Code without a function.
numbers = [5, 10, 15, 20, 25]
total = 0
count = 0
for num in numbers:
    total += num
    count += 1
average = total / count
print(f"The average is: {average}")

# Refactored code with a function
def calculate_average(numbers):
    """The function calculates the average of an array of numbers."""
    total = sum(numbers)
    count = len(numbers)
    average = total / count
    return average

numbers = [5, 10, 15, 20, 25]
average = calculate_average(numbers)
print(f"The average is: {average}")


"""Refactoring Examples"""

# Code using for loop.


# Refactored the code to use a list comprehension

# Code that uses the range() function.

# Refactored the code to use enumerate()

# Code without a function.

# Refactored code with a function


# Declare lists
cities = ["Rome", "Nairobi", "Phnom Penh", "Santiago", "Toronto", "Rotorua"]
cities_daily_cost = [150, 70, 60, 80, 110, 125]

print("Here are the cities in the current list:")
for city in cities:
    print(city)
print("Please add 3 cities to the list of cities:")
# Create a for loop to ask the user to enter a city that will be added to the list.
for _ in range(3):
    city = input("What city should be added to the list? ").title()
    cities.append(city)

# Use a while loop to append to the cities_daily_cost list as long as that list
# is shorter than the cities list
while len(cities_daily_cost) < len(cities):
    # Use the length of cities_daily_cost for the index of cities to print its
    # value when asking the cost to append to the cities_daily_cost list
    index = len(cities_daily_cost)
    cost = int(input(f"What is the daily cost for {cities[index]}? "))
    cities_daily_cost.append(cost)

# Loop through the cities_daily_cost list and add 10 to each item
for i in range(len(cities_daily_cost)):
    cities_daily_cost[i] += 10

# Use a for loop to loop through both of the lists at the same time, since
# they're the same length, and print out the city and daily cost on the same
# line
for i in range(len(cities)):
    print(f"Daily cost of {cities[i]} is ${cities_daily_cost[i]}")


"""Refactoring the travel loops code."""

def travel_cities(cities, cities_daily_cost):
    """
    Update the city list and its corresponding daily cost list.

    This function prompts the user to add cities to the city list and their
    respective daily costs. It then modifies the daily cost by adding 10 to
    each item. Finally, it displays the city and its updated daily cost.

    Args:
        cities (list): A list of cities.
        cities_daily_cost (list): A list of daily costs corresponding to the cities.
    Returns:
        The list of cities with the daily cost for each city.
    """

    print("Here are the cities in the current list:")
    for city in cities:
        print(city)
    print("Please add 3 cities to the list of cities:")
    # Create a for loop to ask the user to enter a city that will be added to the list.
    for _ in range(3):
        city = input("What city should be added to the list? ").title()
        cities.append(city)

    # Use a while loop to append to the cities_daily_cost list as long as that list
    # is shorter than the cities list
    while len(cities_daily_cost) < len(cities):
        # Use the length of cities_daily_cost for the index of cities to print its
        # value when asking the cost to append to the cities_daily_cost list
        index = len(cities_daily_cost)
        cost = int(input(f"What is the daily cost for {cities[index]}? "))
        cities_daily_cost.append(cost)

    # Loop through the cities_daily_cost list and add 10 to each item
    for i, cost in enumerate(cities_daily_cost):
        cities_daily_cost[i] += 10

    # Use a for loop to loop through both of the lists at the same time, since
    # they're the same length, and print out the city and daily cost on the same
    # line
    for i, city in enumerate(cities):
        print(f"Daily cost of {city} is ${cities_daily_cost[i]}")

if __name__ == "__main__":

    # Declare lists
    cities = ["Rome", "Nairobi", "Phnom Penh", "Santiago", "Toronto", "Rotorua"]
    cities_daily_cost = [150, 70, 60, 80, 110, 125]

    # Call the function
    travel_cities(cities, cities_daily_cost)


# Declare lists
cities = ["Rome", "Nairobi", "Phnom Penh", "Santiago", "Toronto", "Rotorua"]
cities_daily_cost = [150, 70, 60, 80, 110, 125]

print("Here are the cities in the current list:")
for city in cities:
    print(city)
print("Please add 3 cities to the list of cities:")
# Create a for loop to ask the user to enter a city that will be added to the list.
for _ in range(3):
    city = input("What city should be added to the list? ").title()
    cities.append(city)

# Use a while loop to append to the cities_daily_cost list as long as that list
# is shorter than the cities list
while len(cities_daily_cost) < len(cities):
    # Use the length of cities_daily_cost for the index of cities to print its
    # value when asking the cost to append to the cities_daily_cost list
    index = len(cities_daily_cost)
    cost = int(input(f"What is the daily cost for {cities[index]}? "))
    cities_daily_cost.append(cost)

# Loop through the cities_daily_cost list and add 10 to each item
for i in range(len(cities_daily_cost)):
    cities_daily_cost[i] += 10

# Use a for loop to loop through both of the lists at the same time, since
# they're the same length, and print out the city and daily cost on the same
# line
for i in range(len(cities)):
    print(f"Daily cost of {cities[i]} is ${cities_daily_cost[i]}")


"""Refactoring the travel loops code using a function and enumerate where applicable."""


# Define the function travel_cities.
def travel_cities(cities, cities_daily_cost):
    """
    Update the city list and its corresponding daily cost list.

    This function prompts the user to add cities to the city list and their
    respective daily costs. It then modifies the daily cost by adding 10 to
    each item. Finally, it displays the city and its updated daily cost.

    Args:
        cities (list): A list of cities.
        cities_daily_cost (list): A list of daily costs corresponding to the cities.
    Returns:
        The list of cities with the daily cost for each city.
    """
    # Add the code to be refactored here:



if __name__ == "__main__":

    # Declare lists
    cities = ["Rome", "Nairobi", "Phnom Penh", "Santiago", "Toronto", "Rotorua"]
    cities_daily_cost = [150, 70, 60, 80, 110, 125]

    # Call the function

"""This is a basic ATM Application.
This is a command line application that mimics the actions of an ATM.
"""

accounts = [
    {
    "pin": 123456,
    "balance" : 1436.19},
    {
    "pin" : 246802,
    "balance": 3571.87},
    {
    "pin": 135791,
    "balance" : 543.79},
    {
    "pin" : 123987,
    "balance": 25.89},
    {
    "pin" : 269731,
    "balance": 3258.42}
]

# Define the `login` function for the ATM application.
def login(pin):
    """Create a login function for the ATM application.
    Args:
        pin (integer): The users pin number

    Returns:
        If the pin matches one of the pin numbers in the "accounts"
        the account balance is returned.

    Notes:
        Create a for loop to check to validate the PIN against this list of `accounts`.
        If the PIN is validated, print the account's balance formatted to two decimal places and thousandths.
    """
    for account in accounts:
        if int(pin) == account["pin"]:
            print(f"The account balance for PIN {account['pin']} is: ${account['balance']: ,.2f}.")




if __name__ == "__main__":
    # Set the function call equal to a variable called account_balance.
    account_balance = login(246802)


"""This is a basic ATM Application.

This is a command line application that mimics the actions of an ATM.
"""

accounts = [
    {
    "pin": 123456,
    "balance" : 1436.19},
    {
    "pin" : 246802,
    "balance": 3571.87},
    {
    "pin": 135791,
    "balance" : 543.79},
    {
    "pin" : 123987,
    "balance": 25.89},
    {
    "pin" : 269731,
    "balance": 3258.42}
]

# Define the `login` function for the ATM application.
def login(pin):
     """Create a login function for the ATM application.
    Args:
        pin (integer): The userâ€™s pin number

    Returns:
        If the pin matches one of the pin numbers in the "accounts",
        the account balance is returned.

    Notes:
        Create a for loop to check to validate the PIN against this list of `accounts`.
        If the PIN is validated, print the account's balance formatted to two decimal places and thousandths.
    """



if __name__ == "__main__":
    # Set the function call equal to a variable called account_balance.


"""This is a basic ATM Application.
This is a program consists of the basic actions of an ATM.
"""

accounts = [
    {
    "pin": 123456,
    "balance" : 1436.19},
    {
    "pin" : 246802,
    "balance": 3571.87},
    {
    "pin": 135791,
    "balance" : 543.79},
    {
    "pin" : 123987,
    "balance": 25.89},
    {
    "pin" : 269731,
    "balance": 3278.42}
]

# Define the `login` function for the ATM application.
def login(pin):
    """This function uses a for loop to check to validate the PIN against this list of `accounts`.
        If the PIN is validated, the function should return the account's balance.
    Args:
        pin (integer): The users pin number

    Returns:
        If the pin matches one of the pin numbers in the "accounts"
        the account balance is returned.
    """
    for account in accounts:
        if int(pin) == account["pin"]:
            account_balance = account['balance']

    return account_balance

# Define the `check_balance` function for the ATM application.
def check_balance(account_balance):
    """The function uses the account balance as a parameter.
    and returns the balance of the account.

    Args:
        account_balance (float): The balance of the account.

    Returns:
        Prints the account balance formatted to two decimal places and thousandths.
    """
    print(f"The balance in your account is ${account_balance}.")

# Define the `make_deposit` function for the ATM application.
def make_deposit(account_balance, deposit):
    """# This function takes in the account balance and deposit amount as parameters.
    Then validates that the deposit amount is greater than 0.

    Args:
        account_balance (float): The balance of the account
        deposit (float): An amount deposited into the account greater than 0.

    Returns:
        The function returns the balance after being adjusted for the deposit.

    Notes:
        The deposit balance should equal the account balance plus the deposit amount.
    """
    if deposit > 0:
        deposit_balance = account_balance + deposit
        print(f"The new balance of your account is ${deposit_balance}.")
    else:
        print("Your deposit amount must be positive.")
    return deposit_balance

# Define the `make_withdrawal` function for the ATM application.
def make_withdrawal(account_balance, withdrawal):
    """This function should take in the account balance and withdrawal amount as parameters.
    The function validates that the account balance is greater than the withdrawal.

    Args:
        account_balance (float): The balance of the account.
        withdrawal (integer): Withdrawals are whole dollars.

    Returns:
         The function returns the account balance after being adjusted for the withdrawal.

    Notes:
        The withdrawal balance should equal the account balance minus the withdrawal amount.
    """
    if account_balance > withdrawal:
        withdrawal_balance = account_balance - withdrawal
        print(f"The new balance of your account is ${withdrawal_balance}.")
    else:
        print("You do not have the funds to make this withdrawal.")
    return withdrawal_balance


"""This is a basic ATM Application.
This is a program consists of the basic actions of an ATM.
"""

accounts = [
    {
    "pin": 123456,
    "balance" : 1436.19},
    {
    "pin" : 246802,
    "balance": 3571.87},
    {
    "pin": 135791,
    "balance" : 543.79},
    {
    "pin" : 123987,
    "balance": 25.89},
    {
    "pin" : 269731,
    "balance": 3278.42}
]

# Define the `login` function for the ATM application.
def login(pin):
    """The function a for loop to check to validate the PIN against this list of `accounts`.
        If the PIN is validated, the function should return the account's balance.
    Args:
        pin (integer): The users pin number

    Returns:
        If the pin matches one of the pin numbers in the "accounts"
        the account balance is returned.
    """
    for account in accounts:
        if int(pin) == account["pin"]:
            account_balance = account['balance']

    return account_balance

# Define the `check_balance` function for the ATM application.
def check_balance():
    """The function uses the account balance as a parameter.
    and returns the balance of the account.

    Args:
        account_balance (float): The balance of the account.

    Returns:
        Prints the account balance formatted to two decimal places and thousandths.
    """
    ## YOUR CODE HERE

# Define the `make_deposit` function for the ATM application.
def make_deposit():
    """# This function takes in the account balance and deposit amount as parameters.
    Then validates that the deposit amount is greater than 0.

    Args:
        account_balance (float): The balance of the account
        deposit (float): An amount deposited into the account greater than 0.

    Returns:
        The function returns the balance after being adjusted for the deposit.

    Notes:
        The deposit balance should equal the account balance plus the deposit amount.
    """
    ## YOUR CODE HERE

"""This is a basic ATM Application.
"""

# Import the dependencies.
import sys
# Import the load_accounts and validate_pin functions from the utils file.
from utils import load_accounts, validate_pin
# Import the make_deposit function from the make_deposit file.
from actions.make_deposit import make_deposit
# Import the make_withdrawal function from the make_withdrawal file.
from actions.make_withdrawal import make_withdrawal


def main_menu():
    """Dialog for the ATM Main Menu."""

    # Determines action taken by application.
    action = input("Would you like to check your balance (b), make a deposit (d) or make a withdrawal (w)? Enter b, d, or w. \n")
    return action


def login():
    """This function uses a for loop to check to validate the PIN against this list of `accounts`.
        If the PIN is validated, the function should return the account's balance.
    Args:
        pin (integer): The users pin number

    Returns:
        If the pin matches one of the pin numbers in the "accounts"
        the account is returned.
    """
    # Calls validate_pin() function to confirm length.
    pin = input("Please enter your pin:\n")
    if not validate_pin(pin):
        sys.exit("Sorry, your account PIN is not valid. It must be 6 digits in length.")

    # If pin validates, calls load_accounts() and then verifies pin against accounts list. Returns account that matches pin.
    accounts = load_accounts()

    for account in accounts:
        if int(pin) == account["pin"]:
            return account
        # If no account was returned above, exit with an error

    sys.exit(
        "Sorry, your login was not successful. Your PIN does not link to an account. Please check your PIN and try again."
    )


def run():
    """This function starts the login process.
    It calls the login function and assigns the verified account to the account variable.
    Then, it calls the main_menU() function and ask the user what they want to do.
    A conditional statement is sued to process the action
    and calls the appropriate function based on the action.

    Returns:
        The adjusted balance after the action.

    """
    # Initiates login process. If pin verified, returns validated account.
    account = login()

    # Initiates ATM action: check balance, deposit or withdrawal.
    action = main_menu()

    # Processes the chosen action
    if action == "b":
        sys.exit(f"Your current account balance is {account['balance']}")
    elif action == "d":
        account = make_deposit(account)
    elif action == "w":
        account = make_withdrawal(account)

    # Prints the adjusted balance.
    print(
        f"Thank you for using this ATM. Your adjusted balance is ${account['balance']: ,.2f}."
    )


if __name__ == "__main__":
    # Call the run function.
    run()


"""This is a basic ATM Application.
"""

# Import the dependencies.
import sys
# Import the load_accounts and validate_pin functions from the utils file.
from utils import load_accounts, validate_pin
# Import the make_deposit function from the make_deposit file.
from actions.make_deposit import make_deposit
# Import the make_withdrawal function from the make_withdrawal file.
from actions.make_withdrawal import make_withdrawal

def main_menu():
    """Dialog for the ATM Main Menu."""

    # Determines action taken by application.
    action = input("Would you like to check your balance (b), make a deposit (d) or make a withdrawal (w)? Enter b, d, or w. \n")
    return action


def login():
    """This function uses a for loop to check to validate the PIN against this list of `accounts`.
        If the PIN is validated, the function should return the account's balance.
    Args:
        pin (integer): The users pin number

    Returns:
        If the pin matches one of the pin numbers in the "accounts"
        the account is returned.
    """
    # Calls validate_pin() function to confirm length.
    pin = input("Please enter your pin:\n")
    if not validate_pin(pin):
        sys.exit("Sorry, your account PIN is not valid. It must be 6 digits in length.")

    # If pin validates, calls load_accounts() and then verifies pin against accounts list. Returns account that matches pin.
    accounts = load_accounts()

    for account in accounts:
        if int(pin) == account["pin"]:
            return account
        # If no account was returned above, exit with an error

    sys.exit(
        "Sorry, your login was not successful. Your PIN does not link to an account. Please check your PIN and try again."
    )



def run():
    """This function starts the login process.
    It calls the login function and assigns the verified account to the account variable.
    Then, it calls the main_menU() function and ask the user what they want to do.
    A conditional statement is sued to process the action
    and calls the appropriate function based on the action.

    Returns:
        The adjusted balance after the action.

    """
    # Initiates login process. If pin verified, returns validated account.
    account = login()

    # Initiates ATM action: check balance, deposit or withdrawal.
    action = main_menu()

    # Processes the chosen action
    if action == "b":
        sys.exit(f"Your current account balance is {account['balance']}")
    elif action == "d":
        account = make_deposit(account)
    elif action == "w":
        account = make_withdrawal(account)

    # Prints the adjusted balance.
    print(
        f"Thank you for using this ATM. Your adjusted balance is ${account['balance']: ,.2f}."
    )




if __name__ == "__main__":
    # Call the run function.
    run()


"""Helper functions for loading accounts and validating PIN number."""

# Import the dependencies.
import csv
import sys
from pathlib import Path


def load_accounts():
    """This function opens the CSV file. And appends each account: the pin and balance,
    to the accounts lists.

    Returns:
        accounts (dict object): A dictionary of all the accounts.
    """
    csvpath = Path('data/accounts.csv')
    accounts = []
    # Open and read the CSV file.
    with open(csvpath, newline='', encoding='utf-8') as csvfile:
        #  Get the rows of the CSV file.
        rows = csv.reader(csvfile)
        # Skip reading the header row.
        header = next(rows)
        for row in rows:
            pin = int(row[0])
            balance = float(row[1])
            account = {
                "pin": pin,
                "balance": balance
            }
            accounts.append(account)
        return accounts


def validate_pin(pin):
    """This function takes in the pin given by the user
    and checks to make sure its length is 6.

    Args:
        pin (integer): The pin for the account.

    Returns:
        If the pin is correct, the login function loads the account.
        If the pin is incorrect, the system lets the user know that the pin is incorrect.
    """
    # Verifies length of pin is 6 digits prints validations message and return True.
    # Else returns False.
    if len(pin) == 6:
        print("Your PIN is valid")
        return True
    else:
        return False

"""Adjusts account balance after a deposit."""

import sys

def make_deposit(account):
    """This function prompts the user to make a deposit.
    If the amount is greater than 0.0 the balance was successful.
    If the amount is less than 0.0 then the system ask the user to try again.

    Args:
        account (dict): The keys and values of the validated account.

    Returns:
        account (dict): The account balance after the deposit.
    """
    # Use input to determine the amount of the deposit
    # Re-type amount from a string to a floating point number.
    amount = input("How much would you like to deposit?\n")
    amount = float(amount)

  # Validates amount of deposit. If true processes deposit, else returns error.
    if amount > 0.0:
        account["balance"] = account["balance"] + amount
        print("Your deposit was successful.")
        return account

    sys.exit("This is not a valid deposit amount. Please try again.")


"""Adjusts account balance after a withdrawal"""

import sys



def make_withdrawal(account):
    """This function prompts the user to make a withdrawal.
    If the amount is less than or equal to 0.0 the withdrawal the system ask the user to try again.
    If the amount is less than or equal to the account balance the withdrawal was successful.
    Else the the withdrawal can't be made, and the system ask the user to try again.

    Args:
        account (dict): The keys and values of the validated account.

    Returns:
        account (dict): The account balance after the withdrawal.
    """
    # Use input to determine the amount of the withdrawal
    # Re-type amount from a string to a floating point number.
    amount = input("How much would you like to withdraw?\n")
    amount = float(amount)

    # Validates amount of withdrawal. If less than or equal to 0 system exits with error message.
    if amount <= 0.0:
        sys.exit("This is not a valid withdrawal amount. Please try again.")

    # Validates if withdrawal amount is less than or equal to account balance, processes withdrawal and returns account.
    # Else system exits with error messages indicating that the account is short of funds.
    if amount <= account["balance"]:
        account["balance"] = account["balance"] - amount
        print("Your withdrawal was successful!")
        return account
    sys.exit(
            "You do not have enough money in your account to make this withdrawal. Please try again."
        )


"""This is a basic ATM Application.
This is a program consists of the basic actions of an ATM.
"""

import csv
import sys
from pathlib import Path


def load_accounts():
    """This function opens the CSV file. And appends each account: the pin and balance,
    to the accounts lists.

    Returns:
        accounts (dict object): A dictionary of all the accounts.
    """
    csvpath = Path('data/accounts.csv')
    accounts = []
    # Open and read the CSV file.
    with open(csvpath, newline='', encoding='utf-8') as csvfile:
        #  Get the rows of the CSV file.
        rows = csv.reader(csvfile)
        # Skip reading the header row.
        header = next(rows)
        for row in rows:
            pin = int(row[0])
            balance = float(row[1])
            account = {
                "pin": pin,
                "balance": balance
            }
            accounts.append(account)
        return accounts


def validate_pin(pin):
    """This function takes in the pin given by the user
    and checks to make sure its length is 6.

    Args:
        pin (integer): The pin for the account.

    Returns:
        If the pin is correct, the login function loads the account.
        If the pin is incorrect, the system lets the user know that the pin is incorrect.
    """
    # Verifies length of pin is 6 digits prints validations message and return True. Else returns False.
    if len(pin) == 6:
        print("Your PIN is valid")
        return True
    else:
        return False


def main_menu():
    """Dialog for the ATM Main Menu."""

    # Determines action taken by application.
    action = input("Would you like to check your balance (b), make a deposit (d) or make a withdrawal (w)? Enter b, d, or w. \n")
    return action


def login():
    """This function uses a for loop to check to validate the PIN against this list of `accounts`.
        If the PIN is validated, the function should return the account's balance.
    Args:
        pin (integer): The users pin number

    Returns:
        If the pin matches one of the pin numbers in the "accounts"
        the account is returned.
    """
    pin = input("Please enter your pin:\n")
    if not validate_pin(pin):
        sys.exit("Sorry, your account PIN is not valid. It must be 6 digits in length.")

    # If pin validates, calls load_accounts() and then verifies pin against accounts list. Returns account that matches pin.
    accounts = load_accounts()

    for account in accounts:
        if int(pin) == account["pin"]:
            return account
        # If no account was returned above, exit with an error

    sys.exit(
        "Sorry, your login was not successful. Your PIN does not link to an account. Please check your PIN and try again."
    )


def make_deposit(account):
    """This function prompts the user to make a deposit.
    If the amount is greater than 0.0 the balance was successful.
    If the amount is less than 0.0 then the system ask the user to try again.

    Args:
        account (dict): The keys and values of the validated account.

    Returns:
        account (dict): The account balance after the deposit.
    """
    # Use input to determine the amount of the deposit
    # Re-type amount from a string to a floating point number.
    amount = input("How much would you like to deposit?\n")
    amount = float(amount)

  # Validates amount of deposit. If true processes deposit, else returns error.
    if amount > 0.0:
        account["balance"] = account["balance"] + amount
        print("Your deposit was successful.")
        return account
    else:
        sys.exit("This is not a valid deposit amount. Please try again.")


def make_withdrawal(account):
    """This function prompts the user to make a withdrawal.
    If the amount is less than or equal to 0.0 the withdrawal the system ask the user to try again.
    If the amount is less than or equal to the account balance the withdrawal was successful.
    Else the the withdrawal can't be made, and the system ask the user to try again.

    Args:
        account (dict): The keys and values of the validated account.

    Returns:
        account (dict): The account balance after the withdrawal.
    """
    # Use input to determine the amount of the withdrawal
    # Re-type amount from a string to a floating point number.
    amount = input("How much would you like to withdraw?\n")
    amount = float(amount)

    # Validates amount of withdrawal. If less than or equal to 0 system exits with error message.
    if amount <= 0.0:
        sys.exit("This is not a valid withdrawal amount. Please try again.")

    # Validates if withdrawal amount is less than or equal to account balance, processes withdrawal and returns account.
    # Else system exits with error messages indicating that the account is short of funds.
    if amount <= account["balance"]:
        account["balance"] = account["balance"] - amount
        print("Your withdrawal was successful!")
        return account
    else:
        sys.exit(
            "You do not have enough money in your account to make this withdrawal. Please try again."
        )


def run():
    """This function starts the login process.
    It calls the login function and assigns the verified account to the account variable.
    Then, it calls the main_menU() function and ask the user what they want to do.
    A conditional statement is sued to process the action
    and calls the appropriate function based on the action.

    Returns:
        The adjusted balance after the action.

    """
    # Initiates login process. If pin verified, returns validated account.
    account = login()

    # Initiates ATM action: check balance, deposit or withdrawal.
    action = main_menu()

    # Processes the chosen action
    if action == "b":
        sys.exit(f"Your current account balance is {account['balance']}")
    elif action == "d":
        account = make_deposit(account)
    elif action == "w":
        account = make_withdrawal(account)

    # Prints the adjusted balance.
    print(
        f"Thank you for using this atm. Your adjusted balance is ${account['balance']: .2f}."
    )


if __name__ == "__main__":
    run()


"""Helper functions for loading accounts and validating PIN number."""

# Import the dependencies.
import csv
import sys
from pathlib import Path


"""Adjusts account balance after a deposit."""

import sys


def make_deposit(account):
    """This function prompts the user to make a deposit.
    If the amount is greater than 0.0 the balance was successful.
    If the amount is less than 0.0 then the system ask the user to try again.

    Args:
        account (dict): The keys and values of the validated account.

    Returns:
        account (dict): The account balance after the deposit.
    """
    # Use input to determine the amount of the deposit
    # Re-type amount from a string to a floating point number.


"""Adjusts account balance after a withdrawal"""

import sys


def make_withdrawal(account):
    """This function prompts the user to make a withdrawal.
    If the amount is less than or equal to 0.0 the withdrawal the system ask the user to try again.
    If the amount is less than or equal to the account balance the withdrawal was successful.
    Else the the withdrawal can't be made, and the system ask the user to try again.

    Args:
        account (dict): The keys and values of the validated account.

    Returns:
        account (dict): The account balance after the withdrawal.
    """
    # Use input to determine the amount of the withdrawal
    # Re-type amount from a string to a floating point number.


"""This is a basic ATM Application.
"""

# Import the dependencies.
import sys
# Import the load_accounts and validate_pin functions from the utils file.
from utils import load_accounts, validate_pin
# Import the make_deposit function from the make_deposit file.
from actions.make_deposit import make_deposit
# Import the make_withdrawal function from the make_withdrawal file.
from actions.make_withdrawal import make_withdrawal


def main_menu():
    """Dialog for the ATM Main Menu."""

    # Determines action taken by application.
    action = input("Would you like to check your balance (b), make a deposit (d) or make a withdrawal (w)? Enter b, d, or w. \n")
    return action


def login():
    """This function uses a for loop to check to validate the PIN against this list of `accounts`.
        If the PIN is validated, the function should return the account's balance.
    Args:
        pin (integer): The users pin number

    Returns:
        If the pin matches one of the pin numbers in the "accounts"
        the account is returned.
    """
    # Calls validate_pin() function to confirm length.
    pin = input("Please enter your pin:\n")
    if not validate_pin(pin):
        sys.exit("Sorry, your account PIN is not valid. It must be 6 digits in length.")

    # If pin validates, calls load_accounts() and then verifies pin against accounts list. Returns account that matches pin.
    accounts = load_accounts()

    for account in accounts:
        if int(pin) == account["pin"]:
            return account
        # If no account was returned above, exit with an error

    sys.exit(
        "Sorry, your login was not successful. Your PIN does not link to an account. Please check your PIN and try again."
    )


def run():
    """This function starts the login process.
    It calls the login function and assigns the verified account to the account variable.
    Then, it calls the main_menU() function and ask the user what they want to do.
    A conditional statement is sued to process the action
    and calls the appropriate function based on the action.

    Returns:
        The adjusted balance after the action.

    """
    # Initiates login process. If pin verified, returns validated account.
    account = login()

    # Initiates ATM action: check balance, deposit or withdrawal.
    action = main_menu()

    # Processes the chosen action
    if action == "b":
        sys.exit(f"Your current account balance is {account['balance']}")
    elif action == "d":
        account = make_deposit(account)
    elif action == "w":
        account = make_withdrawal(account)

    # Prints the adjusted balance.
    print(
        f"Thank you for using this ATM. Your adjusted balance is ${account['balance']: ,.2f}."
    )


if __name__ == "__main__":
    # Call the run function.
    run()


"""Helper functions for loading accounts and validating PIN number."""

# Import the dependencies.
import csv
import sys
from pathlib import Path


def load_accounts():
    """This function opens the CSV file. And appends each account: the pin and balance,
    to the accounts lists.

    Returns:
        accounts (dict object): A dictionary of all the accounts.
    """
    csvpath = Path('data/accounts.csv')
    accounts = []
    # Open and read the CSV file.
    with open(csvpath, newline='', encoding='utf-8') as csvfile:
        #  Get the rows of the CSV file.
        rows = csv.reader(csvfile)
        # Skip reading the header row.
        header = next(rows)
        for row in rows:
            pin = int(row[0])
            balance = float(row[1])
            account = {
                "pin": pin,
                "balance": balance
            }
            accounts.append(account)
        return accounts


def validate_pin(pin):
    """This function takes in the pin given by the user
    and checks to make sure its length is 6.

    Args:
        pin (integer): The pin for the account.

    Returns:
        If the pin is correct, the login function loads the account.
        If the pin is incorrect, the system lets the user know that the pin is incorrect.
    """
    # Verifies length of pin is 6 digits prints validations message and return True.
    # Else returns False.
    if len(pin) == 6:
        print("Your PIN is valid")
        return True
    else:
        return False


"""Adjusts account balance after a deposit."""

import sys


def make_deposit(account):
    """This function prompts the user to make a deposit.
    If the amount is greater than 0.0 the balance was successful.
    If the amount is less than 0.0 then the system ask the user to try again.

    Args:
        account (dict): The keys and values of the validated account.

    Returns:
        account (dict): The account balance after the deposit.
    """
    # Use input to determine the amount of the deposit
    # Re-type amount from a string to a floating point number.
    amount = input("How much would you like to deposit?\n")
    amount = float(amount)

  # Validates amount of deposit. If true processes deposit, else returns error.
    if amount > 0.0:
        account["balance"] = account["balance"] + amount
        print("Your deposit was successful.")
        return account

    sys.exit("This is not a valid deposit amount. Please try again.")


"""Adjusts account balance after a withdrawal"""

import sys


def make_withdrawal(account):
    """This function prompts the user to make a withdrawal.
    If the amount is less than or equal to 0.0 the withdrawal the system ask the user to try again.
    If the amount is less than or equal to the account balance the withdrawal was successful.
    Else the the withdrawal can't be made, and the system ask the user to try again.

    Args:
        account (dict): The keys and values of the validated account.

    Returns:
        account (dict): The account balance after the withdrawal.
    """
    # Use input to determine the amount of the withdrawal
    # Re-type amount from a string to a floating point number.
    amount = input("How much would you like to withdraw?\n")
    amount = float(amount)

    # Validates amount of withdrawal. If less than or equal to 0 system exits with error message.
    if amount <= 0.0:
        sys.exit("This is not a valid withdrawal amount. Please try again.")

    # Validates if withdrawal amount is less than or equal to account balance, processes withdrawal and returns account.
    # Else system exits with error messages indicating that the account is short of funds.
    if amount <= account["balance"]:
        account["balance"] = account["balance"] - amount
        print("Your withdrawal was successful!")
        return account
    sys.exit(
            "You do not have enough money in your account to make this withdrawal. Please try again."
        )


"""This is a basic ATM Application.

This is a program consists of the basic actions of an ATM.

Example:
    $ python app.py
"""
# Import the dependencies.
import csv
import sys
from pathlib import Path


def load_accounts():
    """This function opens the CSV file. And appends each account: the pin and balance,
    to the accounts lists.

    Returns:
        accounts (dict object): A dictionary of all the accounts.
    """
    csvpath = Path('data/accounts.csv')
    accounts = []
    # Open and read the CSV file.
    with open(csvpath, newline='', encoding='utf-8') as csvfile:
        #  Get the rows of the CSV file.
        rows = csv.reader(csvfile)
        # Skip reading the header row.
        header = next(rows)
        for row in rows:
            pin = int(row[0])
            balance = float(row[1])
            account = {
                "pin": pin,
                "balance": balance
            }
            accounts.append(account)
        return accounts


def validate_pin(pin):
    """This function takes in the pin given by the user
    and checks to make sure its length is 6.

    Args:
        pin (integer): The pin for the account.

    Returns:
        If the pin is correct, the login function loads the account.
        If the pin is incorrect, the system lets the user know that the pin is incorrect.
    """
    # Verifies length of pin is 6 digits prints validations message and return True.
    # Else returns False.
    if len(pin) == 6:
        print("Your PIN is valid")
        return True
    else:
        return False


def main_menu():
    """This function prompts the user to make a selection check their balance,
    make a deposit, or make a withdrawal.

    Returns:
        The action that user wants to do.
    """
    # Determines action taken by application.
    action = input("Would you like to: \n"
                "Check your balance (b),\n"
                "Make a deposit (d),\n"
                "Or make a withdrawal (w)?|n"
                "Enter b, d, or w. \n")
    return action


def login():
    """This function uses ask the user to enter their pin number.
    The pin number is passed to the validate_pin function.
    If the pin is valid, then the load_accounts function is called and
    the dictionary of accounts is assigned the accounts variable.
    A for loop verifies the pin against the listed accounts.

    Returns:
        The pin and balance of the account after the pin is validated.
    """
    # Calls validate_pin() function to confirm length.
    pin = input("Please enter your pin:\n")
    if not validate_pin(pin):
        sys.exit("Sorry, your account PIN is not valid. It must be 6 digits in length.")

    # If pin validates, calls load_accounts() and then verifies pin against accounts list.
    # Returns account that matches pin.
    accounts = load_accounts()

    for account in accounts:
        if int(pin) == account["pin"]:
            return account
        # If no account was returned above, exit with an error

    sys.exit(
        "Sorry, your login was not successful. Please check your PIN and try again."
    )


def make_deposit(account):
    """This function prompts the user to make a deposit.
    If the amount is greater than 0.0 the balance was successful.
    If the amount is less than 0.0 then the system ask the user to try again.

    Args:
        account (dict): The keys and values of the validated account.

    Returns:
        account (dict): The account balance after the deposit.
    """
    # Use input to determine the amount of the deposit
    # Re-type amount from a string to a floating point number.
    amount = input("How much would you like to deposit?\n")
    amount = float(amount)

  # Validates amount of deposit. If true processes deposit, else returns error.
    if amount > 0.0:
        account["balance"] = account["balance"] + amount
        print("Your deposit was successful.")
        return account

    sys.exit("This is not a valid deposit amount. Please try again.")


def make_withdrawal(account):
    """This function prompts the user to make a withdrawal.
    If the amount is less than or equal to 0.0 the withdrawal the system ask the user to try again.
    If the amount is less than or equal to the account balance the withdrawal was successful.
    Else the the withdrawal can't be made, and the system ask the user to try again.

    Args:
        account (dict): The keys and values of the validated account.

    Returns:
        account (dict): The account balance after the withdrawal.
    """
    # Use input to determine the amount of the withdrawal
    # Re-type amount from a string to a floating point number.
    amount = input("How much would you like to withdraw?\n")
    amount = float(amount)

    # Validates amount of withdrawal. If less than or equal to 0 system exits with error message.
    if amount <= 0.0:
        sys.exit("This is not a valid withdrawal amount. Please try again.")

    # Validates if withdrawal amount is less than or equal to account balance, processes withdrawal and returns account.
    # Else system exits with error messages indicating that the account is short of funds.
    if amount <= account["balance"]:
        account["balance"] = account["balance"] - amount
        print("Your withdrawal was successful!")
        return account
    sys.exit(
            "You do not have enough money in your account to make this withdrawal. Please try again."
        )


def run():
    """This function starts the login process.
    It calls the login function and assigns the verified account to the account variable.
    Then, it calls the main_menU() function and ask the user what they want to do.
    A conditional statement is sued to process the action
    and calls the appropriate function based on the action.

    Returns:
        The adjusted balance after the action.

    """
    # Initiates login process. If pin verified, returns validated account.
    account = login()

    # Initiates ATM action: check balance, deposit or withdrawal.
    action = main_menu()

    # Processes the chosen action
    if action == "b":
        sys.exit(f"Your current account balance is {account['balance']}")
    elif action == "d":
        account = make_deposit(account)
    elif action == "w":
        account = make_withdrawal(account)

    # Prints the adjusted balance.
    print(
        f"Thank you for using this ATM. Your adjusted balance is ${account['balance']: ,.2f}."
    )


if __name__ == "__main__":
    # Call the run function.
    run()


# Dependencies
import pandas as pd

# We can create a Pandas Series from a raw list
data_series = pd.Series(["UCLA", "UC Berkeley", "UC Irvine",
                         "University of Central Florida", "Rutgers University"])
data_series

# Convert a list of dictionaries into a DataFrame
states_dicts = [{"STATE": "New Jersey", "ABBREVIATION": "NJ"},
                {"STATE": "New York", "ABBREVIATION": "NY"}]

states_df = pd.DataFrame(states_dicts)
states_df

# Convert a single dictionary containing lists into a DataFrame
pharaoh_df = pd.DataFrame(
    {"Dynasty": ["Early Dynastic Period", "Old Kingdom"],
     "Pharaoh": ["Thinis", "Memphis"]
     }
)
pharaoh_df



# Dependencies
import pandas as pd

# We can create a Pandas Series from a raw list


# Convert a list of dictionaries into a DataFrame


# Convert a single dictionary containing lists into a DataFrame




# Import Dependencies
import pandas as pd

# Create a DataFrame from a list of dictionaries.
painting_df = pd.DataFrame([
    {"Painting": "Mona Lisa (Knockoff)", "Price": 25,
     "Popularity": "Very Popular"},
    {"Painting": "Van Gogh (Knockoff)", "Price": 20, "Popularity": "Popular"},
    {"Painting": "Starving Artist", "Price": 10, "Popularity": "Average"},
    {"Painting": "Toddler Drawing", "Price": 1, "Popularity": "Not Popular"}
])
painting_df

# Create a DataFrame of frames using a dictionary of lists.
frame_df = pd.DataFrame({
    "Frame": ["Ornate", "Classical", "Modern", "Wood", "Cardboard"],
    "Price": [15.00, 12.50, 10.00, 5.00, 1.00],
    "Sales": [100, 200, 150, 300, "N/A"]
})
frame_df

# Import Dependencies
import pandas as pd

# The data is given in dictionaries
row1 = {"Painting": "Mona Lisa (Knockoff)", "Price": 25,
     "Popularity": "Very Popular"}
row2 = {"Painting": "Van Gogh (Knockoff)", "Price": 20, "Popularity": "Popular"}
row3 = {"Painting": "Starving Artist", "Price": 10, "Popularity": "Average"}
row4 = {"Painting": "Toddler Drawing", "Price": 1, "Popularity": "Not Popular"}

# Create a list containing the dictionaries

# Create a dataframe using the list of dictionaries

# The data is given in lists
frame_column = ["Ornate", "Classical", "Modern", "Wood", "Cardboard"]
price_column = [15.00, 12.50, 10.00, 5.00, 1.00]
sales_column = [100, 200, 150, 300, "N/A"]

# Create a dictionary containing the lists

# Create a dataframe using the dictionary of lists


# Dependencies
import pandas as pd

# Store filepath in a variable
file_one = "../Resources/DataOne.csv"

# Read our data file with the Pandas library
# Not every CSV requires an encoding, but be aware this can come up
file_one_df = pd.read_csv(file_one, encoding="ISO-8859-1")

# Show the first five rows.
file_one_df.head()

# Show a single column
file_one_df["full_name"].head()

# Show mulitple specific columns--note the extra brackets
file_one_df[["full_name", "email"]].head()

# Show the last five rows.
file_one_df.tail()

# Export file as a CSV, without the Pandas index, but with the header
file_one_df.to_csv("Output/fileOne.csv", index=False, header=True)



# Dependencies
import pandas as pd
from pathlib import Path

# Store filepath in a variable
file_one = Path("../Resources/DataOne.csv")

# Read our Data file with the pandas library
# Not every CSV requires an encoding, but be aware this can come up
file_one_df = pd.read_csv(file_one, encoding="ISO-8859-1")

# Show the first five rows.


# Show a single column


# Show mulitple specific columns--note the extra brackets


# Show the last five rows.


# Export file as a CSV, without the Pandas index, but with the header




# Import Dependencies
import pandas as pd

# A DataFrame of individuals' contact information
people_df = pd.read_csv('../Resources/people.csv')
people_df.head()

# Collect a list of all columns within the DataFrame
people_df.columns

# Reorganize the columns using double brackets
organized_df = people_df[["last_name","company_name","city","email"]]
organized_df.head()

# Use .rename(columns={}) to rename columns
renamed_df = organized_df.rename(columns={"last_name":"Last Name", "company_name":"Company", "city": "City", "email": "Email"})
renamed_df.head()



# Import Dependencies
import pandas as pd

# A DataFrame of individuals' contact information
people_df = pd.read_csv('../Resources/people.csv')
people_df.head()

# Collect a list of all columns within the DataFrame


# Reorganize the columns using double brackets


# Use .rename(columns={}) to rename columns




# Dependencies
import pandas as pd

# Create a DataFrame with given columns and value
donors_df = pd.read_csv("../Resources/donors2021.csv")

donors_df.head()

# Rename columns for readability
donors_df_renamed = donors_df.rename(columns={"donorName": "Donor",
                                                "employerName": "Employer",
                                                "zipcode": "Zip Code",
                                                "usd": "Donation Amount"
                                                })
donors_df_renamed.head()

# Organize the columns so that only the donor name,
# amount, employer, and city are displayed. 
# The name and donation amount should be first

donors_df_organized = donors_df_renamed[['Donor', 
                                         'Donation Amount', 
                                         'Employer',
                                         'City'
                                         ]]

donors_df_organized.head()



# Dependencies
import pandas as pd

# Create a DataFrame with given columns and value
donors_df = pd.read_csv("../Resources/donors2021.csv")

donors_df.head()

# Rename columns for readability


# Organize the columns so that only the donor name,
# amount, employer, and city are displayed. 
# The name and donation amount should be first




# Import libraries and dependencies
import pandas as pd

# Set the file path
file_path = '../Resources/people.csv'

# Read in the CSV as a DataFrame
people_csv = pd.read_csv(file_path)
people_csv.head()

# Select the first row of the DataFrame
people_csv.iloc[0]

# Select the second row of the DataFrame
people_csv.iloc[1] 

# Select the first 10 rows of the DataFrame
people_csv.iloc[0:10] 

# Select the last row of the DataFrame
people_csv.iloc[-1]

# Select the first column of the DataFrame
people_csv.iloc[:,0].head()

# Select the second column of the DataFrame, with all rows
people_csv.iloc[:,1].head()

# Select the last column of the DataFrame, with all rows
people_csv.iloc[:,-1].head()

# Select the first two columns of the DataFrame, with all rows
people_csv.iloc[:, 0:2].head()

# Select the 1st, 5th, 8th, 22nd rows of the 1st 4th and 6th columns.
people_csv.iloc[[0,4,7,21], [0,3,5]]

# Select the first 5 rows of the 3rd, 4th, and 5th columns of the DataFrame
people_csv.iloc[0:5, 2:5] 

# Modify the 'first_name' column value of the first row
people_csv.iloc[0, people_csv.columns.get_loc('first_name')] = 'Arya'
people_csv.head()

# Indexing
people_csv.set_index(people_csv['first_name'])
people_csv.head()

# Set the index as the 'first_name' column
people_csv.set_index(people_csv['first_name'], inplace=True)
people_csv.head()

# Sort by the index
people_csv = people_csv.sort_index()

# Select the row with the index 'Evan'
people_csv.loc['Evan']

# Slice the data to output a range of rows based on the index
people_csv.loc['Aleshia':'Svetlana'].head()

# Filter rows based on a column value conditional
people_csv.loc[people_csv['age'] == 39].head()

# Modify the 'first_name' value of the row with the index 'Yun'
people_csv.loc['Yun', 'first_name'] = 'Yuna'
people_csv.head()



# Import libraries and dependencies
import pandas as pd

# Set the file path
file_path = '../Resources/people.csv'

# Read in the CSV as a DataFrame
people_csv = pd.read_csv(file_path)
people_csv.head()

# Select the first row of the DataFrame


# Select the second row of the DataFrame


# Select the first 10 rows of the DataFrame


# Select the last row of the DataFrame


# Select the first column of the DataFrame


# Select the second column of the DataFrame, with all rows


# Select the last column of the DataFrame, with all rows


# Select the first two columns of the DataFrame, with all rows


# Select the 1st, 5th, 8th, 22nd rows of the 1st 4th and 6th columns.


# Select the first 5 rows of the 3rd, 4th, and 5th columns of the DataFrame


# Modify the 'first_name' column value of the first row


# Indexing


# Set the index as the 'first_name' column


# Sort by the index


# Select the row with the index 'Evan'


# Slice the data to output a range of rows based on the index


# Filter rows based on a column value conditional
people_csv.loc[people_csv['age'] == 39].head()

# Modify the 'first_name' value of the row with the index 'Yun'




# Dependencies
import pandas as pd
from pathlib import Path

# Load in file
# Store filepath in a variable
movie_file = Path("../Resources/movie_scores.csv")

# Read and display the CSV with Pandas
movie_file_df = pd.read_csv(movie_file)
movie_file_df.head()

# List all the columns in the table
movie_file_df.columns

# We only want IMDb data, so create a new table that takes the Film and all the columns relating to IMDB
imdb_df = movie_file_df[["FILM", "IMDB", "IMDB_norm",
                            "IMDB_norm_round", "IMDB_user_vote_count"]]
imdb_df.head()

# Alternate solution
# imdb_columns = ["FILM"] + [col for col in imdb_df.columns if "IMDB" in col]
# imbd_df = movie_file_df[imdb_columns]
# imdb_df.head()

# We only like good movies, so find those that scored over 7, and ignore the norm rating
good_movies_df = movie_file_df.loc[movie_file_df["IMDB"] > 7, [
    "FILM", "IMDB", "IMDB_user_vote_count"]]
good_movies_df.head()

# Find less popular movies--i.e., those with fewer than 20K votes
unknown_movies_df = good_movies_df.loc[good_movies_df["IMDB_user_vote_count"] < 20000, [
    "FILM", "IMDB", "IMDB_user_vote_count"]]
unknown_movies_df.head()

# Finally, export this file to a spread so we can keep track of out new future watch list without the index
unknown_movies_df.to_excel("output/movieWatchlist.xlsx", index=False)

# Dependencies
import pandas as pd
from pathlib import Path

# Load in file
# Store filepath in a variable
movie_file = Path("Resources/movie_scores.csv")

# Read and display the CSV with Pandas


# List all the columns in the table


# We only want IMDb data, so create a new table that takes the Film and all the columns relating to IMDB


# We only like good movies, so find those that scored over 7, and ignore the norm rating


# Find less popular movies--i.e., those with fewer than 20K votes


# Finally, export this file to a spread so we can keep track of out new future watch list without the index


# Import Dependencies
import pandas as pd
from pathlib import Path

# Import the comic_books_expanded.csv file as a DataFrame
original_csv_df = pd.read_csv("../Resources/comic_books_expanded.csv")
original_csv_df.head()

# Remove unnecessary columns from the DataFrame and save the new DataFrame
# Only keep: "ISBN", "Title", "Other titles", "Name", "All names", 
# "Country of publication", "Place of publication", "Publisher", "Date of publication"
reduced_df = original_csv_df[["ISBN", "Title", "Other titles", "Name", "All names", 
                       "Country of publication", "Place of publication", 
                       "Publisher", "Date of publication"]]
reduced_df.head()

# Rename the columns
comics_df = reduced_df.rename(columns={"Other titles": "Other Titles",
                                        "Name": "Author",
                                        "All names": "All Names",
                                        "Country of publication": "Country of Publication",
                                        "Place of publication": "Place of Publication",
                                        "Date of publication": "Publication Year", })
comics_df.head()

# How many comics were published in the 1960s?
comics_1960s_df = comics_df.loc[(comics_df['Publication Year'] >= 1960) & (comics_df['Publication Year'] < 1970)]
len(comics_1960s_df)

# Are there more batman comics or superman comics?
# To start, how many batman comics are listed?
batman_comic_df = comics_df.loc[comics_df['Title'].str.contains('batman', case = False)]
len(batman_comic_df)

# Alternate solution with function
def search_titles_for_string(search_term):
    search_df = comics_df[comics_df['Title'].str.contains(search_term, case=False)]
    return len(search_df)

search_titles_for_string("batman")

# How many superman comics are listed?
superman_comic_df = comics_df.loc[comics_df['Title'].str.contains('superman', case = False)]
len(superman_comic_df)

# Alternate solution using function
search_titles_for_string("superman")

# Import Dependencies
import pandas as pd
from pathlib import Path

# Import the comic_books_expanded.csv file as a DataFrame


# Remove unnecessary columns from the DataFrame and save the new DataFrame
# Only keep: "ISBN", "Title", "Other titles", "Name", "All names", 
# "Country of publication", "Place of publication", "Publisher", "Date of publication"


# Rename the columns


# How many comics were published in the 1960s?
comics_1960s_df = comics_df.loc[(comics_df['Publication Year'] >= 1960) & (comics_df['Publication Year'] < 1970)]
len(comics_1960s_df)

# Are there more batman comics or superman comics?
# To start, how many batman comics are listed?



# How many superman comics are listed?


import pandas as pd

# Import the data
file = "../Resources/lax_temperature.csv"
temperature_df = pd.read_csv(file)

# Show the first 5 rows
temperature_df.head()

# Rename the columns

renamed_df = temperature_df.rename(columns = {
    "STATION": "station",
    "DATE": "date",
    "REPORT_TYPE": "report_type",
    "HourlyDryBulbTemperature": "hourly_temp"
})
renamed_df

# How many reports are there of report type FM-16?

fm16_df = renamed_df.loc[renamed_df['report_type'] == 'FM-16']
len(fm16_df)

# How many readings measured a temp over 70?

over70_df = renamed_df.loc[renamed_df['hourly_temp'] > 70]
len(over70_df)

# What was the temperature for the 276th reading?

renamed_df.iloc[275, 3]

# What were the dates and report types of rows 500 through 505?
renamed_df.iloc[500:506, 1:3]

# Show the last 10 rows of the DataFrame
renamed_df.tail(10)

# Alternate
renamed_df.iloc[-10:]



import pandas as pd

# Import the data

# Show the first 5 rows

# Rename the columns


# How many reports are there of report type FM-16?


# How many readings measured a temp over 70?


# What was the temperature for the 276th reading?


# What were the dates and report types of rows 500 through 505?


# Show the last 10 rows of the DataFrame




# Import Dependencies
import pandas as pd

csv_path = "../Resources/VT_tax_statistics.csv"
taxes_df = pd.read_csv(csv_path)
taxes_df.head()

# Sorting the DataFrame based on "Meals" column
# Will sort from lowest to highest if no other parameter is passed
meals_taxes_df = taxes_df.sort_values("Meals")
meals_taxes_df.head()

# To sort from highest to lowest, ascending=False must be passed in
meals_taxes_df = taxes_df.sort_values("Meals", ascending=False)
meals_taxes_df.head()

# It is possible to sort based upon multiple columns
meals_and_rent_count_df = taxes_df.sort_values(
    ["Meals Count", "Rent Count"], ascending=False)
meals_and_rent_count_df.head(15)

# To see the sorting by multiple columns better, we can compare the last 
# DataFrame with a second column sort on "Alcohol Count"
# (Compare the order of the two "54" value Rent Count rows)
meals_and_alcohol_count_df = taxes_df.sort_values(
    ["Meals Count", "Alcohol Count"], ascending=False)
meals_and_alcohol_count_df.head(15)

# The index can be reset to provide index numbers based on the new rankings.
new_index_df = meals_and_alcohol_count_df.reset_index(drop=True)
new_index_df.head()



# Import Dependencies
import pandas as pd
from pathlib import Path

csv_path = "../Resources/VT_tax_statistics.csv"
taxes_df = pd.read_csv(csv_path)
taxes_df.head()

# Sorting the DataFrame based on "Meals" column
# Will sort from lowest to highest if no other parameter is passed


# To sort from highest to lowest, ascending=False must be passed in


# It is possible to sort based upon multiple columns


# To see the sorting by multiple columns better, we can compare the last 
# DataFrame with a second column sort on "Alcohol Count"
# (Compare the order of the two "54" value Rent Count rows)


# The index can be reset to provide index numbers based on the new rankings.




# Dependencies
import pandas as pd

# Name of the CSV file
file = '../Resources/donors2021_unclean.csv'

# The correct encoding must be used to read the CSV
df = pd.read_csv(file, encoding="ISO-8859-1")

# Preview of the DataFrame
# Note that Memo_CD is likely a meaningless column
df.head()

# Delete extraneous column
del df['Memo_CD']
df.head()

# Identify incomplete rows
df.count()

# Drop all rows with missing information
df = df.dropna(how='any')

# Verify dropped rows
df.count()

# The Zip column is the wrong data type. It should be a string (object).
df.dtypes

# Use df.astype() method to convert the datatype of the Zip column
df = df.astype({"Zip": str}, errors='raise')

# Verify that the Zip column datatype has been made an object
df['Zip'].dtype

# We can view all the unique values in a column

df['Employer'].unique()

# We can count the number of unique values
df['Employer'].nunique()

# We can count the number of occurrences
# for each unique value

df['Employer'].value_counts().head(10)

# Clean up Employer category. Replace 'SELF' and 'SELF EMPLOYED' with 'SELF-EMPLOYED'
df['Employer'] = df['Employer'].replace({'SELF': 'SELF-EMPLOYED', 'SELF EMPLOYED': 'SELF-EMPLOYED'})

# Verify clean-up.
df['Employer'].value_counts().head(10)

# Clean up Employer category. Replace 'NOT EMPLOYED' with 'UNEMPLOYED'
df['Employer'] = df['Employer'].replace({'NOT EMPLOYED': 'UNEMPLOYED'})
df['Employer'].value_counts()

# Display a statistical overview
df.describe()

# We can calculate individual aggregate functions per column
print("AMOUNT:")
print(f"The count is {df['Amount'].count()}")
print(f"The minimum is {df['Amount'].min()}")
print(f"The maximum is {df['Amount'].max()}")
print(f"The mean is {df['Amount'].mean()}")

# We can also calculate them for an entire DataFrame
df.max()

# Save the DataFrame to a CSV file. 
df.to_csv("../Resources/donors2021.csv", index=False, encoding="ISO-8859-1")

# More functions can be found at
# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html

# Dependencies
import pandas as pd

# Name of the CSV file
file = '../Resources/donors2021_unclean.csv'

# The correct encoding must be used to read the CSV
df = pd.read_csv(file, encoding="ISO-8859-1")

# Preview of the DataFrame
# Note that Memo_CD is likely a meaningless column


# Delete extraneous column


# Identify incomplete rows


# Drop all rows with missing information


# Verify dropped rows


# The Zip column is the wrong data type. It should be a string (object).


# Use df.astype() method to convert the datatype of the Zip column


# Verify that the Zip column datatype has been made an object


# We can view all the unique values in a column


# We can count the number of unique values


# We can count the number of occurrences
# for each unique value


# Clean up Employer category. Replace 'SELF' and 'SELF EMPLOYED' with 'SELF-EMPLOYED'


# Verify clean-up.


# Clean up Employer category. Replace 'NOT EMPLOYED' with 'UNEMPLOYED'


# Display a statistical overview


# We can calculate individual aggregate functions per column


# We can also calculate them for an entire DataFrame


# Save the DataFrame to a CSV file. 
# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html



# Import Dependencies
import pandas as pd

# Create reference to CSV file
csv_path = "../Resources/SFO_Airport_Utility_Consumption.csv"

# Import the CSV into a pandas DataFrame
consumption_df = pd.read_csv(csv_path)
consumption_df

# Collect a list of all the unique values in "Utility"
consumption_df["Utility"].unique()

# Create a new DataFrame that only includes electricity usage for tenant owned units.
electricity_df = consumption_df.loc[(consumption_df["Utility"] == "Electricity") &
                                    (consumption_df["Owner"] == "Tenant"), :]
electricity_df.head()

# Sort the DataFrame by the values in the "Usage" column to find the months
# with the highest usage
electricity_df = electricity_df.sort_values(by="Usage", ascending=False)

# Reset the index so that the index is now based on the sorting locations
electricity_df = electricity_df.reset_index(drop=True)

electricity_df.head()

# Save all of the information collected on the highest usage month
highest_month = electricity_df.iloc[0, :]
highest_month

# Show some basic summary statistics for the whole DataFrame
electricity_df.describe()

# Find the average usage

electricity_df['Usage'].mean()

# Find the total usage from all Augusts on record

electricity_df.loc[electricity_df['Month'] == 'Aug', 'Usage'].sum()



# Import Dependencies
import pandas as pd

# Create reference to CSV file

# Import the CSV into a pandas DataFrame


# Collect a list of all the unique values in "Utility"


# Create a new DataFrame that only includes electricity usage for tenant owned units.



# Sort the DataFrame by the values in the "Usage" column to find the months
# with the highest usage


# Reset the index so that the index is now based on the sorting locations


# Save all of the information collected on the highest usage month



# Show some basic summary statistics for the whole DataFrame



# Find the average usage



# Find the total usage from all Augusts on record





# Import dependencies
import numpy as np
import pandas as pd
import scipy.stats as sts

# Read in the LAX temperature data
temperature_df = pd.read_csv('../Resources/lax_temperature.csv')
temperatures = temperature_df['HourlyDryBulbTemperature']

# Calculate the measures of central tendency
mean_numpy = np.mean(temperatures)
print(f"The mean temperature at the LAX airport is {mean_numpy}")

median_numpy = np.median(temperatures)
print(f"The median temperature at the LAX airport is {median_numpy}")

mode_scipy = sts.mode(temperatures)
print(f"The mode temperature at the LAX airport is {mode_scipy}")

# Calculate variance and standard deviation using NumPy
variance = np.var(temperatures)
print(f"The population variance using the NumPy module is {variance}")

stand_dev = np.std(temperatures)
print(f"The population standard deviation using the NumPy module is {stand_dev}")

# Calculate z-scores using SciPy
z_scipy = sts.zscore(temperatures)
print(f"The z-scores using the SciPy module are {z_scipy}")

# Add the z-scores to the original DataFrame as a new column
temperature_df["z_score"] = z_scipy
temperature_df.head()

# Import dependencies
import numpy as np
import pandas as pd
import scipy.stats as sts

# Read in the LAX temperature data


# Calculate the measures of central tendency


# Calculate variance and standard deviation using NumPy


# Calculate z-scores using SciPy


# Add the z-scores to the original DataFrame as a new column


# Dependencies
import pandas as pd

# Read in the LAX temperatures dataset and create a box plot
temperature_df = pd.read_csv('../Resources/lax_temperature.csv')
temperatures = temperature_df['HourlyDryBulbTemperature']

temperatures.head()

# Use pandas to find potential outliers by calculating the interquartile range (IQR)
Q1 = temperatures.quantile(0.25)
median = temperatures.quantile(0.5)
Q3 = temperatures.quantile(0.75)
IQR = Q3 - Q1

print(f"The lower quartile of temperatures is: {Q1}")
print(f"The upper quartile of temperatures is: {Q3}")
print(f"The interquartile range of temperatures is: {IQR}")
print(f"The the median of temperatures is: {median} ")

lower_bound = Q1 - (1.5 * IQR)
upper_bound = Q3 + (1.5 * IQR)
print(f"Values below {lower_bound} could be outliers.")
print(f"Values above {upper_bound} could be outliers.")

# Create a DataFrame of rows that could be outliers
outlier_df = temperature_df.loc[(temperature_df['HourlyDryBulbTemperature'] < 45) |
                                (temperature_df['HourlyDryBulbTemperature'] > 69)]
outlier_df.head()

# How many potential outliers are there in the dataset?
len(outlier_df)



# Dependencies
import pandas as pd

# Read in the LAX temperatures dataset and create a box plot


# Use pandas to find potential outliers by calculating the interquartile range (IQR)



# Create a DataFrame of rows that could be outliers



# How many potential outliers are there in the dataset?




# Import dependencies
import numpy as np
import pandas as pd
import scipy.stats as sts

# Create a DataFrame from the cups_of_coffee data
cups_of_coffee = [1, 1, 0, 0, 3, 1, 3, 0, 2, 3, 2, 3, 0, 3, 2, 0, 3, 0, 2, 3, 2, 3, 0, 2, 0, 0, 1]
coffee_consumed = pd.DataFrame(cups_of_coffee, columns=["cups"])

# Calculate the mean, median, and mode of cups of coffee consumed daily
print("Mean: ", np.mean(coffee_consumed.cups))
print("Median: ", np.median(coffee_consumed.cups))
print("Mode: ", sts.mode(coffee_consumed.cups))

# Read in the California housing data set from the Resources folder
california_data = pd.read_csv('../Resources/California_Housing.csv')
len(california_data)

# Determine the most appropriate measure of central tendency to describe the Population column
print("Population Mean: ", california_data["Population"].mean())
print("Population Median: ", california_data["Population"].median())
print("Population Mode: ", california_data["Population"].mode())

# Determine if there are any potential outliers in the AveOccup column
avg_occup = california_data["AveOccup"]
Q1 = avg_occup.quantile(0.25)
median = avg_occup.quantile(0.5)
Q3 = avg_occup.quantile(0.75)
IQR = Q3 - Q1

print(f"The lower quartile of occupancy is: {Q1}")
print(f"The upper quartile of occupancy is: {Q3}")
print(f"The interquartile range of occupancy is: {IQR}")
print(f"The the median of occupancy is: {median} ")

lower_bound = Q1 - (1.5 * IQR)
upper_bound = Q3 + (1.5 * IQR)
print(f"Values below {lower_bound} could be outliers.")
print(f"Values above {upper_bound} could be outliers.")

# Create a new DataFrame by filtering the original DataFrame to show only the outliers
outlier_occupancy = california_data.loc[(avg_occup < lower_bound) | (avg_occup > upper_bound)]
outlier_occupancy

# Find the lowest and highest median income in the potential outliers
print(f"The minimum median income of the potential outliers is {outlier_occupancy['MedInc'].min()}")
print(f"The maximum median income of the potential outliers is {outlier_occupancy['MedInc'].max()}")

# Import dependencies
import numpy as np
import pandas as pd
import plotly.express as px
import scipy.stats as sts

# Create a DataFrame from the cups_of_coffee data
cups_of_coffee = [1, 1, 0, 0, 3, 1, 3, 0, 2, 3, 2, 3, 0, 3, 2, 0, 3, 0, 2, 3, 2, 3, 0, 2, 0, 0, 1]
coffee_consumed = pd.DataFrame(cups_of_coffee, columns=["cups"])

# Calculate the mean, median, and mode of cups of coffee consumed daily

# Create a histogram of cups of coffee consumed daily

# Read in the California housing data set from the Resources folder

# Determine the most appropriate measure of central tendency to describe the Population column

# Determine if there are any potential outliers in the AveOccup column

# Create a new DataFrame by filtering the original DataFrame to show only the outliers

# Find the lowest and highest median income in the potential outliers

# Dependencies
import pandas as pd

# Create a reference the CSV file desired
csv_path = "../Resources/CT_fires_2015.csv"

# Read the CSV into a Pandas DataFrame
fires_df = pd.read_csv(csv_path, low_memory=False)

# Print the first five rows of data to the screen
fires_df.head()

# Check the names of all the columns and see if there are any rows with missing data
fires_df.count()

# Rename mistyped columns "Aid Given or Received Code " and "Propery Loss"
fires_df = fires_df.rename(columns={"Aid Given or Received Code ": "Aid Given or Received Code", 
                                    "Propery Loss": "Property Loss"})

# Reduce to columns: Reporting Year, Fire Department Name, Incident date, Incident Type,
# Aid Given or Received Code, Aid Given or Received, Number of Alarms, Alarm Date and Time,
# Arrival Date and Time, Last Unit Cleared Date and Time, Actions Taken 1, Actions Taken 2,
# Actions Taken 3, Property Value, Property Loss, Contents Value, Contents Loss,
# Fire Service Deaths, Fire Service Injuries, Other Fire Deaths, Other Fire Injuries,
# Property Use, Incident Street Address, Incident Apartment Number, Incident City, Incident Zip Code

fires_reduced = fires_df[["Reporting Year", "Fire Department Name", "Incident date", 
                          "Incident Type", "Aid Given or Received Code", "Aid Given or Received", 
                          "Number of Alarms", "Alarm Date and Time", "Arrival Date and Time", 
                          "Last Unit Cleared Date and Time", "Actions Taken 1", 
                          "Actions Taken 2", "Actions Taken 3", "Property Value", 
                          "Property Loss", "Contents Value", "Contents Loss", "Fire Service Deaths", 
                          "Fire Service Injuries", "Other Fire Deaths", "Other Fire Injuries", 
                          "Property Use", "Incident Street Address", "Incident Apartment Number", 
                          "Incident City", "Incident Zip Code"]]
fires_reduced.head()

# Fill NAs for columns "Actions Taken 1", "Actions Taken 2", "Actions Taken 3", 
# and "Incident Apartment Number" with ''
# Fill NAs for columns "Other Fire Deaths", "Other Fire Injuries",
# "Property Loss", and "Contents Loss" with 0
fires_reduced = fires_reduced.fillna({"Actions Taken 1": '', 
                                      "Actions Taken 2": '', 
                                      "Actions Taken 3": '',
                                      "Incident Apartment Number": '',
                                      "Other Fire Deaths": 0,
                                      "Other Fire Injuries": 0,
                                      "Property Loss": 0,
                                      "Contents Loss": 0})
fires_reduced.head()

# Remove remaining rows with missing data
fires_cleaned_df = fires_reduced.dropna(how="any")
fires_cleaned_df.count()

# Filter data to incidents that caused Property or Contents Loss
loss_df = fires_cleaned_df.loc[(fires_cleaned_df["Property Loss"] > 0) |
                               (fires_cleaned_df["Contents Loss"] > 0) , :]
loss_df.head()

# Count how many incidents occured in each city
city_counts = loss_df["Incident City"].value_counts()
city_counts

# Convert the city_counts Series into a DataFrame
city_loss_counts_df = pd.DataFrame(city_counts)
city_loss_counts_df.head()

# Convert the column name into "Sum of Loss Incidents"
city_loss_counts_df = city_loss_counts_df.rename(
    columns={"Incident City": "Sum of Loss Incidents"})
city_loss_counts_df.head()

# Calculate the number of deaths from fire incidents where loss occurred
deaths = loss_df["Fire Service Deaths"].sum() + loss_df["Other Fire Deaths"].sum()
deaths

# Want to calculate the fire department response time? There is a problem
# Problem can be seen by examining datatypes within the DataFrame
loss_df.dtypes

# Convert relevant date columns to datetime
loss_df = loss_df.astype({"Incident date": "datetime64",
    "Alarm Date and Time": "datetime64",
    "Arrival Date and Time": "datetime64",
    "Last Unit Cleared Date and Time": "datetime64"})

loss_df.dtypes

# Now it is possible to find the response time in seconds
# Hint: create a new column for "Response Time (seconds)" and use .dt.total_seconds()
# to calculate the seconds
loss_df["Response Time (seconds)"] = loss_df["Arrival Date and Time"] - loss_df["Alarm Date and Time"]
loss_df["Response Time (seconds)"] = loss_df["Response Time (seconds)"].dt.total_seconds()
loss_df["Response Time (seconds)"] = loss_df["Response Time (seconds)"].astype("int")

# Check data for columns of your choosing
response_times = loss_df[["Fire Department Name", "Incident date", "Incident Type", "Arrival Date and Time",
        "Alarm Date and Time", "Response Time (seconds)"]]
response_times.head()



# Dependencies
import pandas as pd
from pathlib import Path

# Create a reference the CSV file desired
csv_path = Path("../Resources/CT_fires_2015.csv")

# Read the CSV into a Pandas DataFrame
fires_df = pd.read_csv(csv_path, low_memory=False)

# Print the first five rows of data to the screen
fires_df.head()

# Check the names of all the columns and see if there are any rows with missing data


# Rename mistyped columns "Aid Given or Received Code " and "Propery Loss"


# Reduce to columns: Reporting Year, Fire Department Name, Incident date, Incident Type,
# Aid Given or Received Code, Aid Given or Received, Number of Alarms, Alarm Date and Time,
# Arrival Date and Time, Last Unit Cleared Date and Time, Actions Taken 1, Actions Taken 2,
# Actions Taken 3, Property Value, Property Loss, Contents Value, Contents Loss,
# Fire Service Deaths, Fire Service Injuries, Other Fire Deaths, Other Fire Injuries,
# Property Use, Incident Street Address, Incident Apartment Number, Incident City, Incident Zip Code



# Fill NAs for columns "Actions Taken 1", "Actions Taken 2", "Actions Taken 3", 
# and "Incident Apartment Number" with ''
# Fill NAs for columns "Other Fire Deaths", "Other Fire Injuries",
# "Property Loss", and "Contents Loss" with 0


# Remove remaining rows with missing data


# Filter data to incidents that caused Property or Contents Loss


# Count how many incidents occured in each city


# Convert the city_counts Series into a DataFrame


# Convert the column name into "Sum of Loss Incidents"


# Calculate the number of deaths from fire incidents where loss occurred


# Want to calculate the fire department response time? There is a problem
# Problem can be seen by examining datatypes within the DataFrame


# Convert relevant date columns to datetime


# Now it is possible to find the response time in seconds
# Hint: create a new column for "Response Time (seconds)" and use .dt.total_seconds()
# to calculate the seconds


# Check data for columns of your choosing




# Dependencies
import pandas as pd
from pathlib import Path

# Save path to the data set as a variable.
data_file = Path("Resources/car_purchases.csv")

# Use Pandas to the read data.
data_file_df = pd.read_csv(data_file)
data_file_df.head()

# Display a statistical overview of the DataFrame.
data_file_df.describe()

# Reference a single column within a DataFrame.
data_file_df["Amount"].head()

# Reference multiple columns within a DataFrame.
data_file_df[["Amount", "Gender"]].head()

# The mean method averages the series
average = data_file_df["Amount"].mean()
average

# The sum method adds every entry in the series
total = data_file_df["Amount"].sum()
total

# The unique method shows every element only once
unique = data_file_df["Car"].unique()
unique

# The value_counts method counts unique values in a column
count = data_file_df["Gender"].value_counts()
count

# Calculations can also be performed on Series and added into DataFrames as new columns
thousands_of_dollars = data_file_df["Amount"]/1000
data_file_df["Thousands of Dollars"] = thousands_of_dollars

data_file_df.head()



# Dependencies
import pandas as pd
from pathlib import Path

# Save path to the data set as a variable.


# Use Pandas to read the data


# Display a statistical overview of the DataFrame.


# Reference a single column within a DataFrame.


# Reference multiple columns within a DataFrame.


# The mean method averages the series


# The sum method adds every entry in the series


# The unique method shows every element only once


# The value_counts method counts unique values in a column


# Calculations can also be performed on Series and added into DataFrames as new columns




# Import Dependencies
import pandas as pd

# A DataFrame of individuals' names, their trainers, their weight, and their days as gym members
training_df = pd.DataFrame({
    "Name":["Gino Walker","Hiedi Wasser","Kerrie Wetzel","Elizabeth Sackett","Jack Mitten","Madalene Wayman","Jamee Horvath","Arlena Reddin","Tula Levan","Teisha Dreier","Leslie Carrier","Arlette Hartson","Romana Merkle","Heath Viviani","Andres Zimmer","Allyson Osman","Yadira Caggiano","Jeanmarie Friedrichs","Leann Ussery","Bee Mom","Pandora Charland","Karena Wooten","Elizabet Albanese","Augusta Borjas","Erma Yadon","Belia Lenser","Karmen Sancho","Edison Mannion","Sonja Hornsby","Morgan Frei","Florencio Murphy","Christoper Hertel","Thalia Stepney","Tarah Argento","Nicol Canfield","Pok Moretti","Barbera Stallings","Muoi Kelso","Cicely Ritz","Sid Demelo","Eura Langan","Vanita An","Frieda Fuhr","Ernest Fitzhenry","Ashlyn Tash","Melodi Mclendon","Rochell Leblanc","Jacqui Reasons","Freeda Mccroy","Vanna Runk","Florinda Milot","Cierra Lecompte","Nancey Kysar","Latasha Dalton","Charlyn Rinaldi","Erline Averett","Mariko Hillary","Rosalyn Trigg","Sherwood Brauer","Hortencia Olesen","Delana Kohut","Geoffrey Mcdade","Iona Delancey","Donnie Read","Cesar Bhatia","Evia Slate","Kaye Hugo","Denise Vento","Lang Kittle","Sherry Whittenberg","Jodi Bracero","Tamera Linneman","Katheryn Koelling","Tonia Shorty","Misha Baxley","Lisbeth Goering","Merle Ladwig","Tammie Omar","Jesusa Avilla","Alda Zabala","Junita Dogan","Jessia Anglin","Peggie Scranton","Dania Clodfelter","Janis Mccarthy","Edmund Galusha","Tonisha Posey","Arvilla Medley","Briana Barbour","Delfina Kiger","Nia Lenig","Ricarda Bulow","Odell Carson","Nydia Clonts","Andree Resendez","Daniela Puma","Sherill Paavola","Gilbert Bloomquist","Shanon Mach","Justin Bangert","Arden Hokanson","Evelyne Bridge","Hee Simek","Ward Deangelis","Jodie Childs","Janis Boehme","Beaulah Glowacki","Denver Stoneham","Tarra Vinton","Deborah Hummell","Ulysses Neil","Kathryn Marques","Rosanna Dake","Gavin Wheat","Tameka Stoke","Janella Clear","Kaye Ciriaco","Suk Bloxham","Gracia Whaley","Philomena Hemingway","Claudette Vaillancourt","Olevia Piche","Trey Chiles","Idalia Scardina","Jenine Tremble","Herbert Krider","Alycia Schrock","Miss Weibel","Pearlene Neidert","Kina Callender","Charlotte Skelley","Theodora Harrigan","Sydney Shreffler","Annamae Trinidad","Tobi Mumme","Rosia Elliot","Debbra Putt","Rena Delosantos","Genna Grennan","Nieves Huf","Berry Lugo","Ayana Verdugo","Joaquin Mazzei","Doris Harmon","Patience Poss","Magaret Zabel","Marylynn Hinojos","Earlene Marcantel","Yuki Evensen","Rema Gay","Delana Haak","Patricia Fetters","Vinnie Elrod","Octavia Bellew","Burma Revard","Lakenya Kato","Vinita Buchner","Sierra Margulies","Shae Funderburg","Jenae Groleau","Louetta Howie","Astrid Duffer","Caron Altizer","Kymberly Amavisca","Mohammad Diedrich","Thora Wrinkle","Bethel Wiemann","Patria Millet","Eldridge Burbach","Alyson Eddie","Zula Hanna","Devin Goodwin","Felipa Kirkwood","Kurtis Kempf","Kasey Lenart","Deena Blankenship","Kandra Wargo","Sherrie Cieslak","Ron Atha","Reggie Barreiro","Daria Saulter","Tandra Eastman","Donnell Lucious","Talisha Rosner","Emiko Bergh","Terresa Launius","Margy Hoobler","Marylou Stelling","Lavonne Justice","Kala Langstaff","China Truett","Louanne Dussault","Thomasena Samaniego","Charlesetta Tarbell","Fatimah Lade","Malisa Cantero","Florencia Litten","Francina Fraise","Patsy London","Deloris Mclaughlin"],
    "Trainer":['Bettyann Savory','Mariah Barberio','Gordon Perrine','Pa Dargan','Blanch Victoria','Aldo Byler','Aldo Byler','Williams Camire','Junie Ritenour','Gordon Perrine','Bettyann Savory','Mariah Barberio','Aldo Byler','Barton Stecklein','Bettyann Savory','Barton Stecklein','Gordon Perrine','Pa Dargan','Aldo Byler','Brittani Brin','Bettyann Savory','Phyliss Houk','Bettyann Savory','Junie Ritenour','Aldo Byler','Calvin North','Brittani Brin','Junie Ritenour','Blanch Victoria','Brittani Brin','Bettyann Savory','Blanch Victoria','Mariah Barberio','Bettyann Savory','Blanch Victoria','Brittani Brin','Junie Ritenour','Pa Dargan','Gordon Perrine','Phyliss Houk','Pa Dargan','Mariah Barberio','Phyliss Houk','Phyliss Houk','Calvin North','Williams Camire','Brittani Brin','Gordon Perrine','Bettyann Savory','Bettyann Savory','Pa Dargan','Phyliss Houk','Barton Stecklein','Blanch Victoria','Coleman Dunmire','Phyliss Houk','Blanch Victoria','Pa Dargan','Harland Coolidge','Calvin North','Bettyann Savory','Phyliss Houk','Bettyann Savory','Harland Coolidge','Gordon Perrine','Junie Ritenour','Harland Coolidge','Blanch Victoria','Mariah Barberio','Coleman Dunmire','Aldo Byler','Bettyann Savory','Gordon Perrine','Bettyann Savory','Barton Stecklein','Harland Coolidge','Aldo Byler','Aldo Byler','Pa Dargan','Junie Ritenour','Brittani Brin','Junie Ritenour','Gordon Perrine','Mariah Barberio','Mariah Barberio','Mariah Barberio','Bettyann Savory','Brittani Brin','Aldo Byler','Phyliss Houk','Blanch Victoria','Pa Dargan','Phyliss Houk','Brittani Brin','Barton Stecklein','Coleman Dunmire','Bettyann Savory','Bettyann Savory','Gordon Perrine','Blanch Victoria','Junie Ritenour','Phyliss Houk','Coleman Dunmire','Williams Camire','Harland Coolidge','Williams Camire','Aldo Byler','Harland Coolidge','Gordon Perrine','Brittani Brin','Coleman Dunmire','Calvin North','Phyliss Houk','Brittani Brin','Aldo Byler','Bettyann Savory','Brittani Brin','Gordon Perrine','Calvin North','Harland Coolidge','Coleman Dunmire','Harland Coolidge','Aldo Byler','Junie Ritenour','Blanch Victoria','Harland Coolidge','Blanch Victoria','Junie Ritenour','Harland Coolidge','Junie Ritenour','Gordon Perrine','Brittani Brin','Coleman Dunmire','Williams Camire','Junie Ritenour','Brittani Brin','Calvin North','Barton Stecklein','Barton Stecklein','Mariah Barberio','Coleman Dunmire','Bettyann Savory','Mariah Barberio','Pa Dargan','Barton Stecklein','Coleman Dunmire','Brittani Brin','Barton Stecklein','Pa Dargan','Barton Stecklein','Junie Ritenour','Bettyann Savory','Williams Camire','Pa Dargan','Calvin North','Williams Camire','Coleman Dunmire','Aldo Byler','Barton Stecklein','Coleman Dunmire','Blanch Victoria','Mariah Barberio','Mariah Barberio','Harland Coolidge','Barton Stecklein','Phyliss Houk','Pa Dargan','Bettyann Savory','Barton Stecklein','Harland Coolidge','Junie Ritenour','Pa Dargan','Mariah Barberio','Blanch Victoria','Williams Camire','Phyliss Houk','Phyliss Houk','Coleman Dunmire','Mariah Barberio','Gordon Perrine','Coleman Dunmire','Brittani Brin','Pa Dargan','Coleman Dunmire','Brittani Brin','Blanch Victoria','Coleman Dunmire','Gordon Perrine','Coleman Dunmire','Aldo Byler','Aldo Byler','Mariah Barberio','Williams Camire','Phyliss Houk','Aldo Byler','Williams Camire','Aldo Byler','Williams Camire','Coleman Dunmire','Phyliss Houk'],
    "Weight":[128,180,193,177,237,166,224,208,177,241,114,161,162,151,220,142,193,193,124,130,132,141,190,239,213,131,172,127,184,157,215,122,181,240,218,205,239,217,234,158,180,131,194,171,177,110,117,114,217,123,248,189,198,127,182,121,224,111,151,170,188,150,137,231,222,186,139,175,178,246,150,154,129,216,144,198,228,183,173,129,157,199,186,232,172,157,246,239,214,161,132,208,187,224,164,177,175,224,219,235,112,241,243,179,208,196,131,207,182,233,191,162,173,197,190,182,231,196,196,143,250,174,138,135,164,204,235,192,114,179,215,127,185,213,250,213,153,217,176,190,119,167,118,208,113,206,200,236,159,218,168,159,156,183,121,203,215,209,179,219,174,220,129,188,217,250,166,157,112,236,182,144,189,243,238,147,165,115,160,134,245,174,238,157,150,184,174,134,134,248,199,165,117,119,162,112,170,224,247,217],
    "Membership (Days)":[52,70,148,124,186,157,127,155,37,185,158,129,93,69,124,13,76,153,164,161,48,121,167,69,39,163,7,34,176,169,108,162,195,86,155,77,197,200,80,142,179,67,58,145,188,147,125,15,13,173,125,4,61,29,132,110,62,137,197,135,162,174,32,151,149,65,18,42,63,62,104,200,189,40,38,199,1,12,8,2,195,30,7,72,130,144,2,34,200,143,43,196,22,115,171,54,143,59,14,52,109,115,187,185,26,19,178,18,120,169,45,52,130,69,168,178,96,22,78,152,39,51,118,130,60,156,108,69,103,158,165,142,86,91,117,77,57,169,86,188,97,111,22,83,81,177,163,35,12,164,21,181,171,138,22,107,58,51,38,128,19,193,157,13,104,89,13,10,26,190,179,101,7,159,100,49,120,109,56,199,51,108,47,171,69,162,74,119,148,88,32,159,65,146,140,171,88,18,59,13]
})
training_df.head()

# Collect a summary of all numeric data
training_df.describe()

# Find the names of the trainers
training_df["Trainer"].unique()

# Find how many students each trainer has
training_df["Trainer"].value_counts()

# Find the average weight of all students
training_df["Weight"].mean()

# Find the combined weight of all students
training_df["Weight"].sum()

# Convert the membership days into weeks and then adding a column to the DataFrame
weeks = training_df["Membership (Days)"]/7
training_df["Membership (Weeks)"] = weeks

training_df.head()

# Import Dependencies
import pandas as pd

# A DataFrame of individuals' names, their trainers, their weight, and their days as gym members
training_df = pd.DataFrame({
    "Name":["Gino Walker","Hiedi Wasser","Kerrie Wetzel","Elizabeth Sackett","Jack Mitten","Madalene Wayman","Jamee Horvath","Arlena Reddin","Tula Levan","Teisha Dreier","Leslie Carrier","Arlette Hartson","Romana Merkle","Heath Viviani","Andres Zimmer","Allyson Osman","Yadira Caggiano","Jeanmarie Friedrichs","Leann Ussery","Bee Mom","Pandora Charland","Karena Wooten","Elizabet Albanese","Augusta Borjas","Erma Yadon","Belia Lenser","Karmen Sancho","Edison Mannion","Sonja Hornsby","Morgan Frei","Florencio Murphy","Christoper Hertel","Thalia Stepney","Tarah Argento","Nicol Canfield","Pok Moretti","Barbera Stallings","Muoi Kelso","Cicely Ritz","Sid Demelo","Eura Langan","Vanita An","Frieda Fuhr","Ernest Fitzhenry","Ashlyn Tash","Melodi Mclendon","Rochell Leblanc","Jacqui Reasons","Freeda Mccroy","Vanna Runk","Florinda Milot","Cierra Lecompte","Nancey Kysar","Latasha Dalton","Charlyn Rinaldi","Erline Averett","Mariko Hillary","Rosalyn Trigg","Sherwood Brauer","Hortencia Olesen","Delana Kohut","Geoffrey Mcdade","Iona Delancey","Donnie Read","Cesar Bhatia","Evia Slate","Kaye Hugo","Denise Vento","Lang Kittle","Sherry Whittenberg","Jodi Bracero","Tamera Linneman","Katheryn Koelling","Tonia Shorty","Misha Baxley","Lisbeth Goering","Merle Ladwig","Tammie Omar","Jesusa Avilla","Alda Zabala","Junita Dogan","Jessia Anglin","Peggie Scranton","Dania Clodfelter","Janis Mccarthy","Edmund Galusha","Tonisha Posey","Arvilla Medley","Briana Barbour","Delfina Kiger","Nia Lenig","Ricarda Bulow","Odell Carson","Nydia Clonts","Andree Resendez","Daniela Puma","Sherill Paavola","Gilbert Bloomquist","Shanon Mach","Justin Bangert","Arden Hokanson","Evelyne Bridge","Hee Simek","Ward Deangelis","Jodie Childs","Janis Boehme","Beaulah Glowacki","Denver Stoneham","Tarra Vinton","Deborah Hummell","Ulysses Neil","Kathryn Marques","Rosanna Dake","Gavin Wheat","Tameka Stoke","Janella Clear","Kaye Ciriaco","Suk Bloxham","Gracia Whaley","Philomena Hemingway","Claudette Vaillancourt","Olevia Piche","Trey Chiles","Idalia Scardina","Jenine Tremble","Herbert Krider","Alycia Schrock","Miss Weibel","Pearlene Neidert","Kina Callender","Charlotte Skelley","Theodora Harrigan","Sydney Shreffler","Annamae Trinidad","Tobi Mumme","Rosia Elliot","Debbra Putt","Rena Delosantos","Genna Grennan","Nieves Huf","Berry Lugo","Ayana Verdugo","Joaquin Mazzei","Doris Harmon","Patience Poss","Magaret Zabel","Marylynn Hinojos","Earlene Marcantel","Yuki Evensen","Rema Gay","Delana Haak","Patricia Fetters","Vinnie Elrod","Octavia Bellew","Burma Revard","Lakenya Kato","Vinita Buchner","Sierra Margulies","Shae Funderburg","Jenae Groleau","Louetta Howie","Astrid Duffer","Caron Altizer","Kymberly Amavisca","Mohammad Diedrich","Thora Wrinkle","Bethel Wiemann","Patria Millet","Eldridge Burbach","Alyson Eddie","Zula Hanna","Devin Goodwin","Felipa Kirkwood","Kurtis Kempf","Kasey Lenart","Deena Blankenship","Kandra Wargo","Sherrie Cieslak","Ron Atha","Reggie Barreiro","Daria Saulter","Tandra Eastman","Donnell Lucious","Talisha Rosner","Emiko Bergh","Terresa Launius","Margy Hoobler","Marylou Stelling","Lavonne Justice","Kala Langstaff","China Truett","Louanne Dussault","Thomasena Samaniego","Charlesetta Tarbell","Fatimah Lade","Malisa Cantero","Florencia Litten","Francina Fraise","Patsy London","Deloris Mclaughlin"],
    "Trainer":['Bettyann Savory','Mariah Barberio','Gordon Perrine','Pa Dargan','Blanch Victoria','Aldo Byler','Aldo Byler','Williams Camire','Junie Ritenour','Gordon Perrine','Bettyann Savory','Mariah Barberio','Aldo Byler','Barton Stecklein','Bettyann Savory','Barton Stecklein','Gordon Perrine','Pa Dargan','Aldo Byler','Brittani Brin','Bettyann Savory','Phyliss Houk','Bettyann Savory','Junie Ritenour','Aldo Byler','Calvin North','Brittani Brin','Junie Ritenour','Blanch Victoria','Brittani Brin','Bettyann Savory','Blanch Victoria','Mariah Barberio','Bettyann Savory','Blanch Victoria','Brittani Brin','Junie Ritenour','Pa Dargan','Gordon Perrine','Phyliss Houk','Pa Dargan','Mariah Barberio','Phyliss Houk','Phyliss Houk','Calvin North','Williams Camire','Brittani Brin','Gordon Perrine','Bettyann Savory','Bettyann Savory','Pa Dargan','Phyliss Houk','Barton Stecklein','Blanch Victoria','Coleman Dunmire','Phyliss Houk','Blanch Victoria','Pa Dargan','Harland Coolidge','Calvin North','Bettyann Savory','Phyliss Houk','Bettyann Savory','Harland Coolidge','Gordon Perrine','Junie Ritenour','Harland Coolidge','Blanch Victoria','Mariah Barberio','Coleman Dunmire','Aldo Byler','Bettyann Savory','Gordon Perrine','Bettyann Savory','Barton Stecklein','Harland Coolidge','Aldo Byler','Aldo Byler','Pa Dargan','Junie Ritenour','Brittani Brin','Junie Ritenour','Gordon Perrine','Mariah Barberio','Mariah Barberio','Mariah Barberio','Bettyann Savory','Brittani Brin','Aldo Byler','Phyliss Houk','Blanch Victoria','Pa Dargan','Phyliss Houk','Brittani Brin','Barton Stecklein','Coleman Dunmire','Bettyann Savory','Bettyann Savory','Gordon Perrine','Blanch Victoria','Junie Ritenour','Phyliss Houk','Coleman Dunmire','Williams Camire','Harland Coolidge','Williams Camire','Aldo Byler','Harland Coolidge','Gordon Perrine','Brittani Brin','Coleman Dunmire','Calvin North','Phyliss Houk','Brittani Brin','Aldo Byler','Bettyann Savory','Brittani Brin','Gordon Perrine','Calvin North','Harland Coolidge','Coleman Dunmire','Harland Coolidge','Aldo Byler','Junie Ritenour','Blanch Victoria','Harland Coolidge','Blanch Victoria','Junie Ritenour','Harland Coolidge','Junie Ritenour','Gordon Perrine','Brittani Brin','Coleman Dunmire','Williams Camire','Junie Ritenour','Brittani Brin','Calvin North','Barton Stecklein','Barton Stecklein','Mariah Barberio','Coleman Dunmire','Bettyann Savory','Mariah Barberio','Pa Dargan','Barton Stecklein','Coleman Dunmire','Brittani Brin','Barton Stecklein','Pa Dargan','Barton Stecklein','Junie Ritenour','Bettyann Savory','Williams Camire','Pa Dargan','Calvin North','Williams Camire','Coleman Dunmire','Aldo Byler','Barton Stecklein','Coleman Dunmire','Blanch Victoria','Mariah Barberio','Mariah Barberio','Harland Coolidge','Barton Stecklein','Phyliss Houk','Pa Dargan','Bettyann Savory','Barton Stecklein','Harland Coolidge','Junie Ritenour','Pa Dargan','Mariah Barberio','Blanch Victoria','Williams Camire','Phyliss Houk','Phyliss Houk','Coleman Dunmire','Mariah Barberio','Gordon Perrine','Coleman Dunmire','Brittani Brin','Pa Dargan','Coleman Dunmire','Brittani Brin','Blanch Victoria','Coleman Dunmire','Gordon Perrine','Coleman Dunmire','Aldo Byler','Aldo Byler','Mariah Barberio','Williams Camire','Phyliss Houk','Aldo Byler','Williams Camire','Aldo Byler','Williams Camire','Coleman Dunmire','Phyliss Houk'],
    "Weight":[128,180,193,177,237,166,224,208,177,241,114,161,162,151,220,142,193,193,124,130,132,141,190,239,213,131,172,127,184,157,215,122,181,240,218,205,239,217,234,158,180,131,194,171,177,110,117,114,217,123,248,189,198,127,182,121,224,111,151,170,188,150,137,231,222,186,139,175,178,246,150,154,129,216,144,198,228,183,173,129,157,199,186,232,172,157,246,239,214,161,132,208,187,224,164,177,175,224,219,235,112,241,243,179,208,196,131,207,182,233,191,162,173,197,190,182,231,196,196,143,250,174,138,135,164,204,235,192,114,179,215,127,185,213,250,213,153,217,176,190,119,167,118,208,113,206,200,236,159,218,168,159,156,183,121,203,215,209,179,219,174,220,129,188,217,250,166,157,112,236,182,144,189,243,238,147,165,115,160,134,245,174,238,157,150,184,174,134,134,248,199,165,117,119,162,112,170,224,247,217],
    "Membership (Days)":[52,70,148,124,186,157,127,155,37,185,158,129,93,69,124,13,76,153,164,161,48,121,167,69,39,163,7,34,176,169,108,162,195,86,155,77,197,200,80,142,179,67,58,145,188,147,125,15,13,173,125,4,61,29,132,110,62,137,197,135,162,174,32,151,149,65,18,42,63,62,104,200,189,40,38,199,1,12,8,2,195,30,7,72,130,144,2,34,200,143,43,196,22,115,171,54,143,59,14,52,109,115,187,185,26,19,178,18,120,169,45,52,130,69,168,178,96,22,78,152,39,51,118,130,60,156,108,69,103,158,165,142,86,91,117,77,57,169,86,188,97,111,22,83,81,177,163,35,12,164,21,181,171,138,22,107,58,51,38,128,19,193,157,13,104,89,13,10,26,190,179,101,7,159,100,49,120,109,56,199,51,108,47,171,69,162,74,119,148,88,32,159,65,146,140,171,88,18,59,13]
})
training_df.head()

# Collect a summary of all numeric data


# Find the names of the trainers


# Find how many students each trainer has


# Find the average weight of all students


# Find the combined weight of all students


# Convert the membership days into weeks and then add a column to the DataFrame


import pandas as pd

# Import the data
file = '../Resources/donors2021.csv'
donors_df = pd.read_csv(file)
donors_df.head()

# If 10% of every donation was fully matched by an 
# anonymous philanthropist, making a new column would be easy.

donors_df['Match Amount'] = donors_df['Amount'] * 0.1
donors_df.head()



# What if the match percentage changed based on the amount donated?
# 10% on donations below $500, and 20% on donations of $500 or more.
# We need a new solution! A perfect opportunity for 'apply'.

# Define a function
def match_amount(amount):
    if amount < 500:
        return amount * 0.1
    return amount * 0.2

# "Apply" the function to the amount column
donors_df['Match Amount'] = donors_df['Amount'].apply(match_amount)
donors_df.head()

# Apply can also use values from multiple columns by
# setting the axis argument to 1. Suppose the donor 
# was only matching donations from Delaware.

def match_amount(row):
    if (row['State'] != 'DE'):
        return 0
    if (row['Amount'] < 500):
        return row['Amount'] * 0.1
    return row['Amount'] * 0.2

# "Apply" the function to the DataFrame
donors_df['Match Amount'] = donors_df.apply(match_amount, axis = 1)
donors_df.head()

# Finally, apply can also be used with lambda functions.

donors_df['Match Amount'] = donors_df['Amount'].apply(lambda x: x * 0.1 if x < 500 else x * 0.2)
donors_df.head()



import pandas as pd

# Import the data
file = '../Resources/donors2021.csv'
donors_df = pd.read_csv(file)
donors_df.head()

# If 10% of every donation was fully matched by an 
# anonymous philanthropist, making a new column would be easy.
donors_df['Match Amount'] = donors_df['Amount'] * 0.1
donors_df.head()


# What if the match percentage changed based on the amount donated?
# 10% on donations below $500, and 20% on donations of $500 or more.
# We need a new solution! A perfect opportunity for 'apply'.

# Define a function

# "Apply" the function to the amount column


# Apply can also use values from multiple columns by
# setting the axis argument to 1. Suppose the donor 
# was only matching donations from Delaware.

# Define the function

# "Apply" the function to the DataFrame


# Finally, apply can also be used with lambda functions.





import pandas as pd

# Import the data
file = '../Resources/SFO_Airport_Utility_Consumption.csv'
utilities_df = pd.read_csv(file)
utilities_df.head()

# Add a column that tracks the tax rate
# Assume every year and type of utility had 
# a tax rate of 5%, except for 2019 when the 
# tax was raised to 5.5%

# Define a function
def tax_rate(year):
    return 0.055 if year >= 2019 else 0.05

# Apply the function to the Year column
utilities_df['Tax Rate'] = utilities_df['Year'].apply(tax_rate)
utilities_df.tail()

# Recalculate the tax rate assuming that
# commission owned units were taxed an
# additional 1% on Electricity.

# Define a function
def tax_rate(row):
    rate = 0.05
    if row['Year'] == 2019:
        rate += 0.005
    if (row['Owner'] == 'Commission') & (row['Utility'] == 'Electricity'):
        rate += 0.01
    return rate

# Apply the function to the DataFrame
utilities_df['Tax Rate'] = utilities_df.apply(tax_rate, axis = 1)
utilities_df.tail()



# Use apply with a lambda function to set
# the existing Tax Rate column to 0 if
# the utility was "Passengers"

# Apply a lambda function
utilities_df['Tax Rate'] = utilities_df\
            .apply(lambda x: 0 if x['Utility'] == 'Passengers' else x['Tax Rate'],
                axis = 1)

utilities_df.head()



import pandas as pd

# Import the data
file = '../Resources/SFO_Airport_Utility_Consumption.csv'
utilities_df = pd.read_csv(file)
utilities_df.head()

# Add a column that tracks the tax rate
# Assume every year and type of utility had 
# a tax rate of 5%, except for 2019 when the 
# tax was raised to 5.5%

# Define a function

# Apply the function to the Year column


# Recalculate the tax rate assuming that
# commission owned units were taxed an
# additional 1% on Electricity.

# Define a function

# Apply the function to the DataFrame




# Use apply with a lambda function to set
# the existing Tax Rate column to 0 if
# the utility was "Passengers"

# Apply a lambda function




# Import Libraries
import pandas as pd

csv_path = "../Resources/stock_data.csv"
csv_data = pd.read_csv(csv_path)

csv_data.shape

csv_data.head()

csv_data.count()

csv_data.isnull().mean() * 100

csv_data = csv_data.dropna()

csv_data.isnull().sum()

csv_data['ebitda'] = csv_data['ebitda'].fillna(0)
csv_data['ebitda'].isnull().sum()

csv_data = csv_data.drop_duplicates()

csv_data['price'] = csv_data['price'].str.replace('$', '')
csv_data['price']

csv_data['price'].dtype

csv_data['price'] = csv_data['price'].astype('float')
csv_data['price'].dtype

# Import Libraries
import pandas as pd

























import pandas as pd

# Import the data
file = '../Resources/SFO_Airport_Utility_Consumption.csv'
utilities_df = pd.read_csv(file)

utilities_df.head()

# Data Collection and Cleaning
# Select only the needed columns
utils_df_cleaning = utilities_df[['Year', 'Utility', 'Units', 'Usage']].copy()
utils_df_cleaning.head()

# Scale the Usage column to be more readable
# Rows with "Water" as the utility can be left as is
def scale_to_millions(row):
    if row['Utility'] == 'Water':
        return row['Usage']
    return row['Usage'] / 1000000

utils_df_cleaning['Usage'] = utils_df_cleaning.apply(scale_to_millions, axis = 1)
utils_df_cleaning.head()

# Alter the Units column to reflect the changes
def millions_of_units(row):
    if row['Utility'] == 'Water':
        return row['Units']
    return 'Million ' + row['Units']
    
utils_df_cleaning['Units'] = utils_df_cleaning.apply(millions_of_units, axis = 1)
utils_df_cleaning.head()

# Combine the Utility and Units columns
# by putting Units in parentheses
def combine_utility_and_units(row):
    return f"{row['Utility']} ({row['Units']})"

utils_df_cleaning['Utility'] = utils_df_cleaning.apply(combine_utility_and_units, axis = 1)
utils_df_cleaning.head()


# Create two new DataFrames with data from 2013
# and 2018 that each contain only the Utility and 
# Usage column. Reset the index  for each 
# DataFrame to Utility
utils_2013_df_cleaned = utils_df_cleaning\
                .loc[utils_df_cleaning['Year'] == 2013, ['Utility', 'Usage']]\
                .copy()\
                .set_index('Utility')
utils_2018_df_cleaned = utils_df_cleaning\
                .loc[utils_df_cleaning['Year'] == 2018, ['Utility', 'Usage']]\
                .copy()\
                .set_index('Utility')

utils_2018_df_cleaned.head()

# Analyze

# Calculate the totals for each utility
utilities = utils_2013_df_cleaned.index.unique()
totals_2013 = {'Period': 2013}
totals_2018 = {'Period': 2018}
for utility in utilities:
    totals_2013[utility] = utils_2013_df_cleaned.loc[utility, 'Usage'].sum()
    totals_2018[utility] = utils_2018_df_cleaned.loc[utility, 'Usage'].sum()

print(totals_2013)
print(totals_2018)



# Calculate the change per utility as a percentage
# of each utility's 2013 total.

def get_percentage(original, final):
    return round((final - original) / original, 3) * 100

data = []
for utility in totals_2013.keys():
    if utility == 'Period':
        continue
    original = totals_2013[utility]
    final = totals_2018[utility]
    row = {
        'Utility': utility,
        '2013': round(original, 1),
        '2018': round(final, 1),
        'Difference': round(final - original, 1),
        'Change %': get_percentage(original, final)
    }
    data.append(row)



# Create a dataframe with the results
summary_df = pd.DataFrame(data)
summary_df


# Set the index to the utility column
summary_df = summary_df.set_index('Utility')

# Sort the rows based on Change %
summary_df = summary_df.sort_values('Change %', ascending=False)
summary_df



import pandas as pd

# Import the data
file = '../Resources/SFO_Airport_Utility_Consumption.csv'
utilities_df = pd.read_csv(file)

utilities_df.head()



import pandas as pd

# Import the data
file = '../Resources/SFO_Airport_Utility_Consumption.csv'
utilities_df = pd.read_csv(file)

utilities_df.head()

# Data Collection and Cleaning
# Select only the needed columns


# Scale the Usage column to be more readable
# Rows with "Water" as the utility can be left as is


# Alter the Units column to reflect the changes


# Combine the Utility and Units columns
# by putting Units in parentheses


# Create two new DataFrames with data from 2013
# and 2018 that each contain only the Utility and 
# Usage column. Reset the index  for each 
# DataFrame to Utility


# Analyze

# Calculate the totals for each utility



# Calculate the change per utility as a percentage
# of each utility's 2013 total.





# Create a dataframe with the results



# Set the index to the utility column

# Sort the rows based on Change %




import pandas as pd
from pathlib import Path

# Import data
apple_path = Path('Resources/AAPL.csv')
google_path = Path('Resources/GOOG.csv')
meta_path = Path('Resources/META.csv')

# Read in data and index by date
apple_data = pd.read_csv(apple_path, index_col='Date')
goog_data = pd.read_csv(google_path, index_col='Date')
meta_data = pd.read_csv(meta_path, index_col='Date')

# Show sample of Apple data
apple_data.head()

# Show sample of Google data
goog_data.head()

# Show sample of Meta data
meta_data.head()

# Join by rows axis
joined_data_rows = pd.concat([apple_data, goog_data, meta_data], axis="rows", join="inner")
joined_data_rows.head(30)

# Join by columns axis
joined_data_cols = pd.concat([apple_data, goog_data, meta_data], axis='columns', join='inner')
joined_data_cols.head(15)

# Join by rows and add the stock ticker as the key.
joined_data_rows = pd.concat([apple_data, goog_data, meta_data], axis="rows",
                             join="inner", keys=['Apple','Google', 'Meta'] )

joined_data_rows.head(10)

joined_data_rows.tail(30)

# Join by columns and add the stock ticker as the key.
joined_data_cols = pd.concat([apple_data, goog_data, meta_data], axis="columns",
                             join="inner", keys=['Apple','Google', 'Meta'] )
joined_data_cols.head(15)



import pandas as pd
from pathlib import Path

# Import data

apple_path = Path('Resources/AAPL.csv')
google_path = Path('Resources/GOOG.csv')
meta_path = Path('Resources/META.csv')

# Read in data and index by date
apple_data = pd.read_csv(apple_path, index_col='Date')
goog_data = pd.read_csv(google_path, index_col='Date')
meta_data = pd.read_csv(meta_path, index_col='Date')

# Show sample of Apple data


# Show sample of Google data


# Show sample of Meta data


# Join by rows axis


# Join by columns axis


# Join by rows and add the stock ticker as the key.




# Join by columns and add the stock ticker as the key.




# Import Libraries and Dependencies
import pandas as pd
from pathlib import Path

# Import data
france_data_path = Path('Resources/france_products.csv')
uk_data_path = Path('Resources/uk_products.csv')
netherlands_data_path = Path('Resources/netherlands_products.csv')
customer_data_path = Path('Resources/customer_info.csv')
products_data_path = Path('Resources/products.csv')

# Read in data and index by CustomerID
france_data = pd.read_csv(france_data_path, index_col='CustomerID')
uk_data = pd.read_csv(uk_data_path, index_col='CustomerID')
netherlands_data = pd.read_csv(netherlands_data_path, index_col='CustomerID')
customer_data = pd.read_csv(customer_data_path, index_col='CustomerID')
products_data = pd.read_csv(products_data_path, index_col='CustomerID')

# Show the first five rows of France data
france_data.head()

# Show the first five rows of UK data
uk_data.head()

# Show the first five rows of Netherlands data
netherlands_data.head()

# Join UK, France, and Netherlands full datasets by axis
joined_data_rows = pd.concat([france_data, uk_data, netherlands_data], axis="rows", join="inner")
joined_data_rows

# Join UK, France, and Netherlands full datasets by axis and add the countries to the index.
joined_data_rows = pd.concat([france_data, uk_data, netherlands_data],
                             axis="rows", join="inner", keys=["France","United Kingdom", "Netherlands"])

# Drop the Country column and display the updated DataFrame.
joined_data_rows_keys = joined_data_rows.drop(['Country'],axis=1)
joined_data_rows_keys.head(20)

# Show the customer data
customer_data

# Show the products data
products_data

# Join the customer and products by columns axis
joined_data_cols = pd.concat([customer_data, products_data], axis='columns', join='inner')
joined_data_cols.head()



import pandas as pd
from pathlib import Path

# Import data


# Read in data and make the index the CustomerID


# Show the first five rows of France data


# Show the first five rows of UK data


# Show the first five rows of Netherlands data


# Join UK, France, and Netherlands full datasets by axis


# Join UK, France, and Netherlands full datasets by axis and add the countries to the index.


# Drop the Country column and display the updated DataFrame.


# Show the customer data


# Show the products data


# Join the customer and products by columns axes. 




# Import Libraries and Dependencies
import pandas as pd
from pathlib import Path

# Import data
wheat_2018_path = Path('Resources/G20_wheat_production_2018.csv')
wheat_2019_path = Path('Resources/G20_wheat_production_2019.csv')
wheat_2020_path = Path('Resources/G20_wheat_production_2020.csv')

# Read in data 
wheat_2018_df = pd.read_csv(wheat_2018_path, index_col="Country")
wheat_2019_df = pd.read_csv(wheat_2019_path, index_col="Country")
wheat_2020_df = pd.read_csv(wheat_2020_path, index_col="Country")

# Display the 2018 wheat data
wheat_2018_df.head()

# Display the 2019 wheat data
wheat_2019_df.head()

# Display the 2020 wheat data
wheat_2020_df.head()

# Join the 2018 and 2019 wheat data where the left suffix is 2019 and right suffix is 2018. 
wheat_2018_19_data = wheat_2019_df.join(wheat_2018_df, lsuffix='_2019', rsuffix='_2018')
wheat_2018_19_data

#  Join the 2018 and 2019 wheat data with the 2020 wheat data and add the suffix, '_2020' to the 2020 data. 
all_wheat_data = wheat_2020_df.add_suffix('_2020').join(wheat_2018_19_data)
all_wheat_data

# Alternative method to joining DataFrames.

# Add the '_2018' &  '_2019' suffixes to the 2018 and 2019 data.
wheat_2018_2019 = [wheat_2019_df.add_suffix('_2019'),wheat_2018_df.add_suffix('_2018')]

# Add '_2020' suffix to the 2020 data and join the 2018 and 2019 data. 
all_wheat_data = wheat_2020_df.add_suffix('_2020').join(wheat_2018_2019)
all_wheat_data

# Read in the 2018 and 2019 data without setting the "index_col" parameter to the "Country"
wheat_2018_df = pd.read_csv(wheat_2018_path)
wheat_2019_df = pd.read_csv(wheat_2019_path)
wheat_2020_df = pd.read_csv(wheat_2020_path)

# Display the 2018 wheat data
wheat_2018_df.head()

# Display the 2019 wheat data
wheat_2019_df.head()

# Display the 2020 wheat data
wheat_2020_df.head()

# Combine the 2018 and 2019 DataFrames by setting the index to the "Country" and "Crop" columns 
# Then perform an inner join on the "Country" and "Crop" columns.
# Add the _2019 and _2018 suffixes to the appropriate columns. 
inner_joined_2018_2019_data = wheat_2019_df.join(
    wheat_2018_df.set_index(["Country", "Crop"]),
    on=["Country", "Crop"],
    how="inner",
    lsuffix="_2019",
    rsuffix="_2018")

# Display the DataFrame
inner_joined_2018_2019_data 

# How would you add the 2020 DataFrame to the 2018-2019 inner join DataFrame?
# Combine the DataFrames by setting the index to the "Country" and "Crop" columns 
# Then perform an inner join on the "Country" and "Crop" columns.
inner_joined_2018_2020_data = wheat_2020_df.join(
    inner_joined_2018_2019_data.set_index(["Country", "Crop"]),
    on=["Country", "Crop"],
    how="inner")

# Display the DataFrame
inner_joined_2018_2020_data



# Import Libraries and Dependencies
import pandas as pd
from pathlib import Path

# Import data
wheat_2018_path = Path('Resources/G20_wheat_production_2018.csv')
wheat_2019_path = Path('Resources/G20_wheat_production_2019.csv')
wheat_2020_path = Path('Resources/G20_wheat_production_2020.csv')

# Read in data 
wheat_2018_df = pd.read_csv(wheat_2018_path, index_col="Country")
wheat_2019_df = pd.read_csv(wheat_2019_path, index_col="Country")
wheat_2020_df = pd.read_csv(wheat_2020_path, index_col="Country")

# Display the 2018 wheat data


# Display the 2019 wheat data


# Display the 2020 wheat data


# Join the 2018 and 2019 wheat data where the left suffix is 2019 and right suffix is 2018. 


#  Join the 2018 and 2019 wheat data with the 2020 wheat data and add the suffix, '_2020' to the 2020 data. 


# Alternative method to joining DataFrames.

# Add the '_2018' &  '_2019' suffixes to the 2018 and 2019 data.


# Add '_2020' suffix to the 2020 data and join the 2018 and 2019 data. 


# Read in the 2018 and 2019 data without setting the "index_col" parameter to the "Country"
wheat_2018_df = pd.read_csv(wheat_2018_path)
wheat_2019_df = pd.read_csv(wheat_2019_path)
wheat_2020_df = pd.read_csv(wheat_2020_path)

# Display the 2018 wheat data


# Display the 2019 wheat data


# Display the 2020 wheat data


# Combine the 2018 and 2019 DataFrames by setting the index to the "Country" and "Crop" columns 
# Then perform an inner join on the "Country" and "Crop" columns.
# Add the _2019 and _2018 suffixes to the appropriate columns. 


# Display the DataFrame


# How would you add the 2020 DataFrame to the 2018-2019 inner join DataFrame?
# Combine the DataFrames by setting the index to the "Country" and "Crop" columns 
# Then perform an inner join on the "Country" and "Crop" columns.


# Display the DataFrame




# Import libraries and dependencies
import pandas as pd
from pathlib import Path

# Import the 2016 - 2020 alternative fuel stations data. 
alt_fuel_stations_2016_path = Path('Resources/alt_fuel_stations_2016.csv')
alt_fuel_stations_2017_path = Path('Resources/alt_fuel_stations_2017.csv')
alt_fuel_stations_2018_path = Path('Resources/alt_fuel_stations_2018.csv')
alt_fuel_stations_2019_path = Path('Resources/alt_fuel_stations_2019.csv')
alt_fuel_stations_2020_path = Path('Resources/alt_fuel_stations_2020.csv')

# Read in data setting the `index_col` to the Fuel Type.
alt_fuel_stations_2016_df = pd.read_csv(alt_fuel_stations_2016_path, index_col="Fuel_Type")
alt_fuel_stations_2017_df = pd.read_csv(alt_fuel_stations_2017_path, index_col="Fuel_Type")
alt_fuel_stations_2018_df = pd.read_csv(alt_fuel_stations_2018_path, index_col="Fuel_Type")
alt_fuel_stations_2019_df = pd.read_csv(alt_fuel_stations_2019_path, index_col="Fuel_Type")
alt_fuel_stations_2020_df = pd.read_csv(alt_fuel_stations_2020_path, index_col="Fuel_Type")

# Display the 2016 data.
alt_fuel_stations_2016_df

# Display the 2017 data.
alt_fuel_stations_2017_df

# Display the 2018 data.
alt_fuel_stations_2018_df

# Display the 2019 data.
alt_fuel_stations_2019_df

# Display the 2020 data.
alt_fuel_stations_2020_df

# Join the 2016 and 2017 data where the left suffix is 2016 and right suffix is 2017. 
alt_fuel_2016_2017 = alt_fuel_stations_2016_df.join(alt_fuel_stations_2017_df, lsuffix='_2016', rsuffix='_2017')
alt_fuel_2016_2017

# Create a list to hold the 2018, 2019, and 2020 DataFrames and add "_2018", "_2019" and, "_2020" 
# suffixes to the 2018, 2019, and 2020 DataFrames using the `add_suffix()` method.
alt_fuel_2018_2020 = [alt_fuel_stations_2018_df.add_suffix('_2018'),
                      alt_fuel_stations_2019_df.add_suffix('_2019'),
                      alt_fuel_stations_2020_df.add_suffix('_2020')]

# Join the 2016-2017 data with the 2018-2020 data. 
all_alt_fuel_years = alt_fuel_2016_2017.join(alt_fuel_2018_2020)
all_alt_fuel_years

# Get the column names
all_alt_fuel_years.columns

# Drop the columns with the "Year_<year>".
clean_alt_fuel_years = all_alt_fuel_years.drop(['Year_2016','Year_2017','Year_2018', 'Year_2019', 'Year_2020'], axis=1)
clean_alt_fuel_years

# Sort the DataFrame on all the columns in ascending order.  
sorted_by_alt_fuel_type = clean_alt_fuel_years.sort_values(by=['Number_Stations_2016',
                                                           'Number_Stations_2017',
                                                           'Number_Stations_2018',
                                                           'Number_Stations_2019',
                                                           'Number_Stations_2020'], ascending=False)

# Display the sorted DataFrame.
sorted_by_alt_fuel_type



# Import libraries and dependencies
import pandas as pd
from pathlib import Path

# Import the 2016 - 2020 alternative fuel stations data. 

# Read in data setting the `index_col` to the Fuel Type.


# Display the 2016 data.


# Display the 2017 data.


# Display the 2018 data.


# Display the 2019 data.


# Display the 2020 data.


# Join the 2016 and 2017 data where the left suffix is 2016 and right suffix is 2017. 


# Create a list to hold the 2018, 2019, and 2020 DataFrames and add "_2018", "_2019" and, "_2020" 
# suffixes to the 2018, 2019, and 2020 DataFrames using the `add_suffix()` method.

# Join the 2016-2017 data with the 2018-2020 data. 


# Get the column names


# Drop the columns with the "year_<year>".


# Sort the DataFrame on all the columns in ascending order.  



# Display the sorted DataFrame.




# Dependencies
import pandas as pd

raw_data_info = {
    "customer_id": [112, 403, 999, 543, 123],
    "name": ["John", "Kelly", "Sam", "April", "Bobbo"],
    "email": ["jman@gmail", "kelly@aol.com", "sports@school.edu", "April@yahoo.com", "HeyImBobbo@msn.com"]
}
info_df = pd.DataFrame(raw_data_info, columns=["customer_id", "name", "email"])
info_df

# Create DataFrames
raw_data_items = {
    "customer_id": [403, 112, 543, 999, 654],
    "item": ["soda", "chips", "TV", "Laptop", "Cooler"],
    "cost": [3.00, 4.50, 600, 900, 150]
}
items_df = pd.DataFrame(raw_data_items, columns=[
                        "customer_id", "item", "cost"])
items_df

# Merge two DataFrames. An inner join is used by default.
merge_df = pd.merge(info_df, items_df, on="customer_id")
merge_df

# Merge two DataFrames using an outer join
merge_df = pd.merge(info_df, items_df, on="customer_id", how="outer")
merge_df

# Merge two DataFrames using a left join
merge_df = pd.merge(info_df, items_df, on="customer_id", how="left")
merge_df

# Merge two DataFrames using a right join
merge_df = pd.merge(info_df, items_df, on="customer_id", how="right")
merge_df

# Drop null values if it is necessary.
cleaned_merged_df = merge_df.dropna(subset=['name', 'email'])
cleaned_merged_df



# Dependencies
import pandas as pd

raw_data_info = {
    "customer_id": [112, 403, 999, 543, 123],
    "name": ["John", "Kelly", "Sam", "April", "Bobbo"],
    "email": ["jman@gmail", "kelly@aol.com", "sports@school.edu", "April@yahoo.com", "HeyImBobbo@msn.com"]
}
info_df = pd.DataFrame(raw_data_info, columns=["customer_id", "name", "email"])
info_df

# Create DataFrames
raw_data_items = {
    "customer_id": [403, 112, 543, 999, 654],
    "item": ["soda", "chips", "TV", "Laptop", "Cooler"],
    "cost": [3.00, 4.50, 600, 900, 150]
}
items_df = pd.DataFrame(raw_data_items, columns=[
                        "customer_id", "item", "cost"])
items_df

# Merge two DataFrames. An inner join is used by default.


# Merge two DataFrames using an outer join


# Merge two DataFrames using a left join


# Merge two DataFrames using a right join


# Drop null values if it is necessary.




# Import Dependencies
import pandas as pd
from pathlib import Path

# Store filepaths into variable
state_avg_csv = Path("Resources/state_avg.csv")
state_totals_csv = Path("Resources/state_totals.csv")

# Read in files.
state_avg_df = pd.read_csv(state_avg_csv)
state_totals_df = pd.read_csv(state_totals_csv)

# Display the state averages.
state_avg_df.head()

# Display the state totals.
state_totals_df.head()

# Merge the two DataFrames together based on the Year and State they share
census_df = pd.merge(state_avg_df, state_totals_df, on=["Year", "State"])
census_df.head()

# Create a DataFrame that filters the data on only 2019
census_2019_df = pd.DataFrame(census_df.loc[census_df["Year"]==2019,:])
census_2019_df.head()

# Add a new column that calculates the Poverty Rate
census_2019_df["Poverty Rate (%)"] = census_2019_df["Total Population in Poverty"] / \
                                        census_2019_df["Total Population"] * 100
census_2019_df.head()

# Sort the data by Poverty Rate and Average Per Capita Income by County, Highest to Lowest
poverty_sorted_df = census_2019_df.sort_values(["Poverty Rate (%)", 
                                             "Average Per Capita Income by County"],
                                           ascending=False)

# Reset Index
poverty_sorted_df = poverty_sorted_df.reset_index(drop=True)
poverty_sorted_df.head()

# Print out the data for the state or territory with the highest poverty rate
highest_poverty = poverty_sorted_df.loc[0, :]
highest_poverty

# Print out the data for the state or territory with the lowest poverty rate with one line of code
poverty_sorted_df.loc[len(poverty_sorted_df)-1, :]



# Import Dependencies
import pandas as pd
from pathlib import Path

# Store filepaths into variable


# Read in files.


# Display the state averages.


# Display the state totals.


# Merge the two DataFrames together based on the Year and State they share


# Create a DataFrame that filters the data on only 2019


# Add a new column that calculates the Poverty Rate


# Sort the data by Poverty Rate and Average Per Capita Income by County, Highest to Lowest


# Reset Index


# Print out the data for the state or territory with the highest poverty rate


# Print out the data for the state or territory with the lowest poverty rate with one line of code




import pandas as pd
from pathlib import Path

# Import data

apple_path = Path('Resources/AAPL.csv')
google_path = Path('Resources/GOOG.csv')
meta_path = Path('Resources/META.csv')

# Read in data and index by date
apple_data = pd.read_csv(apple_path)
google_data = pd.read_csv(google_path)
meta_data = pd.read_csv(meta_path)

# Show sample of Apple data
apple_data.head()

# Show sample of Google data
google_data.head()

# Show sample of Meta data
meta_data.head()

# Merge Apple stock with Google stock on the date using pd.merge()
merged_apple_google = pd.merge(apple_data, google_data, on="Date")
merged_apple_google.head(10)

# Alternative: Merge Apple stock with Google stock on the date.
merged_apple_google = apple_data.merge(google_data, on='Date')
merged_apple_google.head(10)

# Get the column names.
merged_apple_google.columns

# Rename the columns
merged_apple_google = merged_apple_google.rename(columns={"Open_x": "Apple_Open","High_x": "Apple_High","Low_x": "Apple_Low", 
                                         "Close_x": "Apple_Close", "Adj Close_x": "Apple_Adj_Close", "Volume_x": "Apple_Volume",
                                         "Open_y": "Google_Open","High_y": "Google_High","Low_y": "Google_Low", 
                                         "Close_y": "Google_Close", "Adj Close_y": "Google_Adj_Close", "Volume_y": "Google_Volume"})

merged_apple_google.head(10)

# Merge Apple stock with Google stock and then merge Meta stock with the Apple and Google stocks.
# The Meta stock data will be added on the left side. 
merged_apple_google_meta = meta_data.merge(merged_apple_google, on='Date')
merged_apple_google_meta.head(10)

# Merge Apple, Google and Meta stocks on the "Date". And have the meta stocks last. 
# The Meta stock data will be added on the right side. 
merged_apple_google_meta = merged_apple_google.merge(meta_data, on='Date')
merged_apple_google_meta.head(10)

# Rename the meta columns
merged_apple_google_meta = merged_apple_google_meta.rename(columns={"Open": "Meta_Open","High": "Meta_High",
                                                                  "Low": "Meta_Low", "Close": "Meta_Close",
                                                                  "Adj Close": "Meta_Adj_Close", "Volume": "Meta_Volume"})

merged_apple_google_meta.head(10)

# Calculate the best opening for each stock
apple_open = merged_apple_google_meta["Apple_Open"].max()
google_open = merged_apple_google_meta["Google_Open"].max()
meta_open = merged_apple_google_meta["Meta_Open"].max()

# Calculate the best closing price for each stock
apple_close = merged_apple_google_meta["Apple_Close"].max()
google_close = merged_apple_google_meta["Google_Close"].max()
meta_close = merged_apple_google_meta["Meta_Close"].max()

# Calculate the total volume for each stock to the millions and rounded to two decimal places.
apple_volume = round(merged_apple_google_meta["Apple_Volume"].sum()/1000000,2)
google_volume = round(merged_apple_google_meta["Google_Volume"].sum()/1000000,2)
meta_volume = round(merged_apple_google_meta["Meta_Volume"].sum()/1000000,2)

# Create a Summary DataFrame of the Opening, Closing and Volume for each stock
summary_df = pd.DataFrame({"Best Apple Open": [apple_open],
                           "Best Apple Close": [apple_close],
                           "Total Apple Volume (million)": apple_volume,
                           "Best Google Open": [google_open],
                           "Best Google Close": [google_close],
                           "Total Google Volume (million)": google_volume,
                           "Best Meta Open": [meta_open],
                           "Best Meta Close": [meta_close],
                           "Total Meta Volume (million)": meta_volume})

summary_df



import pandas as pd
from pathlib import Path

# Import data

apple_path = Path('Resources/AAPL.csv')
google_path = Path('Resources/GOOG.csv')
meta_path = Path('Resources/META.csv')

# Read in data and index by date
apple_data = pd.read_csv(apple_path)
google_data = pd.read_csv(google_path)
meta_data = pd.read_csv(meta_path)

# Show sample of Apple data
apple_data.head()

# Show sample of Google data
google_data.head()

# Show sample of Meta data
meta_data.head()

# Merge Apple stock with Google stock on the date using pd.merge()


# Alternative: Merge Apple stock with Google stock on the date.


# Get the column names.
merged_apple_google.columns

# Rename the columns


# Merge Apple stock with Google stock and then merge Meta stock with the Apple and Google stocks.
# The Meta stock data will be added on the left side. 


# Merge Apple, Google and Meta stocks on the "Date". And have the meta stocks last. 
# The Meta stock data will be added on the right side. 


# Rename the meta columns


# Calculate the best opening for each stock


# Calculate the best closing price for each stock


# Calculate the total volume for each stock to the millions and rounded to two decimal places.


# Create a Summary DataFrame of the Opening, Closing and Volume for each stock




# Import Libraries and Dependencies
import pandas as pd
from pathlib import Path

# Import data
wheat_2018_path = Path('Resources/G20_wheat_production_2018.csv')
wheat_2019_path = Path('Resources/G20_wheat_production_2019.csv')
wheat_2020_path = Path('Resources/G20_wheat_production_2020.csv')

# Read in data 
wheat_2018_df = pd.read_csv(wheat_2018_path)
wheat_2019_df = pd.read_csv(wheat_2019_path)
wheat_2020_df = pd.read_csv(wheat_2020_path)

# Display the 2018 wheat data
wheat_2018_df.head()

# Display the 2019 wheat data
wheat_2019_df.head()

# Display the 2020 wheat data
wheat_2020_df.head()

# Perform an inner merge that combines the 2018 and 2019 DataFrames on the `Country` column.
merged_2018_2019 = wheat_2018_df.merge(wheat_2019_df, on='Country', how='inner')
merged_2018_2019.head(10)

# Merge the 2018 and 2019 wheat data with the 2020 wheat data so the 2020 data is added on the right side. 
merged_2018_2020 = merged_2018_2019.merge(wheat_2020_df, on='Country', how='inner')
merged_2018_2020.head(10)

# Get the column names.
merged_2018_2020.columns

# Drop the "Crop_x", "Year_x", "Crop_y", "Year_y", "Crop" and "Year" columns. 
merged_2018_2020_clean = merged_2018_2020.drop(["Crop_x", "Year_x", "Crop_y", "Year_y", "Crop", "Year"], axis=1)
merged_2018_2020_clean

# Rename the remaining columns (Tonnes of Wheat HA)_2018, (Tonnes of Wheat HA)_2019, and (Tonnes of Wheat HA)_2020. 
merged_2018_2020_clean = merged_2018_2020_clean.rename(columns={"Value(tonnes of HA)_x": "(Tonnes of Wheat HA)_2018",
                                                                "Value(tonnes of HA)_y": "(Tonnes of Wheat HA)_2019",
                                                                "Value(tonnes of HA)": "(Tonnes of Wheat HA)_2020"})

# Display the DataFrame
merged_2018_2020_clean.head(10)

# Get the columns
merged_2018_2020_clean.columns

# Sort on the amount of tonnes of wheat for each year and reset the index. 
sorted_by_amount_2018_2020 = merged_2018_2020_clean.sort_values(by=['(Tonnes of Wheat HA)_2018',
                                                                    '(Tonnes of Wheat HA)_2019',
                                                                    '(Tonnes of Wheat HA)_2020'], ascending=False).reset_index(drop=True)

# Display the sorted DataFrame.
sorted_by_amount_2018_2020



# Import Libraries and Dependencies
import pandas as pd
from pathlib import Path

# Import data


# Read in data 


# Display the 2018 wheat data


# Display the 2019 wheat data


# Display the 2020 wheat data


#  Perform an inner merge that combines the 2018 and 2019 DataFrames on the `Country` column.


# Perform another inner merge that combines the 2018-2019 merged DataFrame 
# with the 2020 DataFrame on the `Country` column. 


# Get the column names.


# Drop the "Crop_x", "Year_x", "Crop_y", "Year_y", "Crop" and "Year" columns. 


# Rename the remaining columns (Tonnes of Wheat HA)_2018, (Tonnes of Wheat HA)_2019, and (Tonnes of Wheat HA)_2020. 


# Display the DataFrame


# Get the columns


# Sort on the amount of tonnes of wheat for each year and reset the index. 


# Display the sorted DataFrame.




# Import Dependencies
import pandas as pd

# Create a reference the CSV file desired
csv_path = "Resources/ufoSightings.csv"

# Read the CSV into a Pandas DataFrame
ufo_df = pd.read_csv(csv_path, low_memory = False)

# Print the first five rows of data to the screen
ufo_df.head()

# Remove the rows with missing data
clean_ufo_df = ufo_df.dropna(how="any")
clean_ufo_df.count()

clean_ufo_df.head()

clean_ufo_df.dtypes

# Converting the "duration (seconds)" column's values to numeric
converted_ufo = clean_ufo_df.copy()
converted_ufo["duration (seconds)"] = converted_ufo.loc[:, "duration (seconds)"].astype(float)

converted_ufo.dtypes

converted_ufo.head()

# Filter the data so that only those sightings in the US are in a DataFrame
usa_ufo_df = converted_ufo.loc[converted_ufo["country"] == "us", :]
usa_ufo_df.head()

# Count how many sightings have occured within each state
state_counts = usa_ufo_df["state"].value_counts()
state_counts.head()

# Using GroupBy in order to separate the data into fields according to "state" values
grouped_usa_df = usa_ufo_df.groupby(['state'])

# The object returned is a "GroupBy" object and cannot be viewed normally...
print(grouped_usa_df)

# In order to be visualized, a data function must be used...
grouped_usa_df.count().head(10)

grouped_usa_df["duration (seconds)"].sum()

# Since "duration (seconds)" was converted to a numeric time, it can now be summed up per state
state_duration = grouped_usa_df["duration (seconds)"].sum()
state_duration.head()

# Creating a new DataFrame using both duration and count
state_summary_table = pd.DataFrame({"Number of Sightings": state_counts,
                                    "Total Visit Time": state_duration})
state_summary_table.head()



# Import Dependencies
import pandas as pd

# Create a reference the CSV file desired
csv_path = "Resources/ufoSightings.csv"

# Read the CSV into a Pandas DataFrame
ufo_df = pd.read_csv(csv_path, low_memory = False)

# Print the first five rows of data to the screen
ufo_df.head()

# Remove the rows with missing data






# Converting the "duration (seconds)" column's values to numeric






# Filter the data so that only those sightings in the US are in a DataFrame


# Count how many sightings have occured within each state


# Using GroupBy in order to separate the data into fields according to "state" values

# The object returned is a "GroupBy" object and cannot be viewed normally...


# In order to be visualized, a data function must be used...




# Since "duration (seconds)" was converted to a numeric time, it can now be summed up per state


# Creating a new DataFrame using both duration and count




# Import Dependencies
import pandas as pd

# A seriously gigantic dataset of individuals' names, their trainers, their weight, and their days as gym members
athletic_training_data = {
    "Name":["Gino Walker","Hiedi Wasser","Kerrie Wetzel","Elizabeth Sackett","Jack Mitten","Madalene Wayman","Jamee Horvath","Arlena Reddin","Tula Levan","Teisha Dreier","Leslie Carrier","Arlette Hartson","Romana Merkle","Heath Viviani","Andres Zimmer","Allyson Osman","Yadira Caggiano","Jeanmarie Friedrichs","Leann Ussery","Bee Mom","Pandora Charland","Karena Wooten","Elizabet Albanese","Augusta Borjas","Erma Yadon","Belia Lenser","Karmen Sancho","Edison Mannion","Sonja Hornsby","Morgan Frei","Florencio Murphy","Christoper Hertel","Thalia Stepney","Tarah Argento","Nicol Canfield","Pok Moretti","Barbera Stallings","Muoi Kelso","Cicely Ritz","Sid Demelo","Eura Langan","Vanita An","Frieda Fuhr","Ernest Fitzhenry","Ashlyn Tash","Melodi Mclendon","Rochell Leblanc","Jacqui Reasons","Freeda Mccroy","Vanna Runk","Florinda Milot","Cierra Lecompte","Nancey Kysar","Latasha Dalton","Charlyn Rinaldi","Erline Averett","Mariko Hillary","Rosalyn Trigg","Sherwood Brauer","Hortencia Olesen","Delana Kohut","Geoffrey Mcdade","Iona Delancey","Donnie Read","Cesar Bhatia","Evia Slate","Kaye Hugo","Denise Vento","Lang Kittle","Sherry Whittenberg","Jodi Bracero","Tamera Linneman","Katheryn Koelling","Tonia Shorty","Misha Baxley","Lisbeth Goering","Merle Ladwig","Tammie Omar","Jesusa Avilla","Alda Zabala","Junita Dogan","Jessia Anglin","Peggie Scranton","Dania Clodfelter","Janis Mccarthy","Edmund Galusha","Tonisha Posey","Arvilla Medley","Briana Barbour","Delfina Kiger","Nia Lenig","Ricarda Bulow","Odell Carson","Nydia Clonts","Andree Resendez","Daniela Puma","Sherill Paavola","Gilbert Bloomquist","Shanon Mach","Justin Bangert","Arden Hokanson","Evelyne Bridge","Hee Simek","Ward Deangelis","Jodie Childs","Janis Boehme","Beaulah Glowacki","Denver Stoneham","Tarra Vinton","Deborah Hummell","Ulysses Neil","Kathryn Marques","Rosanna Dake","Gavin Wheat","Tameka Stoke","Janella Clear","Kaye Ciriaco","Suk Bloxham","Gracia Whaley","Philomena Hemingway","Claudette Vaillancourt","Olevia Piche","Trey Chiles","Idalia Scardina","Jenine Tremble","Herbert Krider","Alycia Schrock","Miss Weibel","Pearlene Neidert","Kina Callender","Charlotte Skelley","Theodora Harrigan","Sydney Shreffler","Annamae Trinidad","Tobi Mumme","Rosia Elliot","Debbra Putt","Rena Delosantos","Genna Grennan","Nieves Huf","Berry Lugo","Ayana Verdugo","Joaquin Mazzei","Doris Harmon","Patience Poss","Magaret Zabel","Marylynn Hinojos","Earlene Marcantel","Yuki Evensen","Rema Gay","Delana Haak","Patricia Fetters","Vinnie Elrod","Octavia Bellew","Burma Revard","Lakenya Kato","Vinita Buchner","Sierra Margulies","Shae Funderburg","Jenae Groleau","Louetta Howie","Astrid Duffer","Caron Altizer","Kymberly Amavisca","Mohammad Diedrich","Thora Wrinkle","Bethel Wiemann","Patria Millet","Eldridge Burbach","Alyson Eddie","Zula Hanna","Devin Goodwin","Felipa Kirkwood","Kurtis Kempf","Kasey Lenart","Deena Blankenship","Kandra Wargo","Sherrie Cieslak","Ron Atha","Reggie Barreiro","Daria Saulter","Tandra Eastman","Donnell Lucious","Talisha Rosner","Emiko Bergh","Terresa Launius","Margy Hoobler","Marylou Stelling","Lavonne Justice","Kala Langstaff","China Truett","Louanne Dussault","Thomasena Samaniego","Charlesetta Tarbell","Fatimah Lade","Malisa Cantero","Florencia Litten","Francina Fraise","Patsy London","Deloris Mclaughlin"],
    "Trainer":['Bettyann Savory','Mariah Barberio','Gordon Perrine','Pa Dargan','Blanch Victoria','Aldo Byler','Aldo Byler','Williams Camire','Junie Ritenour','Gordon Perrine','Bettyann Savory','Mariah Barberio','Aldo Byler','Barton Stecklein','Bettyann Savory','Barton Stecklein','Gordon Perrine','Pa Dargan','Aldo Byler','Brittani Brin','Bettyann Savory','Phyliss Houk','Bettyann Savory','Junie Ritenour','Aldo Byler','Calvin North','Brittani Brin','Junie Ritenour','Blanch Victoria','Brittani Brin','Bettyann Savory','Blanch Victoria','Mariah Barberio','Bettyann Savory','Blanch Victoria','Brittani Brin','Junie Ritenour','Pa Dargan','Gordon Perrine','Phyliss Houk','Pa Dargan','Mariah Barberio','Phyliss Houk','Phyliss Houk','Calvin North','Williams Camire','Brittani Brin','Gordon Perrine','Bettyann Savory','Bettyann Savory','Pa Dargan','Phyliss Houk','Barton Stecklein','Blanch Victoria','Coleman Dunmire','Phyliss Houk','Blanch Victoria','Pa Dargan','Harland Coolidge','Calvin North','Bettyann Savory','Phyliss Houk','Bettyann Savory','Harland Coolidge','Gordon Perrine','Junie Ritenour','Harland Coolidge','Blanch Victoria','Mariah Barberio','Coleman Dunmire','Aldo Byler','Bettyann Savory','Gordon Perrine','Bettyann Savory','Barton Stecklein','Harland Coolidge','Aldo Byler','Aldo Byler','Pa Dargan','Junie Ritenour','Brittani Brin','Junie Ritenour','Gordon Perrine','Mariah Barberio','Mariah Barberio','Mariah Barberio','Bettyann Savory','Brittani Brin','Aldo Byler','Phyliss Houk','Blanch Victoria','Pa Dargan','Phyliss Houk','Brittani Brin','Barton Stecklein','Coleman Dunmire','Bettyann Savory','Bettyann Savory','Gordon Perrine','Blanch Victoria','Junie Ritenour','Phyliss Houk','Coleman Dunmire','Williams Camire','Harland Coolidge','Williams Camire','Aldo Byler','Harland Coolidge','Gordon Perrine','Brittani Brin','Coleman Dunmire','Calvin North','Phyliss Houk','Brittani Brin','Aldo Byler','Bettyann Savory','Brittani Brin','Gordon Perrine','Calvin North','Harland Coolidge','Coleman Dunmire','Harland Coolidge','Aldo Byler','Junie Ritenour','Blanch Victoria','Harland Coolidge','Blanch Victoria','Junie Ritenour','Harland Coolidge','Junie Ritenour','Gordon Perrine','Brittani Brin','Coleman Dunmire','Williams Camire','Junie Ritenour','Brittani Brin','Calvin North','Barton Stecklein','Barton Stecklein','Mariah Barberio','Coleman Dunmire','Bettyann Savory','Mariah Barberio','Pa Dargan','Barton Stecklein','Coleman Dunmire','Brittani Brin','Barton Stecklein','Pa Dargan','Barton Stecklein','Junie Ritenour','Bettyann Savory','Williams Camire','Pa Dargan','Calvin North','Williams Camire','Coleman Dunmire','Aldo Byler','Barton Stecklein','Coleman Dunmire','Blanch Victoria','Mariah Barberio','Mariah Barberio','Harland Coolidge','Barton Stecklein','Phyliss Houk','Pa Dargan','Bettyann Savory','Barton Stecklein','Harland Coolidge','Junie Ritenour','Pa Dargan','Mariah Barberio','Blanch Victoria','Williams Camire','Phyliss Houk','Phyliss Houk','Coleman Dunmire','Mariah Barberio','Gordon Perrine','Coleman Dunmire','Brittani Brin','Pa Dargan','Coleman Dunmire','Brittani Brin','Blanch Victoria','Coleman Dunmire','Gordon Perrine','Coleman Dunmire','Aldo Byler','Aldo Byler','Mariah Barberio','Williams Camire','Phyliss Houk','Aldo Byler','Williams Camire','Aldo Byler','Williams Camire','Coleman Dunmire','Phyliss Houk'],
    "Weight":[128,180,193,177,237,166,224,208,177,241,114,161,162,151,220,142,193,193,124,130,132,141,190,239,213,131,172,127,184,157,215,122,181,240,218,205,239,217,234,158,180,131,194,171,177,110,117,114,217,123,248,189,198,127,182,121,224,111,151,170,188,150,137,231,222,186,139,175,178,246,150,154,129,216,144,198,228,183,173,129,157,199,186,232,172,157,246,239,214,161,132,208,187,224,164,177,175,224,219,235,112,241,243,179,208,196,131,207,182,233,191,162,173,197,190,182,231,196,196,143,250,174,138,135,164,204,235,192,114,179,215,127,185,213,250,213,153,217,176,190,119,167,118,208,113,206,200,236,159,218,168,159,156,183,121,203,215,209,179,219,174,220,129,188,217,250,166,157,112,236,182,144,189,243,238,147,165,115,160,134,245,174,238,157,150,184,174,134,134,248,199,165,117,119,162,112,170,224,247,217],
    "Membership (Days)":[52,70,148,124,186,157,127,155,37,185,158,129,93,69,124,13,76,153,164,161,48,121,167,69,39,163,7,34,176,169,108,162,195,86,155,77,197,200,80,142,179,67,58,145,188,147,125,15,13,173,125,4,61,29,132,110,62,137,197,135,162,174,32,151,149,65,18,42,63,62,104,200,189,40,38,199,1,12,8,2,195,30,7,72,130,144,2,34,200,143,43,196,22,115,171,54,143,59,14,52,109,115,187,185,26,19,178,18,120,169,45,52,130,69,168,178,96,22,78,152,39,51,118,130,60,156,108,69,103,158,165,142,86,91,117,77,57,169,86,188,97,111,22,83,81,177,163,35,12,164,21,181,171,138,22,107,58,51,38,128,19,193,157,13,104,89,13,10,26,190,179,101,7,159,100,49,120,109,56,199,51,108,47,171,69,162,74,119,148,88,32,159,65,146,140,171,88,18,59,13]
}

# Create a DataFrame of the athletic training data.
training_data = pd.DataFrame(athletic_training_data)
training_data.head()

# Convert the membership days into weeks and then add a this data in a new column to the DataFrame.
weeks = training_data["Membership (Days)"]/7
training_data["Membership (Weeks)"] = weeks

training_data.head()

# Create a Dataframe that has the Trainer, Weight, and Membership.
trainers_data =  training_data[["Trainer", "Weight", "Membership (Days)", "Membership (Weeks)"]]
trainers_data

# Using groupby get the average weight and length membership for each trainer.

trainers_means = trainers_data.groupby(["Trainer"]).mean()
trainers_means

# Sort the group by table on Membership in descending order.
trainers_means.sort_values(by='Membership (Days)', ascending=False)



# Import Dependencies
import pandas as pd

# A seriously gigantic dataset of individuals' names, their trainers, their weight, and their days as gym members
athletic_training_data = {
    "Name":["Gino Walker","Hiedi Wasser","Kerrie Wetzel","Elizabeth Sackett","Jack Mitten","Madalene Wayman","Jamee Horvath","Arlena Reddin","Tula Levan","Teisha Dreier","Leslie Carrier","Arlette Hartson","Romana Merkle","Heath Viviani","Andres Zimmer","Allyson Osman","Yadira Caggiano","Jeanmarie Friedrichs","Leann Ussery","Bee Mom","Pandora Charland","Karena Wooten","Elizabet Albanese","Augusta Borjas","Erma Yadon","Belia Lenser","Karmen Sancho","Edison Mannion","Sonja Hornsby","Morgan Frei","Florencio Murphy","Christoper Hertel","Thalia Stepney","Tarah Argento","Nicol Canfield","Pok Moretti","Barbera Stallings","Muoi Kelso","Cicely Ritz","Sid Demelo","Eura Langan","Vanita An","Frieda Fuhr","Ernest Fitzhenry","Ashlyn Tash","Melodi Mclendon","Rochell Leblanc","Jacqui Reasons","Freeda Mccroy","Vanna Runk","Florinda Milot","Cierra Lecompte","Nancey Kysar","Latasha Dalton","Charlyn Rinaldi","Erline Averett","Mariko Hillary","Rosalyn Trigg","Sherwood Brauer","Hortencia Olesen","Delana Kohut","Geoffrey Mcdade","Iona Delancey","Donnie Read","Cesar Bhatia","Evia Slate","Kaye Hugo","Denise Vento","Lang Kittle","Sherry Whittenberg","Jodi Bracero","Tamera Linneman","Katheryn Koelling","Tonia Shorty","Misha Baxley","Lisbeth Goering","Merle Ladwig","Tammie Omar","Jesusa Avilla","Alda Zabala","Junita Dogan","Jessia Anglin","Peggie Scranton","Dania Clodfelter","Janis Mccarthy","Edmund Galusha","Tonisha Posey","Arvilla Medley","Briana Barbour","Delfina Kiger","Nia Lenig","Ricarda Bulow","Odell Carson","Nydia Clonts","Andree Resendez","Daniela Puma","Sherill Paavola","Gilbert Bloomquist","Shanon Mach","Justin Bangert","Arden Hokanson","Evelyne Bridge","Hee Simek","Ward Deangelis","Jodie Childs","Janis Boehme","Beaulah Glowacki","Denver Stoneham","Tarra Vinton","Deborah Hummell","Ulysses Neil","Kathryn Marques","Rosanna Dake","Gavin Wheat","Tameka Stoke","Janella Clear","Kaye Ciriaco","Suk Bloxham","Gracia Whaley","Philomena Hemingway","Claudette Vaillancourt","Olevia Piche","Trey Chiles","Idalia Scardina","Jenine Tremble","Herbert Krider","Alycia Schrock","Miss Weibel","Pearlene Neidert","Kina Callender","Charlotte Skelley","Theodora Harrigan","Sydney Shreffler","Annamae Trinidad","Tobi Mumme","Rosia Elliot","Debbra Putt","Rena Delosantos","Genna Grennan","Nieves Huf","Berry Lugo","Ayana Verdugo","Joaquin Mazzei","Doris Harmon","Patience Poss","Magaret Zabel","Marylynn Hinojos","Earlene Marcantel","Yuki Evensen","Rema Gay","Delana Haak","Patricia Fetters","Vinnie Elrod","Octavia Bellew","Burma Revard","Lakenya Kato","Vinita Buchner","Sierra Margulies","Shae Funderburg","Jenae Groleau","Louetta Howie","Astrid Duffer","Caron Altizer","Kymberly Amavisca","Mohammad Diedrich","Thora Wrinkle","Bethel Wiemann","Patria Millet","Eldridge Burbach","Alyson Eddie","Zula Hanna","Devin Goodwin","Felipa Kirkwood","Kurtis Kempf","Kasey Lenart","Deena Blankenship","Kandra Wargo","Sherrie Cieslak","Ron Atha","Reggie Barreiro","Daria Saulter","Tandra Eastman","Donnell Lucious","Talisha Rosner","Emiko Bergh","Terresa Launius","Margy Hoobler","Marylou Stelling","Lavonne Justice","Kala Langstaff","China Truett","Louanne Dussault","Thomasena Samaniego","Charlesetta Tarbell","Fatimah Lade","Malisa Cantero","Florencia Litten","Francina Fraise","Patsy London","Deloris Mclaughlin"],
    "Trainer":['Bettyann Savory','Mariah Barberio','Gordon Perrine','Pa Dargan','Blanch Victoria','Aldo Byler','Aldo Byler','Williams Camire','Junie Ritenour','Gordon Perrine','Bettyann Savory','Mariah Barberio','Aldo Byler','Barton Stecklein','Bettyann Savory','Barton Stecklein','Gordon Perrine','Pa Dargan','Aldo Byler','Brittani Brin','Bettyann Savory','Phyliss Houk','Bettyann Savory','Junie Ritenour','Aldo Byler','Calvin North','Brittani Brin','Junie Ritenour','Blanch Victoria','Brittani Brin','Bettyann Savory','Blanch Victoria','Mariah Barberio','Bettyann Savory','Blanch Victoria','Brittani Brin','Junie Ritenour','Pa Dargan','Gordon Perrine','Phyliss Houk','Pa Dargan','Mariah Barberio','Phyliss Houk','Phyliss Houk','Calvin North','Williams Camire','Brittani Brin','Gordon Perrine','Bettyann Savory','Bettyann Savory','Pa Dargan','Phyliss Houk','Barton Stecklein','Blanch Victoria','Coleman Dunmire','Phyliss Houk','Blanch Victoria','Pa Dargan','Harland Coolidge','Calvin North','Bettyann Savory','Phyliss Houk','Bettyann Savory','Harland Coolidge','Gordon Perrine','Junie Ritenour','Harland Coolidge','Blanch Victoria','Mariah Barberio','Coleman Dunmire','Aldo Byler','Bettyann Savory','Gordon Perrine','Bettyann Savory','Barton Stecklein','Harland Coolidge','Aldo Byler','Aldo Byler','Pa Dargan','Junie Ritenour','Brittani Brin','Junie Ritenour','Gordon Perrine','Mariah Barberio','Mariah Barberio','Mariah Barberio','Bettyann Savory','Brittani Brin','Aldo Byler','Phyliss Houk','Blanch Victoria','Pa Dargan','Phyliss Houk','Brittani Brin','Barton Stecklein','Coleman Dunmire','Bettyann Savory','Bettyann Savory','Gordon Perrine','Blanch Victoria','Junie Ritenour','Phyliss Houk','Coleman Dunmire','Williams Camire','Harland Coolidge','Williams Camire','Aldo Byler','Harland Coolidge','Gordon Perrine','Brittani Brin','Coleman Dunmire','Calvin North','Phyliss Houk','Brittani Brin','Aldo Byler','Bettyann Savory','Brittani Brin','Gordon Perrine','Calvin North','Harland Coolidge','Coleman Dunmire','Harland Coolidge','Aldo Byler','Junie Ritenour','Blanch Victoria','Harland Coolidge','Blanch Victoria','Junie Ritenour','Harland Coolidge','Junie Ritenour','Gordon Perrine','Brittani Brin','Coleman Dunmire','Williams Camire','Junie Ritenour','Brittani Brin','Calvin North','Barton Stecklein','Barton Stecklein','Mariah Barberio','Coleman Dunmire','Bettyann Savory','Mariah Barberio','Pa Dargan','Barton Stecklein','Coleman Dunmire','Brittani Brin','Barton Stecklein','Pa Dargan','Barton Stecklein','Junie Ritenour','Bettyann Savory','Williams Camire','Pa Dargan','Calvin North','Williams Camire','Coleman Dunmire','Aldo Byler','Barton Stecklein','Coleman Dunmire','Blanch Victoria','Mariah Barberio','Mariah Barberio','Harland Coolidge','Barton Stecklein','Phyliss Houk','Pa Dargan','Bettyann Savory','Barton Stecklein','Harland Coolidge','Junie Ritenour','Pa Dargan','Mariah Barberio','Blanch Victoria','Williams Camire','Phyliss Houk','Phyliss Houk','Coleman Dunmire','Mariah Barberio','Gordon Perrine','Coleman Dunmire','Brittani Brin','Pa Dargan','Coleman Dunmire','Brittani Brin','Blanch Victoria','Coleman Dunmire','Gordon Perrine','Coleman Dunmire','Aldo Byler','Aldo Byler','Mariah Barberio','Williams Camire','Phyliss Houk','Aldo Byler','Williams Camire','Aldo Byler','Williams Camire','Coleman Dunmire','Phyliss Houk'],
    "Weight":[128,180,193,177,237,166,224,208,177,241,114,161,162,151,220,142,193,193,124,130,132,141,190,239,213,131,172,127,184,157,215,122,181,240,218,205,239,217,234,158,180,131,194,171,177,110,117,114,217,123,248,189,198,127,182,121,224,111,151,170,188,150,137,231,222,186,139,175,178,246,150,154,129,216,144,198,228,183,173,129,157,199,186,232,172,157,246,239,214,161,132,208,187,224,164,177,175,224,219,235,112,241,243,179,208,196,131,207,182,233,191,162,173,197,190,182,231,196,196,143,250,174,138,135,164,204,235,192,114,179,215,127,185,213,250,213,153,217,176,190,119,167,118,208,113,206,200,236,159,218,168,159,156,183,121,203,215,209,179,219,174,220,129,188,217,250,166,157,112,236,182,144,189,243,238,147,165,115,160,134,245,174,238,157,150,184,174,134,134,248,199,165,117,119,162,112,170,224,247,217],
    "Membership (Days)":[52,70,148,124,186,157,127,155,37,185,158,129,93,69,124,13,76,153,164,161,48,121,167,69,39,163,7,34,176,169,108,162,195,86,155,77,197,200,80,142,179,67,58,145,188,147,125,15,13,173,125,4,61,29,132,110,62,137,197,135,162,174,32,151,149,65,18,42,63,62,104,200,189,40,38,199,1,12,8,2,195,30,7,72,130,144,2,34,200,143,43,196,22,115,171,54,143,59,14,52,109,115,187,185,26,19,178,18,120,169,45,52,130,69,168,178,96,22,78,152,39,51,118,130,60,156,108,69,103,158,165,142,86,91,117,77,57,169,86,188,97,111,22,83,81,177,163,35,12,164,21,181,171,138,22,107,58,51,38,128,19,193,157,13,104,89,13,10,26,190,179,101,7,159,100,49,120,109,56,199,51,108,47,171,69,162,74,119,148,88,32,159,65,146,140,171,88,18,59,13]
}

# Create a DataFrame of the athletic training data.


# Convert the membership days into weeks and then add a this data in a new column to the DataFrame.


# Create a Dataframe that has the Trainer, Weight, and Membership.


# Using groupby get the average weight and length membership for each trainer.



# Sort the group by table on Membership in descending order.




# Import Dependencies
import pandas as pd

# Read the CSV into a Pandas DataFrame
ufo_df = pd.read_csv('Resources/ufoSightings.csv', low_memory=False)

# Remove the rows with missing data
clean_ufo_df = ufo_df.dropna(how="any")

# Converting the "duration (seconds)" column's values to numeric
converted_ufo_df = clean_ufo_df.copy()
converted_ufo_df["duration (seconds)"] = converted_ufo_df.loc[:, "duration (seconds)"].astype(float)

# Display the DataFrame
converted_ufo_df.head()

# Add double brackets around the column to for the aggregation to create a DataFrame.
grouped_ufo_duration_shape = converted_ufo_df.groupby("shape")[["duration (seconds)"]].mean()
grouped_ufo_duration_shape.head(10)

# The agg() function can be used for aggregation. 
grouped_ufo_duration_shape = converted_ufo_df.groupby("shape").agg({"duration (seconds)":'mean'})
grouped_ufo_duration_shape.head(10)

# The agg() function can be used to pass more than one aggregation.
ufo_shape_avg_sum = converted_ufo_df.groupby("shape")[["duration (seconds)"]].agg(['mean','sum'])
ufo_shape_avg_sum.head(10)

# It is also possible to group a DataFrame by multiple columns
# This returns an object with multiple indices, however, which can be harder to deal with.
# Get the average duration in seconds of UFOs by Country and State.
country_state_avg_duration = converted_ufo_df.groupby(['country', 'state'])[["duration (seconds)"]].mean()

# Display the DataFrame.
country_state_avg_duration.head(10)

# The agg() function can be used to pass more than one aggregation.
country_state_duration_metrics = converted_ufo_df.groupby(['country', 'state'])[["duration (seconds)"]].agg(['count','mean','sum'])
country_state_duration_metrics.head(10)

# One method of flattening the MultiIndex columns to a single column: use the to_flat_index() on the columns.
country_state_duration_flatten = country_state_duration_metrics.copy()

# Get the columns after apply the to_flat_index()
country_state_duration_flatten.columns = country_state_duration_flatten.columns.to_flat_index()

# Display the columns
country_state_duration_flatten.columns

# Use a list comprehension to join the each tuple for each column. 
country_state_duration_flatten.columns = ['_'.join(column) for column in country_state_duration_metrics.columns]
# Display the flattened DataFrame
country_state_duration_flatten

# The second method for flattening the multiIndex to one column.
# Get the first level of the multi-index
level_0 = country_state_duration_metrics.columns.get_level_values(0)
print(level_0)
# Get the second level of the multi-index
level_1 = country_state_duration_metrics.columns.get_level_values(1)
print(level_1)

# Combine the levels and display the DataFrame. 
country_state_duration_metrics.columns = level_0 +"_"+ level_1
country_state_duration_metrics

# Get the new column names to rename the columns
country_state_duration_metrics.columns

# Rename the columns in a multi-index DataFrame
country_state_duration_metrics = country_state_duration_metrics.rename(columns={"duration (seconds)_count": "Number of Sightings",
                                                                     "duration (seconds)_mean": "Avg Duration(seconds)",
                                                                     "duration (seconds)_sum": "Total Duration(seconds)"}, level=0)
country_state_duration_metrics.head(10)



# Import Dependencies
import pandas as pd

# Read the CSV into a Pandas DataFrame
ufo_df = pd.read_csv('Resources/ufoSightings.csv', low_memory=False)

# Remove the rows with missing data
clean_ufo_df = ufo_df.dropna(how="any")

# Converting the "duration (seconds)" column's values to numeric
converted_ufo_df = clean_ufo_df.copy()
converted_ufo_df["duration (seconds)"] = converted_ufo_df.loc[:, "duration (seconds)"].astype(float)

# Display the DataFrame
converted_ufo_df.head()

# Add double brackets around the column to for the aggregation to create a DataFrame.
grouped_ufo_duration_shape = converted_ufo_df.groupby("shape")[["duration (seconds)"]].mean()
grouped_ufo_duration_shape.head(10)

# The agg() function can be used for aggregation. 


# The agg() function can be used to pass more than one aggregation.


# It is also possible to group a DataFrame by multiple columns
# This returns an object with multiple indices, however, which can be harder to deal with.
# Get the average duration in seconds of UFOs by Country and State.


# Display the DataFrame.


# The agg() function can be used to pass more than one aggregation.


# One method of flattening the MultiIndex columns to a single column: use the to_flat_index() on the columns.



# Get the columns after apply the to_flat_index()


# Display the columns


# Use a list comprehension to join the each tuple for each column. 


# Display the flattened DataFrame


# The second method for flattening the multiIndex to one column.
# Get the first level of the multi-index


# Get the second level of the multi-index


# Combine the levels and display the DataFrame. 


# Get the new column names to rename the columns


# Rename the columns in a multi-index DataFrame





# Import the necessary dependencies
import pandas as pd

# Read in the data as a DataFrame
# Provide the dataset over AWS S3 bucket. It's ~100 MB.
delayed_flights_url = "https://static.bc-edx.com/ai/ail-v-1-0/m5/lesson_2/delayed_flights.csv"
delayed_flights_df = pd.read_csv(delayed_flights_url)
delayed_flights_df.head()

# Get the columns
delayed_flights_df.columns

# 1. Show the average time for delayed arrivals (ArrDelay) for each carrier.
grouped_arrival_delay = delayed_flights_df.groupby("UniqueCarrier").agg({'ArrDelay':'mean'})
grouped_arrival_delay

# 2. Show the average time for delayed arrivals and delayed departures for each carrier.
grouped_arrival_dept_delay = delayed_flights_df.groupby("UniqueCarrier")[["ArrDelay", "DepDelay"]].agg('mean')
grouped_arrival_dept_delay

# 3. Show the total number of flights that were diverted for each carrier by day of the week.
total_flights_diverted = delayed_flights_df.groupby(["UniqueCarrier","DayOfWeek"])[["Diverted"]].agg(["sum"])

# Display the top 25 rows
total_flights_diverted.head(25)

# Display the last 25 rows.
total_flights_diverted.tail(25)

# 4. Show the total and average number flights that were diverted and cancelled for each carrier by day of the week.
flights_cancelled_diverted = delayed_flights_df.groupby(["UniqueCarrier","DayOfWeek"])\
            [["Cancelled", "Diverted"]].agg(["sum","mean"])

# Display the top 25 rows
flights_cancelled_diverted.head(25)

# 5.Show the total and average number flights that were diverted and cancelled 
# for each flight origin and destination.
origin_dest_flights_cancelled_diverted = delayed_flights_df.groupby(["Origin","Dest"])\
                [["Cancelled", "Diverted"]].agg(["sum","mean"])


# Display the top 25 rows
origin_dest_flights_cancelled_diverted.head(25)

# 6.Show the total number flights that were diverted for each airline carrier, flight origin,
# and destination by day of the week.
origin_dest_flights_cancelled_diverted = delayed_flights_df.groupby(["UniqueCarrier","Origin","Dest"])\
                [["Diverted"]].agg(["sum"])

# Display the top 10 rows
origin_dest_flights_cancelled_diverted.head(10)

# 7. Remove the multi-index from the previous DataFrame so the "Diverted" and "sum" are a column name. 
# Retrieve the first level 
level_0 = origin_dest_flights_cancelled_diverted.columns.get_level_values(0)
print(level_0)
# Retrieve the second level 
level_1 = origin_dest_flights_cancelled_diverted.columns.get_level_values(1)
print(level_1)

# Combine the levels and display the DataFrame.
origin_dest_flights_cancelled_diverted.columns = level_0 +"_"+ level_1
origin_dest_flights_cancelled_diverted



# Import the necessary dependencies
import pandas as pd

# Read in the data as a DataFrame
# Provide the dataset over AWS S3 bucket. It's ~100 MB.
delayed_flights_url = "https://static.bc-edx.com/ai/ail-v-1-0/m5/lesson_2/delayed_flights.csv"
delayed_flights_df = pd.read_csv(delayed_flights_url)
delayed_flights_df.head()

# Get the columns
delayed_flights_df.columns

# 1. Show the average time for delayed arrivals (ArrDelay) for each carrier.


# 2. Show the average time for delayed arrivals and delayed departures for each carrier.


# 3. Show the total number of flights that were diverted for each carrier by day of the week.


# Display the top 25 rows


# Display the last 25 rows.


# 4. Show the total and average number flights that were diverted and cancelled for each carrier by day of the week.


# Display the top 25 rows


# 5.Show the total and average number flights that were diverted and cancelled 
# for each flight origin and destination.



# Display the top 25 rows


# 6. Show the total number flights that were diverted for each airline carrier, flight origin,
# and destination by day of the week.


# Display the top 10 rows


# 7.Remove the multi-index from the previous DataFrame so the "Diverted" and "sum" are a column name. 
# Retrieve the first level 


# Retrieve the second level 



# Combine the levels and display the DataFrame.




# Import Dependencies
import pandas as pd

# Read the CSV into a Pandas DataFrame
ufo_df = pd.read_csv('Resources/ufoSightings.csv', low_memory=False)

# Remove the rows with missing data
clean_ufo_df = ufo_df.dropna(how="any")

# Converting the "duration (seconds)" column's values to numeric
converted_ufo_df = clean_ufo_df.copy()
converted_ufo_df["duration (seconds)"] = converted_ufo_df.loc[:, "duration (seconds)"].astype(float)

# Display the DataFrame
converted_ufo_df.head()

# Create a DataFrame with the average time in seconds for each UFO shape.
grouped_ufo_duration_shape = converted_ufo_df.groupby("shape")[["duration (seconds)"]].mean()
grouped_ufo_duration_shape

# Create a custom function that will calculate the average of DataFrame column.
def custom_avg(x):
    return x.mean()

# Show the average time in seconds for each UFO shape.
# Pass the '"duration (seconds)' column to the custom function.
# Name the new column "Avg_Duration(seconds)".
avg_ufo_duration_shape = converted_ufo_df.groupby("shape").apply(lambda x: pd.Series({"Avg_Duration(seconds)": custom_avg(x["duration (seconds)"])}))

# Display the DataFrame
avg_ufo_duration_shape.head(10)

# Use the custom_avg function to calculate the average duration in seconds for the country and state columns. 
avg_duration_country_state = converted_ufo_df.groupby(['country', 'state']).apply(lambda x: pd.Series({"Avg_Duration(seconds)": custom_avg(x["duration (seconds)"])}))

# Display the DataFrame
avg_duration_country_state.head(10)

# Create two custom functions. 
# 1) One to calculate the total count of items for a DataFrame column.
def custom_count(x):
    return x.count()

# 2) The second will add up the values for DataFrame column.
def custom_sum(x):
    return x.sum()

# Get the number of UFO sightings, the average and total duration in seconds for each country and state.
# Use the custom functions to determine the total sightings, average and total duration in seconds. 
country_state_total_avg = converted_ufo_df.groupby(['country', 'state']).\
    apply(lambda x: pd.Series({"Number sightings": custom_count(x["duration (seconds)"]),
                              "Avg_Duration(seconds)": custom_avg(x["duration (seconds)"]),
                              "Total_Duration(seconds)": custom_sum(x["duration (seconds)"])}))

# Display the DataFrame
country_state_total_avg.head(10)



# Import Dependencies
import pandas as pd

# Read the CSV into a Pandas DataFrame
ufo_df = pd.read_csv('Resources/ufoSightings.csv', low_memory=False)

# Remove the rows with missing data
clean_ufo_df = ufo_df.dropna(how="any")

# Converting the "duration (seconds)" column's values to numeric
converted_ufo_df = clean_ufo_df.copy()
converted_ufo_df["duration (seconds)"] = converted_ufo_df.loc[:, "duration (seconds)"].astype(float)

# Display the DataFrame
converted_ufo_df.head()

# Create a DataFrame with the average time in seconds for each UFO shape.
grouped_ufo_duration_shape = converted_ufo_df.groupby("shape")[["duration (seconds)"]].mean()
grouped_ufo_duration_shape

# Create a custom function that will calculate the average of DataFrame column.


# Show the average time in seconds for each UFO shape.
# Pass the '"duration (seconds)' column to the custom function.
# Name the new column "Avg_Duration(seconds)".

# Display the DataFrame


# Use the custom_avg function to calculate the average duration in seconds for the country and state columns. 


# Display the DataFrame


# Create two custom functions. 
# 1) One to calculate the total count of items for a DataFrame column.


# 2) The second will add up the values for DataFrame column.


# Get the number of UFO sightings, the average and total duration in seconds for each country and state.
# Use the custom functions to determine the total sightings, average and total duration in seconds. 


# Display the DataFrame




# Import the necessary dependencies
import pandas as pd

# Read in the data as a DataFrame
# Provide the dataset over AWS S3 bucket. It's ~100 MB.
delayed_flights_url = "https://static.bc-edx.com/ai/ail-v-1-0/m5/lesson_2/delayed_flights.csv"
delayed_flights_df = pd.read_csv(delayed_flights_url)
delayed_flights_df.head()

# Get the columns
delayed_flights_df.columns

# Create a custom function that will calculate of a column from the DataFrame.
def custom_avg(x):
    return x.mean()

# Show the average time for delayed arrivals (ArrDelay) grouped by each airline carrier.
# Pass the 'ArrDelay' column to the custom function.
# Name the new column "Avg_ArrDelay".
grouped_arrival_delay = delayed_flights_df.groupby("UniqueCarrier").apply(lambda x: pd.Series({"Avg_ArrDelay": custom_avg(x["ArrDelay"])}))
grouped_arrival_delay.head(10)

# Use the custom function you created to calculate the average for a DataFrame colummn and
# show the average time for delayed arrivals and departures for each carrier.
grouped_arrival_dept_delay = delayed_flights_df.groupby("UniqueCarrier").\
            apply(lambda x: pd.Series({"Avg_ArrDelay": custom_avg(x["ArrDelay"]),
                                       "Avg_DepDelay": custom_avg(x["DepDelay"])}))

grouped_arrival_dept_delay.head(10)

# Create a custom function that returns the total of the items.
def custom_sum(x):
    return x.sum()

# Use the two custom functions you created and show the total number of flights 
# and the average number of flights that were diverted for each flight origin and destination.
origin_dest_flights_cancelled_diverted = delayed_flights_df.groupby(["Origin","Dest"]).\
            apply(lambda x: pd.Series({"Total_Diverted": custom_sum(x["Diverted"]),
                                        "Avg_Diverted": custom_avg(x["Diverted"])}))
                                      


# Sort the results in descending order on the new columns, and display the top 25 results.
origin_dest_flights_cancelled_diverted.sort_values(by=['Total_Diverted','Avg_Diverted' ], ascending=False).head(25)

# Create two more custom functions.
# The first one returns the number of items.
def custom_count(x):
    return x.count()

# The second one calculates the percentage.
def custom_percentage(x): 
    if x.count()>0:
        return (x.sum()/x.count()*100).round(2)
    else:
        return 0

# Show the total number of diverted flights, the total flights, 
# and the percentage of diverted flights for each carrier by day of the week.
flights_diverted = delayed_flights_df.groupby(["UniqueCarrier","DayOfWeek"]).\
            apply(lambda x: pd.Series({"Total_Diverted": custom_sum(x["Diverted"]),
                                       "Number of Flights": custom_count(x["Diverted"]),
                                      "% of Flights Diverted":custom_percentage(x["Diverted"])}))


# Display the top 25 rows. 
flights_diverted.head(25)



# Import the necessary dependencies
import pandas as pd

# Read in the data as a DataFrame
# Provide the dataset over AWS S3 bucket. It's ~100 MB.
delayed_flights_url = "https://static.bc-edx.com/ai/ail-v-1-0/m5/lesson_2/delayed_flights.csv"
delayed_flights_df = pd.read_csv(delayed_flights_url)
delayed_flights_df.head()

# Get the columns
delayed_flights_df.columns

# Create a custom function that will calculate of a column from the DataFrame.


# Show the average time for delayed arrivals (ArrDelay) grouped by each airline carrier.
# Pass the 'ArrDelay' column to the custom function.
# Name the new column "Avg_ArrDelay".


# Use the custom function you created to calculate the average for a DataFrame colummn and
# show the average time for delayed arrivals and departures for each carrier.


# Create a custom function that returns the total of the items.


# Use the two custom functions you created and show the total number of flights 
# and the average number of flights that were diverted for each flight origin and destination.


# Sort the results in descending order on the new columns, and display the top 25 results.


# Create two more custom functions.
# The first one returns the number of items.


# The second one calculates the percentage.


# Show the total number of diverted flights, the total flights, 
# and the percentage of diverted flights for each carrier by day of the week.


# Display the top 25 rows. 




# Import Dependencies
import pandas as pd

# Create a DataFrame from dictionary of lists. 
class_data = {
    'Class': ['Oct', 'Oct', 'Jan', 'Jan', 'Oct', 'Jan'], 
    'Name': ["Cyndy", "Logan", "Laci", "Elmer", "Crystle", "Emmie"], 
    'Test Score': [90, 59, 72, 88, 98, 60]}

test_scores_df = pd.DataFrame(class_data)
test_scores_df

# Create the bins in which Data will be held
# Bins are 0, 59.9, 69.9, 79.9, 89.9, 100.   
bins = [0, 59.9, 69.9, 79.9, 89.9, 100]

# Create the names for the five bins
group_names = ["F", "D", "C", "B", "A"]

# Slice the data and place it into bins
test_scores_df["Test Score Summary"] = pd.cut(test_scores_df["Test Score"], 
                                              bins, labels=group_names, 
                                              include_lowest=True)
test_scores_df

# Creating a group based off of the bins
test_scores_df = test_scores_df.groupby("Test Score Summary").max()
test_scores_df

# Sorting a groupby object.
test_scores_df = test_scores_df.groupby("Test Score Summary").max().sort_values(by="Test Score Summary", ascending=False)
test_scores_df



# Import Dependencies
import pandas as pd

# Create a DataFrame from dictionary of lists. 
class_data = {
    'Class': ['Oct', 'Oct', 'Jan', 'Jan', 'Oct', 'Jan'], 
    'Name': ["Cyndy", "Logan", "Laci", "Elmer", "Crystle", "Emmie"], 
    'Test Score': [90, 59, 72, 88, 98, 60]}

test_scores_df = pd.DataFrame(class_data)
test_scores_df

# Create the bins in which Data will be held
# Bins are 0, 59.9, 69.9, 79.9, 89.9, 100.   


# Create the names for the five bins


# Slice the data and place it into bins


# Creating a group based off of the bins


# Sorting a groupby object.




# Import Dependencies
import pandas as pd
from pathlib import Path

# Create a path to the csv and read it into a Pandas DataFrame
csv_path = Path("Resources/movie_scores.csv")
movies_df = pd.read_csv(csv_path)

movies_df.head()

# Figure out the minimum and maximum IMDB user vote count
print(movies_df["IMDB_user_vote_count"].max())
print(movies_df["IMDB_user_vote_count"].min())

# Create bins in which to place values based upon IMDB vote count
bins = [0, 2499, 4999, 9999, 14999, 19999, 29999, 49999, 99999, 350000]

# Create labels for these bins
group_labels = ["0 to 2.4k", "2.5k to 4.9k", "5k to 9k", "10k to 14k", "15k to 19k", "20k to 29k",
                "30k to 49k", "50k to 99k", "100k to 350k"]

# Slice the data and place it into bins
pd.cut(movies_df["IMDB_user_vote_count"], bins, labels=group_labels)

# Place the data series into a new column inside of the DataFrame
movies_df["IMDB User Votes Group"] = pd.cut(movies_df["IMDB_user_vote_count"], bins, labels=group_labels)
movies_df.head()

# Create a GroupBy object based upon "IMDB User Votes Group"
imdb_group = movies_df.groupby("IMDB User Votes Group")

# Find how many rows fall into each bin
print(imdb_group["IMDB"].count())

# Get the average of each of the first 5 rating columns within the GroupBy object
imdb_group[["RottenTomatoes", "RottenTomatoes_User", "Metacritic", "Metacritic_User", "IMDB"]].mean()



# Import Dependencies
import pandas as pd
from pathlib import Path

# Create a path to the csv and read it into a Pandas DataFrame


# Figure out the minimum and maximum IMDB user vote count


# Create bins in which to place values based upon IMDB vote count

# Create labels for these bins


# Slice the data and place it into bins


# Place the data series into a new column inside of the DataFrame


# Create a GroupBy object based upon "IMDB User Votes Group"


# Find how many rows fall into each bin


# Get the average of each of the first 5 rating columns within the GroupBy object




# Import Dependencies
import pandas as pd

# Read file into DataFrame
book_sales_df = pd.read_csv('Resources/book_sales.csv')
book_sales_df.head(20)

# Show the unique values in the book_name column.
book_sales_df["book_name"].unique()

# Show the unique values in the date_ending column.
book_sales_df["date_ending"].unique()

#  Pivot on the date_ending with the book_name as the index, and pass the "total_sales" as the values.
pivot_date_short_form = pd.pivot(book_sales_df, columns="date_ending",index="book_name",values="total_sales" )
# Show the table.
pivot_date_short_form

# Reorder columns
pivot_date_short_form.iloc[:,[3,4,0,1,2]]

#  Pivot on the book_name with the date_ending as the index, and pass the "total_sales" as the values.
pivot_books_long_form = pd.pivot(book_sales_df, columns="book_name",index="date_ending",values="total_sales" )
pivot_books_long_form

# We can't use sort_values to sort the index. We have to use `reindex`.  
# Get the index values to copy.
pivot_books_long_form.index

# Reindex in monthly ascending order. 
pivot_books_long_form.reindex(['8/31/23', '9/30/23','10/31/23', '11/30/23', '12/31/23'])

# Using the `pivot_table()` function, get the total book sales for each book. 
# Make the columns the book title. 
pivot_table_books_sum = pd.pivot_table(book_sales_df, 
                                       values='total_sales',
                                       columns='book_name', 
                                       aggfunc='sum')

# Show the table.
pivot_table_books_sum

# Using the `pivot_table()` function, get the average book sales for each book. 
# Make the columns the book title. 
pivot_table_books_avg = book_sales_df.pivot_table(values='total_sales',
                                                  columns='book_name',
                                                  aggfunc='mean')
# Show the table.
pivot_table_books_avg

# Rename the index: "total_sales" to "Avg_Sales".
pivot_table_books_avg.rename(index={'total_sales': 'Avg_Sales'})

# Get the total and average book sales for each book. 
# Make the books the columns, and the mean and sum of the total sales under each book. 
avg_sum_books = pd.pivot_table(book_sales_df, 
                               values='total_sales',
                               columns='book_name',
                               aggfunc=('sum', 'mean'))
# Show the table
avg_sum_books

# Using the pivot_table function get the average and the total of the book sales 
# for each date. Make the date the index and round to one decimal place. 
date_ending_pivot_table = book_sales_df.pivot_table(index="date_ending", 
                                             values="total_sales",
                                            aggfunc=('mean','sum')).round(1)
# Show the table
date_ending_pivot_table

# Reindex in monthly ascending order. 
date_ending_pivot_table.reindex(['8/31/23', '9/30/23','10/31/23', '11/30/23', '12/31/23'])



# Import Dependencies
import pandas as pd

# Read file into DataFrame
book_sales_df = pd.read_csv('Resources/book_sales.csv')
book_sales_df.head(20)

# Show the unique values in the book_name column.
book_sales_df["book_name"].unique()

# Show the unique values in the date_ending column.
book_sales_df["date_ending"].unique()

#  Pivot on the date_ending with the book_name as the index, and pass the "total_sales" as the values.

# Show the table.


# Reorder columns


#  Pivot on the book_name with the date_ending as the index, and pass the "total_sales" as the values.


# We can't use sort_values to sort the index. We have to use `reindex`.  
# Get the index values to copy.


# Reindex in monthly ascending order. 


# Using the `pivot_table()` function, get the total book sales for each book. 
# Make the columns the book title. 


# Show the table.


# Using the `pivot_table()` function, get the average book sales for each book. 
# Make the columns the book title. 

# Show the table.


# Rename the index: "total_sales" to "Avg_Sales".


# Get the total and average book sales for each book. 
# Make the books the columns, and the mean and sum of the total sales under each book. 


# Show the table


# Using the pivot_table function get the average and the total of the book sales 
# for each date. Make the date the index and round to one decimal place. 

# Show the table


# Reindex in monthly ascending order. 




# Import Dependencies
import pandas as pd

# Read file into DataFrame
exam_scores_df = pd.read_csv('Resources/students_exam_scores.csv')
exam_scores_df.head(10)

# Using the `pivot_table()` function show the average exam score for each subject.
# The subject should be the columns. Round to one decimal place.
avg_exam_scores = pd.pivot_table(exam_scores_df, 
                                 values='exam_score',
                                 columns='subject',
                                 aggfunc='mean')

# Rename the index: "Avg_Subject_Score" and round to one decimal place. 
avg_exam_scores.rename(index={'exam_score': 'Avg. Subject Score'}).round(1)

# Using the `pivot_table()` function show the minimum and maximum score for each subject. 
max_exam_scores = pd.pivot_table(exam_scores_df, 
                                 values='exam_score',
                                 columns='subject',
                                 aggfunc=('min','max'))

# Show the table
max_exam_scores

# Using the `pivot_table()` function show the average score for each student across all subjects. 
# Round to one decimal place.
avg_student_scores = pd.pivot_table(exam_scores_df, 
                                 values='exam_score',
                                 columns='student_name',
                                 aggfunc='mean').round(1)

# Rename the index "Avg. Student Score" and show the table. 
avg_student_scores.rename(index={'exam_score': 'Avg. Student Score'})

# Using the `pivot_table()` function get the minimum and maximum exam score for each student across all subjects. 
min_max_student_scores = pd.pivot_table(exam_scores_df, 
                                 values='exam_score',
                                 columns='student_name',
                                 aggfunc=('min','max'))

# Rename the index "Avg. Student Score"
min_max_student_scores

# Using the `pivot_table` or the `pivot` function show each students exam score for each subject.
# The student names are the columns, the subject is the index, and the values are the exam scores.
exam_scores_subject = exam_scores_df.pivot_table(index="subject", 
                                                 columns="student_name",
                                                 values="exam_score")
exam_scores_subject



# Import Dependencies
import pandas as pd

# Read file into DataFrame
exam_scores_df = pd.read_csv('Resources/students_exam_scores.csv')
exam_scores_df.head(10)

# Using the `pivot_table()` function show average exam score for each subject.
# The subject should be the columns. Round to one decimal place.


# Rename the index: "Avg_Subject_Score" and round to one decimal place. 


# Using the `pivot_table()` function show the minumum and maximum score for each subject. 


# Using the `pivot_table()` function show the average score for each student across all subjects. 
# Round to one decimal place.


# Rename the index "Avg. Student Score"


# Using the `pivot_table()` function show the minimum and maximum exam score for each student across all subjects. 




# Using the `pivot_table` or the `pivot` function show each students exam score for each subject.
# The student names are the columns, the subject is the index, and the values are the exam scores.




# Import Dependencies
import pandas as pd

# Read file into DataFrame
ufo_df = pd.read_csv('Resources/ufoSightings.csv', low_memory=False)

# Remove the rows with missing data
clean_ufo_df = ufo_df.dropna(how="any")

# Converting the "duration (seconds)" column's values to numeric
converted_ufo_df = clean_ufo_df.copy()
converted_ufo_df["duration (seconds)"] = converted_ufo_df.loc[:, "duration (seconds)"].astype(float)

# Display the DataFrame
converted_ufo_df.head()

# Show the average seconds for each country. And, round to one decimal place.
ufo_country_avg_secs = pd.pivot_table(converted_ufo_df, 
                             columns='country',
                             values='duration (seconds)',
                             aggfunc='mean').round(1)

# Rename the index
ufo_country_avg_secs.rename(index={"duration (seconds)":"Duration: Avg. Seconds"})

# Show the average seconds for each country and state and round to one decimal place
ufo_country_state = pd.pivot_table(converted_ufo_df, 
                                   index=['country','state'],
                                   values='duration (seconds)',
                                   aggfunc='mean').round(1)
# Show the table.
ufo_country_state.head(20)

# Show the number of UFOs for each country, state, and city. 
ufo_country_state_city_cnt = pd.pivot_table(converted_ufo_df, 
                                            index=['country','state','city'],
                                            values='shape',
                                            aggfunc='count')
# Show the table.
ufo_country_state_city_cnt.head(20)

# Show the number of UFO sightings for each country, state, and city.  
# And, use `sort=False` to sort in descending order. 
ufo_country_state_city_cnt = pd.pivot_table(converted_ufo_df, 
                                           index=['country','state','city'],
                                           values='shape',
                                           aggfunc='count',
                                           sort=False)
# Show the table. 
ufo_country_state_city_cnt.head(20)

# Rename the "shape" column to "UFO Sightings"
ufo_country_state_city_sightings = ufo_country_state_city_cnt.rename(columns={"shape": "UFO Sightings"})

# Sort the pivot table to show the highest number of UFO sightings by country, state, and city.
ufo_country_state_city_sightings.sort_values(by=["UFO Sightings"], ascending=False).head(20)

# Show the minimum and maximum seconds for each country and state.
ufo_country_state_min_max = pd.pivot_table(converted_ufo_df, 
                                           index=['country','state'],
                                           values='duration (seconds)',
                                           aggfunc=('min', 'max'))
# Show the table. 
ufo_country_state_min_max.head(10)



# Import Dependencies
import pandas as pd

# Read file into DataFrame
ufo_df = pd.read_csv('Resources/ufoSightings.csv', low_memory=False)

# Remove the rows with missing data
clean_ufo_df = ufo_df.dropna(how="any")

# Converting the "duration (seconds)" column's values to numeric
converted_ufo_df = clean_ufo_df.copy()
converted_ufo_df["duration (seconds)"] = converted_ufo_df.loc[:, "duration (seconds)"].astype(float)

# Display the DataFrame
converted_ufo_df.head()

# Show the average seconds for each country. And, round to one decimal place.


# Rename the index


# Show the average seconds for each country and state and round to one decimal place


# Show the table.


# Show the number of UFOs for each country, state, and city.  

# Show the table.


# Show the number of UFO sightings for each country, state, and city.  
# And, use `sort=False` to sort in descending order. 

# Show the table. 


# Rename the "shape" column to "UFO Sightings"


# Sort the pivot table to show the highest number of UFO sightings by country, state, and city.


# Show the minimum and maximum seconds for each country and state.

# Show the table. 




# Import dependencies
import pandas as pd

# Read the CSV file into a DataFrame
vehicles_df = pd.read_csv('Resources/vehicles.csv')
vehicles_df

# Show the total number of vehicles for each model and make.
make_model_sum = pd.pivot_table(vehicles_df, 
                                 index=['model', 'make'],
                                 values='count',
                                 aggfunc='sum')
# Show the top 20 results.
make_model_sum.head(20)

# Rename the "count" column
make_model_totals = make_model_sum.rename(columns={"count": "Total"})

# Sort the renamed pivot table on the "Total" column to show the top 20 vehicles sold by model and make.
make_model_totals.sort_values(by=["Total"], ascending=False).head(20)

# Show the total number of vehicles for each model, make, and year.
make_model_yr_sum = pd.pivot_table(vehicles_df, 
                                     index=['model','make','year'],
                                     values='count',
                                     aggfunc='sum')
# Show the top 20 results.
make_model_yr_sum.head(20)

# Rename the "count" column to "Total"
make_model_yr_totals = make_model_yr_sum.rename(columns={"count": "Total"})

# Sort the pivot table on the "year" index and "Total" column and show the top 20 vehicles sold by the year.
make_model_yr_totals.sort_values(by=["year","Total"], ascending=False).head(20)

# Show the total number of vehicles for each category and year. 
category_yr_sum = pd.pivot_table(vehicles_df, 
                                     index=['category', 'year'],
                                     values='count',
                                     aggfunc='sum')
# Show the top 20 results.
category_yr_sum.head(20)

# Rename the "count" column to "Total"
category_yr_totals = category_yr_sum.rename(columns={"count": "Total"})

# Sort the pivot table on the "Total" column to show the top 20 vehicles sold by category and year.
category_yr_totals.sort_values(by=["Total"], ascending=False).head(20)

# Show the total number vehicles for each category, model, and year.
category_model_yr_sum = pd.pivot_table(vehicles_df, 
                                     index=['category','model','year'],
                                     values='count',
                                     aggfunc='sum')
# Show the top 20 results.
category_model_yr_sum.head(20)

# Rename the "count" column to "Total"
category_model_yr_totals = category_model_yr_sum.rename(columns={"count": "Total"})

# Sort the pivot table on the "Total" column to show the top 20 vehicles sold by category, model, and year.
category_model_yr_totals.sort_values(by=["Total"], ascending=False).head(20)



# Import dependencies
import pandas as pd

# Read the CSV file into a DataFrame


# Show the total number of vehicles for each model and make.

# Show the top 20 results.


# Rename the "count" column "Total"


# Sort the renamed pivot table on the "Total" column to show the top 20 vehicles sold by model and make. 


# Show the total number of vehicles for each model, make, and year.

# Show the top 20 results.


# Rename the "count" column to "Total".


# Sort the pivot table on the "year" index and "Total" column and show the top 20 vehicles sold by the year.



# Show the total number of vehicles for each category and year. 

# Show the top 20 results.


# Rename the "count" column to "Total".


# Sort the pivot table on the on the "Total" column to show the top 20 vehicles sold by category and year.


# Show the total number vehicles for each category, model, and year.


# Show the top 20 results.


# Rename the "count" column to "Total"


# Sort the pivot table on the on the "Total" column to show the top 20 vehicles sold by category, model, and year.




# Import Dependencies
import pandas as pd

# Read file into DataFrame
ufo_df = pd.read_csv('Resources/ufoSightings.csv', low_memory=False)

# Remove the rows with missing data
clean_ufo_df = ufo_df.dropna(how="any")

# Converting the "duration (seconds)" column's values to numeric
converted_ufo_df = clean_ufo_df.copy()
converted_ufo_df["duration (seconds)"] = converted_ufo_df.loc[:, "duration (seconds)"].astype(float)

# Display the DataFrame
converted_ufo_df.head()

# Get the columns.
converted_ufo_df.columns

# Create a custom function that will calculate the average of DataFrame column
def custom_avg(x):
    return x.mean()

# Use the custom_avg function to show the average seconds for each country and state and round to one decimal place
ufo_country_state = pd.pivot_table(converted_ufo_df, 
                                   index=['country','state'],
                                   values='duration (seconds)',
                                   aggfunc=custom_avg).round(1)
ufo_country_state.head(20)

# Rename the columns to reflect the results. 
ufo_country_state = ufo_country_state.rename(columns={'duration (seconds)': 'Avg. Seconds'})
ufo_country_state.head(20)

# Create two more custom functions. 
# 1) Returns the number of items from a DataFrame column.
def custom_count(x):
    return x.count()

# 2) Returns the total from a DataFrame column.
def custom_sum(x):
    return x.sum()

# Show the total number of sighting, and the avg and total number of seconds  of UFOs
# for each country, state, and city.  
country_state_total_avg = pd.pivot_table(converted_ufo_df, 
                                           index=['country','state'],
                                           values='duration (seconds)',
                                           aggfunc=(custom_count,
                                                    custom_avg,
                                                    custom_sum)).round(1)
# Display the top 25 results
country_state_total_avg.head(25)

# Get the column names 
country_state_total_avg.columns

# Rename the columns to reflect the results. 
country_state_total_avg = country_state_total_avg.rename(columns={'custom_avg': 'Avg. Seconds',
                                                        'custom_count': 'Number Sightings',
                                                        'custom_sum': 'Total Seconds'})
# Display the top 20 results
country_state_total_avg.head(20)

# Create a function the column value of a DataFrame if the value is greater than 20.
def custom_count(x):
    if x.count()>20:
        return x.count()

# Show the number of UFOs for each city, state, and country. 
# And, sort in ascending order. 
ufo_country_state_metrics = pd.pivot_table(converted_ufo_df, 
                                           index=['country','state'],
                                           values='duration (seconds)',
                                           aggfunc=(custom_count,
                                                    custom_avg,
                                                    custom_sum)).round(1)

# Display the results.
ufo_country_state_metrics

# Drop the null values
ufo_country_state_metrics.dropna(how="any")

# Show the total number of UFO sightings for each city, state, and country. 
# Limit the number of sightings to 20 or more by using the updated custom function.
ufo_country_state_city_cnt = pd.pivot_table(converted_ufo_df, 
                                           index=['country','state','city'],
                                           values='shape',
                                           aggfunc=custom_count)

# Drop the null values
ufo_country_state_city_cnt.dropna(how="any")

# Show the top 25 results
ufo_country_state_city_cnt.head(25)

# Rename the column to reflect the results. 
ufo_country_state_city_cnt = ufo_country_state_city_cnt.rename(columns={'shape': 'Number of Sightings'})

# Sort the pivot table to show the highest number of UFO sightings by country, state, and city.
ufo_country_state_city_cnt.sort_values(by=["Number of Sightings"], ascending=False)



# Import Dependencies
import pandas as pd

# Read file into DataFrame
ufo_df = pd.read_csv('Resources/ufoSightings.csv', low_memory=False)

# Remove the rows with missing data
clean_ufo_df = ufo_df.dropna(how="any")

# Converting the "duration (seconds)" column's values to numeric
converted_ufo_df = clean_ufo_df.copy()
converted_ufo_df["duration (seconds)"] = converted_ufo_df.loc[:, "duration (seconds)"].astype(float)

# Display the DataFrame
converted_ufo_df.head()

converted_ufo_df.columns

# Create a custom function that will calculate the average of DataFrame column


# Use the custom_avg function to show the average seconds for each country and state and round to one decimal place


# Rename the columns to reflect the results. 


# Create two more custom functions. 
# 1) Returns the number of items from a DataFrame column.


# 2) Returns the total the total from a DataFrame column.



# Show the total number of sighting, and the avg and total number of seconds of UFOs
# for each country, state, and city.  

# Display the top 25 results


# Get the column names 


# Rename the columns to reflect the results. 

# Display the top 20 results


# Create a function the column value of a DataFrame if the value is greater than 20.


# Show the umber of UFOs for each city, state, and country. 
 


# Display the results.


# Drop the null values


# Show the total number of UFO sightings for each city, state, and country. 
# Limit the number of sightings to 20 or more by using the updated custom function.


# Drop the null values


# Show the top 25 results


# Rename the column to reflect the results. 


# Sort the pivot table to show the highest number of UFO sightings by country, state, and city.




# Import the necessary dependencies
import pandas as pd

# Read in the data as a DataFrame
# Provide the dataset URL from an AWS S3 bucket. It's ~100 MB.
delayed_flights_url = "https://static.bc-edx.com/ai/ail-v-1-0/m5/lesson_2/delayed_flights.csv"
delayed_flights_df = pd.read_csv(delayed_flights_url)
delayed_flights_df.head()

# Get the columns
delayed_flights_df.columns

# Create a custom function that will calculate the average of DataFrame column
def custom_avg(x):
    return x.mean()

# Use the custom_avg function to show the average arrival delay 
# for each carrier, for the day of the month, and day of the week. Round to one decimal place. 
delayed_flights_avg = pd.pivot_table(delayed_flights_df, 
                                   index=['UniqueCarrier','DayofMonth', 'DayOfWeek' ],
                                   values='ArrDelay',
                                   aggfunc=custom_avg).round(1)


# Rename the column to reflect the aggregation. 
delayed_flights_avg = delayed_flights_avg.rename(columns={'ArrDelay': 'Avg. Arrival Delay'})

# Sort the pivot table to show the top 25 largest average arrival delay by carrier, day of the month, and day of the week.
delayed_flights_avg.sort_values(by=['Avg. Arrival Delay'], ascending=False).head(25)

# Use the custom_avg function to show the average delay of arrivals and departures
# for each carrier, day of the months and day of the week. Round to one decimal place. 
flights_arrival_dept_delay = pd.pivot_table(delayed_flights_df, 
                                   index=['UniqueCarrier','DayofMonth', 'DayOfWeek' ],
                                   values=["ArrDelay", "DepDelay"],
                                   aggfunc=custom_avg).round(1)


# Rename the columns to reflect the aggregation. 
flights_arrival_dept_delay = flights_arrival_dept_delay.rename(columns={'ArrDelay': 'Avg. Arrival Delay',
                                                                       "DepDelay": 'Avg. Dept. Delay'})

# Sort the pivot table to show the greatest average arrival and departures delays
# by carrier, day of the months and day of the week. Show the top 25 results
flights_arrival_dept_delay.sort_values(by=['Avg. Arrival Delay','Avg. Dept. Delay' ], ascending=False).head(25)

# Show the bottom 25 results.
flights_arrival_dept_delay.sort_values(by=['Avg. Arrival Delay','Avg. Dept. Delay' ]).head(25)

# Create a custom function that calculates the total of a DataFrame column.
def custom_sum(x):
    return x.sum()

# Show the total and average number of flights that were diverted for each carrier, the origin, and destination.
# Use the custom functions you created above.
total_avg_diverted = pd.pivot_table(delayed_flights_df, 
                                   index=['UniqueCarrier',"Origin","Dest"],
                                   values="Diverted",
                                   aggfunc=(custom_sum, custom_avg))


# Rename the columns to reflect the results. 
total_avg_diverted = total_avg_diverted.rename(columns={'custom_sum': 'Total_Diverted',
                                                        'custom_avg': 'Avg_Diverted'})

total_avg_diverted

# Sort the pivot table to show the greatest average and number of diverted flights
# for each carrier, the origin, and destination.. Show the top 25 results
total_avg_diverted.sort_values(by=['Total_Diverted','Avg_Diverted' ], ascending=False).head(25)



# Import the necessary dependencies
import pandas as pd

# Read in the data as a DataFrame
# Provide the dataset URL from an AWS S3 bucket. It's ~100 MB.
delayed_flights_url = "https://static.bc-edx.com/ai/ail-v-1-0/m5/lesson_2/delayed_flights.csv"
delayed_flights_df = pd.read_csv(delayed_flights_url)
delayed_flights_df.head()

# Get the columns


# Create a custom function that will calculate the average of DataFrame column


# Use the custom_avg function to show the average arrival delay 
# for each carrier, for the day of the month, and day of the week. Round to one decimal place. 



# Rename the column to reflect the aggregation. 


# Sort the pivot table to show the top 25 largest average arrival delay by carrier, day of the month, and day of the week.


# Use the custom_avg function to show the average delay of arrivals and departures
# for each carrier, day of the months and day of the week. Round to one decimal place. 



# Rename the columns to reflect the aggregation. 


# Sort the pivot table to show the greatest average arrival and departures delays
# by carrier, day of the months and day of the week. Show the top 25 results


# Show the bottom 25 results.


# Create a custom function that calculates the total of a DataFrame column.


# Show the total and average number of flights that were diverted for each carrier, the origin, and destination.
# Use the custom functions you created above.



# Rename the columns to reflect the results. 




# Sort the pivot table to show the greatest average and number of diverted flights
# for each carrier, the origin, and destination.. Show the top 25 results




# Import the dependencies
import pandas as pd
import random

# Create a Data Series with a range of 90 dates.
dates = pd.date_range('1/1/23', periods=90, freq='D')
random_integers = [random.randint(10, 50) for _ in range(90)]
data = {"date": dates, "visits": random_integers}
sales_data_df = pd.DataFrame(data)
sales_data_df

# Make the dates the index because resample only works on a datetime index
sales_df = sales_data_df.set_index('date')
sales_df

# Get the total visits for each week.
sales_df.resample('W').sum()

# Get the number of visits for each week.
sales_df.resample('W').count()

# Get the average number of visits for each week rounded to one decimal place.
sales_df.resample('W').mean().round(1)

# Get the total visits for each month.
sales_df.resample('M').sum()

# Read file into DataFrame
exam_scores_df = pd.read_csv('Resources/students_exam_scores.csv')
exam_scores_df.head(10)

# Using the `pivot_table` or the `pivot` function show each students exam score for each subject.
# The student names are the columns, the subject is the index, and the values are the exam scores.
exam_scores_subject = exam_scores_df.pivot_table(index="subject", 
                                                 columns="student_name",
                                                 values="exam_score").rename_axis(None, axis=1)
exam_scores_subject

# Reset the index so "subject" is a column.
exam_scores_reindexed = exam_scores_subject.reset_index()
exam_scores_reindexed

# Convert the DataFrame from short form to long form. 
# Melt the DataFrame
exam_scores_reindexed.melt()

# Melt the DataFrame using the variable we'd like to keep in the long DataFrame.
exam_scores_reindexed.melt(id_vars="subject")

# Melt the DataFrame and rename the columns
melted_df = exam_scores_reindexed.melt(id_vars="subject", var_name="student_name", value_name="exam_score")
melted_df

# Group the melted DataFrame on the subject get the average exam score rounded to one decimal place.
subject_exam_scores = melted_df.groupby("subject")[["exam_score"]].mean().round(1)
subject_exam_scores



# Import the dependencies
import pandas as pd
import random

# Create a Data Series with a range of 90 dates.
dates = pd.date_range('1/1/23', periods=90, freq='D')
random_integers = [random.randint(10, 50) for _ in range(90)]
data = {"date": dates, "visits": random_integers}
sales_data_df = pd.DataFrame(data)
sales_data_df

# Make the dates the index because resample only works on a datetime index


# Get the total visits for each week.


# Get the number of visits for each week.


# Get the average number of visits for each week rounded to one decimal place.


# Get the total visits for each month.


# Read file into DataFrame
exam_scores_df = pd.read_csv('Resources/students_exam_scores.csv')
exam_scores_df.head(10)

# Using the `pivot_table` or the `pivot` function show each students exam score for each subject.
# The student names are the columns, the subject is the index, and the values are the exam scores.
exam_scores_subject = exam_scores_df.pivot_table(index="subject", 
                                                 columns="student_name",
                                                 values="exam_score").rename_axis(None, axis=1)
exam_scores_subject

# Reset the index so "subject" is a column.


# Convert the DataFrame from short form to long form. 
# Melt the DataFrame


# Melt the DataFrame using the variable we'd like to keep in the long DataFrame.


# Melt the DataFrame and rename the columns


# Group the melted DataFrame on the subject get the average exam score rounded to one decimal place.




# Import Dependencies
import pandas as pd

# Read file into DataFrame
ufo_df = pd.read_csv('Resources/ufoSightings.csv', low_memory=False)

# Remove the rows with missing data
clean_ufo_df = ufo_df.dropna(how="any")

# Converting the "duration (seconds)" column's values to numeric
converted_ufo_df = clean_ufo_df.copy()
converted_ufo_df["duration (seconds)"] = converted_ufo_df.loc[:, "duration (seconds)"].astype(float)

# Change the 'date' column to a datetime object because resample will only work on datetime data types.
converted_ufo_df['datetime']= pd.to_datetime(converted_ufo_df['datetime'], errors='coerce')

# Drop the values that didn't get converted to a datetime format. 
converted_ufo_df = converted_ufo_df.dropna(subset=['datetime']).reset_index(drop=True)
converted_ufo_df.head(20)

# Get the columns.
converted_ufo_df.info()

# Create a pivot table with the 'datetime' as the index, the columns ='outside/inside', and the "temp" as the values.
ufo_pivot = pd.pivot_table(converted_ufo_df, 
                                            index=['datetime'],
                                            values='duration (seconds)',
                                            aggfunc='sum')
# Show the table.
ufo_pivot.head(20)

# Resample the pivot table into weekly bins 
# and get the average duration in seconds for each week rounded to one decimal place.
avg_weekly_ufo = ufo_pivot.resample('W').mean().round(1)

# Sort the resampled pivot table in ascending order on "duration (seconds)".
avg_weekly_ufo.sort_values(by="duration (seconds)", ascending=False).head(10)

# Resample the pivot table into monthly bins 
# and get the average duration in seconds for each month rounded to one decimal place.
avg_monthly_ufo = ufo_pivot.resample('M').mean().round(1)

# Sort the resampled pivot table in ascending order on "duration (seconds)".
avg_monthly_ufo.sort_values(by="duration (seconds)", ascending=False).head(10)

# Create a pivot table with the 'datetime' as the index, the columns ='shape', and the count as the values.
ufo_pivot_sum = pd.pivot_table(converted_ufo_df, 
                                            index=['datetime'],
                                            values='shape',
                                            aggfunc='count')
# Show the table.
ufo_pivot_sum.head(20)

# Resample the pivot table into weekly bins and get the total number of sightings for each week.
weekly_ufo_sightings = ufo_pivot_sum.resample('W').sum()

# Sort the resampled pivot table in ascending order on "shape".
weekly_ufo_sightings.sort_values(by="shape", ascending=False).head(10)

# Resample the pivot table into monthly bins and get the total number of sightings for each month.
monthly_ufo_sightings = ufo_pivot_sum.resample('M').sum()

# Sort the resampled pivot table in ascending order on "shape".
monthly_ufo_sightings.sort_values(by="shape", ascending=False).head(10)

# Read the book_sales.csv file into a DataFrame
book_sales_df = pd.read_csv('Resources/book_sales.csv')

# Pivot on the date_ending with the book_name as the index, and pass the "total_sales" as the values.
# Remove the index axis "date_ending".
book_sales_pivot = pd.pivot(book_sales_df, columns="date_ending",index="book_name",values="total_sales" ).rename_axis(None, axis=1)

# Reset the index so "book_name" is a column.
book_sales_reindexed = book_sales_pivot.reset_index()
book_sales_reindexed

# Convert the DataFrame from short form to long form. 
# Melt the DataFrame
book_sales_reindexed.melt()

# Convert the DataFrame using the variable ("book_name") we'd like to keep in the long DataFrame.
book_sales_reindexed.melt(id_vars="book_name")

# Convert the DataFrame and rename the columns to reflect the values. 
melted_book_sales = book_sales_reindexed.melt(id_vars="book_name", var_name="date", value_name="total_sales")
melted_book_sales

# Group the previous DataFrame on the date and show the total sales by the "date".
book_sales_grouped = melted_book_sales.groupby("date")[["total_sales"]].sum()
book_sales_grouped



# Import Dependencies
import pandas as pd

# Read file into DataFrame
ufo_df = pd.read_csv('Resources/ufoSightings.csv', low_memory=False)

# Remove the rows with missing data
clean_ufo_df = ufo_df.dropna(how="any")

# Converting the "duration (seconds)" column's values to numeric
converted_ufo_df = clean_ufo_df.copy()
converted_ufo_df["duration (seconds)"] = converted_ufo_df.loc[:, "duration (seconds)"].astype(float)

# Change the 'date' column to a datetime object because resample will only work on datetime data types.
converted_ufo_df['datetime']= pd.to_datetime(converted_ufo_df['datetime'], errors='coerce')

# Drop the values that didn't get converted to a datetime format. 
converted_ufo_df = converted_ufo_df.dropna(subset=['datetime']).reset_index(drop=True)
converted_ufo_df.head(20)

# Get the columns.
converted_ufo_df.info()

# Create a pivot table with the 'datetime' as the index, the columns ='duration (seconds)', and the "sum" as the values.
ufo_pivot = pd.pivot_table(converted_ufo_df, 
                                            index=['datetime'],
                                            values='duration (seconds)',
                                            aggfunc='sum')
# Show the table.
ufo_pivot.head(20)

# Resample the pivot table into weekly bins 
# and get the average duration in seconds for each week rounded to one decimal place.


# Sort the resampled pivot table in ascending order on "duration (seconds)".


# Resample the pivot table into monthly bins 
# and get the average duration in seconds for each month rounded to one decimal place.

# Sort the resampled pivot table in ascending order on "duration (seconds)".


# Create a pivot table with the 'datetime' as the index, the columns ='shape', and the "count" as the values.
ufo_pivot_sum = pd.pivot_table(converted_ufo_df, 
                                            index=['datetime'],
                                            values='shape',
                                            aggfunc='count')
# Show the table.
ufo_pivot_sum.head(20)

# Resample the pivot table into weekly bins and get the total number of sightings for each week.


# Sort the resampled pivot table in ascending order on "shape".


# Resample the pivot table into monthly bins and get the total number of sightings for each month.


# Sort the resampled pivot table in ascending order on "shape".


# Read the book_sales.csv file into a DataFrame
book_sales_df = pd.read_csv('Resources/book_sales.csv')

# Pivot on the date_ending with the book_name as the index, and pass the "total_sales" as the values.
# Remove the index axis "date_ending".
book_sales_pivot = pd.pivot(book_sales_df, columns="date_ending",index="book_name",values="total_sales" ).rename_axis(None, axis=1)

# Reset the index so "book_name" is a column.
book_sales_reindexed = book_sales_pivot.reset_index()
book_sales_reindexed

# Convert the DataFrame from short form to long form. 
# Melt the DataFrame


# Convert the DataFrame using the variable ("book_name") we'd like to keep in the long DataFrame.


# Convert the DataFrame and rename the columns to reflect the values. 


# Group the previous DataFrame on the date and show the total sales by the "date".




import pandas as pd

url = 'https://en.wikipedia.org/wiki/List_of_Australian_capital_cities'

tables = pd.read_html(url)
tables

type(tables)

df = tables[1]
df.head()

cols = list(df.columns)
cols[2] = "City population"
cols[3] = "State/territory population"
df.columns = cols
df.head()

df = df.drop(['Image'], axis=1)
df.head()

df = df.reset_index(drop=True)
df.head()

df.loc[df["State/territory"]=="New South Wales"]

df.to_csv("australian_city_data.csv", index=False)



import pandas as pd

url = 'https://en.wikipedia.org/wiki/List_of_Australian_capital_cities'



















import pandas as pd

url = 'https://en.wikipedia.org/wiki/2019_FIFA_Women%27s_World_Cup'

# Use the Pandas `read_html` to parse the url
tables = pd.read_html(url)
tables

# Find the correct table
for i in range(79):
    print(f"table: {i}")
    print(tables[i])
# tables[13]

# Save the table to a DataFrame
stats_df = pd.DataFrame(tables[18])
stats_df

# Drop NA rows and reset the index
stats_df = stats_df.dropna().reset_index()
stats_df

# Check the data types
stats_df.dtypes

# Remove the "+" and replace the "âˆ’" with "-" from the "GD" column
stats_df["GD"] = stats_df["GD"].str.replace("+", "", regex=False)
stats_df["GD"] = stats_df["GD"].str.replace("âˆ’", "-", regex=False)
stats_df

# Convert the following columns to dtype int
cols = ["Pos", "Pld", "W", "D", "L", "GF", "GA", "GD", "Pts"]

for col in cols:
    stats_df[col] = stats_df[col].astype('int')
stats_df

# Export as a CSV without the index
stats_df.to_csv("fifa_stats.csv", index=False)



import pandas as pd

url = 'https://en.wikipedia.org/wiki/2019_FIFA_Women%27s_World_Cup'

# Use the Pandas `read_html` to parse the url


# Find the correct table


# Save the table to a DataFrame


# Drop NA rows and reset the index


# Check the data types


# Remove the "+" and replace the "âˆ’" with "-" from the "GD" column


# Convert the following columns to dtype int
cols = ["Pos", "Pld", "W", "D", "L", "GF", "GA", "GD", "Pts"]



# Export as a CSV without the index




import requests

# Create variable to hold request url
url = "http://api.worldbank.org/v2/country/us/indicator/NY.GDP.MKTP.CD"

# Add format specifier to request url
url = url + "?format=json"

# Execute get request
requests.get(url)

# Example 404 error response
requests.get("http://httpstat.us/404")

# Get the text of the 404 status code page
requests.get("http://httpstat.us/404").text

# Example 500 error response
requests.get("http://httpstat.us/500")

# Get the text of the 500 status code page
requests.get("http://httpstat.us/500").text

# Execute GET request and store response
response_data = requests.get(url)

# Get content
response_content = response_data.content
print(response_content)

import json

# Formatting as json
data = response_data.json()

# Add indents to JSON and output to screen
print(json.dumps(data, indent=4))

# Select country and GDP value for second row
country = data[1][1]['country']['value']
gdp_value = data[1][1]['value']

print("Country: " + country)
print("GDP Value: " + str(gdp_value))



import requests

# Create variable to hold request url
url = "http://api.worldbank.org/v2/country/us/indicator/NY.GDP.MKTP.CD"

# Add format specifier to request url
url = url + "?format=json"

# Execute get request


# Example 404 error response


# Get the text of the 404 status code page


# Example 500 error response


# Get the text of the 500 status code page


# Execute GET request and store response


# Get content


import json

# Formatting as json


# Add indents to JSON and output to screen


# Select country and GDP value for second row




import requests
import json

# Declare "url" variables
star_wars_url = "https://swapi.dev/api/people/"
who_url = "https://ghoapi.azureedge.net/api/HWF_0004"
library_url = "https://openlibrary.org/authors/OL2864471A/works.json"
exchange_rate_url = "https://open.er-api.com/v6/latest/CAD"
us_treasury_url = "https://api.usaspending.gov//api/v2/references/agency/456/"
us_gdp_data_url = "http://api.worldbank.org/v2/country/us?format=json"

# Execute "GET" request with url
response_data = requests.get(star_wars_url)

# Print "response_data" variable
print(response_data)

# Store response using "content" attribute
response_content = response_data.content

# Format data as JSON
data = response_data.json()

# Use json.dumps with argument indent=4 to format data
print(json.dumps(data, indent=4))

# Select two values
selected_value = data['results'][0]['name']
selected_value_2 = data['results'][0]['birth_year']

# Print selected values
print(selected_value)
print(selected_value_2)



import requests
import json

# Declare "url" variables
star_wars_url = "https://swapi.dev/api/people/"
who_url = "https://ghoapi.azureedge.net/api/HWF_0004"
library_url = "https://openlibrary.org/authors/OL2864471A/works.json"
exchange_rate_url = "https://open.er-api.com/v6/latest/CAD"
us_treasury_url = "https://api.usaspending.gov//api/v2/references/agency/456/"
us_gdp_data_url = "http://api.worldbank.org/v2/country/us?format=json"

# Execute "GET" request with url


# Print "response_data" variable


# Store response using "content" attribute


# Format data as JSON


# Use json.dumps with argument indent=4 to format data


# Select two values


# Print selected values




import requests
import json

# Create parameterized url
request_url = "http://numbersapi.com/42?json"

# Submit request and format output
response_data = requests.get(request_url).json()
print(json.dumps(response_data, indent=4))

# Select fact 
response_data['text']

# Create parameterized url
request_url = "http://numbersapi.com/8?json"

# Submit request and format output
response_data = requests.get(request_url).json()
print(json.dumps(response_data, indent=4))

# Select fact
response_data['text']



import requests
import json

# Create parameterized url
request_url = "http://numbersapi.com/42?json"

# Submit request and format output


# Select fact 


# Create parameterized url
request_url = "http://numbersapi.com/8?json"

# Submit request and format output


# Select fact


import requests
import json

# Declare request url to create deck id
create_deck_url = "https://deckofcardsapi.com/api/deck/new/shuffle/?deck_count=6"

# Execute create deck url
response_data = requests.get(create_deck_url).json()
response_data

# Select deck_id
deck_id = response_data['deck_id']
print(deck_id)

# Declare draw_cards_url and shuffle_deck_url
# Use string interpolation to incorporate the deck_id
draw_cards_url = f"https://deckofcardsapi.com/api/deck/{deck_id}/draw/?count=2"
shuffle_deck_url = f"https://deckofcardsapi.com/api/deck/{deck_id}/shuffle/"
print(draw_cards_url)
print(shuffle_deck_url)

# Draw two cards
drawn_cards = requests.get(draw_cards_url).json()

# Select returned card's value and suit (i.e. 3 of clubs)
player_1_card_1 = drawn_cards['cards'][0]['value'] + " of " + drawn_cards['cards'][0]['suit']
player_1_card_2 = drawn_cards['cards'][1]['value'] + " of " + drawn_cards['cards'][1]['suit']

# Print player cards
print(player_1_card_1)
print(player_1_card_2)

# Draw a third card
draw_cards_url = f"https://deckofcardsapi.com/api/deck/{deck_id}/draw/?count=1"
drawn_cards = requests.get(draw_cards_url).json()

# Select returned card's value and suit (i.e. 3 of clubs)
player_1_card_3 = drawn_cards['cards'][0]['value'] + " of " + drawn_cards['cards'][0]['suit']

# Print player card
print(player_1_card_3)

# Draw two cards for player 1
draw_cards_url = f"https://deckofcardsapi.com/api/deck/{deck_id}/draw/?count=2"
drawn_cards = requests.get(draw_cards_url).json()

# Select card value and suit 
player_2_card_1 = drawn_cards['cards'][0]['value'] + " of " + drawn_cards['cards'][0]['suit']
player_2_card_2 = drawn_cards['cards'][1]['value'] + " of " + drawn_cards['cards'][1]['suit']

# Print player cards
print(player_2_card_1)
print(player_2_card_2)

# Draw third card for player 2
draw_cards_url = f"https://deckofcardsapi.com/api/deck/{deck_id}/draw/?count=1"
drawn_cards = requests.get(draw_cards_url).json()
player_2_card_3 = drawn_cards['cards'][0]['value'] + " of " + drawn_cards['cards'][0]['suit']

# Print player card
print(player_2_card_3)

import requests
import json

# Declare request url to create deck id
create_deck_url = "https://deckofcardsapi.com/api/deck/new/shuffle/?deck_count=6"

# Execute create deck url


# Select deck_id

print(deck_id)

# Declare draw_cards_url and shuffle_deck_url
# Use string interpolation to incorporate the deck_id
draw_cards_url = f"https://deckofcardsapi.com/api/deck/{deck_id}/draw/?count=2"
shuffle_deck_url = f"https://deckofcardsapi.com/api/deck/{deck_id}/shuffle/"
print(draw_cards_url)
print(shuffle_deck_url)

# Draw two cards


# Select returned card's value and suit (i.e. 3 of clubs)


# Print player cards
print(player_1_card_1)
print(player_1_card_2)

# Draw a third card


# Select returned card's value and suit (i.e. 3 of clubs)


# Print player card
print(player_1_card_3)

# Draw two cards for player 1


# Select card value and suit 


# Print player cards
print(player_2_card_1)
print(player_2_card_2)

# Draw third card for player 2


# Print player card
print(player_2_card_3)

# Dependencies
import json
import requests
import pandas as pd

# Open library results limit to 50 results per page. 
# Authors documentation: https://openlibrary.org/dev/docs/api/authors
# URL for Neil Gaiman
url = "https://openlibrary.org/authors/OL53305A/works.json"

# Create an empty list to store the responses
response_json = []

# Make a request for 3 pages of results
for x in range(3):
    print(f"Making request number: {x}")

    # Get the results
    post_response = requests.get(url + "?offset=" + str(x * 50)).json()

    # Loop through the "entries" of the results and
    # append them to the response_json list
    for result in post_response["entries"]:
        # Save post's JSON
        response_json.append(result)

# Now we have 150 book objects, 
# which we got by making 3 requests to the API.
print(f"We have {len(response_json)} books!")

# preview the JSON
print(json.dumps(response_json, indent=4))

# Convert the results to a DataFrame
gaiman_books_df = pd.DataFrame(response_json)
gaiman_books_df

# Convert the results to a DataFrame, normalizing the JSON
gaiman_books_normalized_df = pd.json_normalize(response_json)
gaiman_books_normalized_df

# Export to CSV
gaiman_books_normalized_df.to_csv("output/gaiman_books.csv", index=False)



# Dependencies
import json
import requests
import pandas as pd

# Open library results limit to 50 results per page. 
# Authors documentation: https://openlibrary.org/dev/docs/api/authors
# URL for Neil Gaiman
url = "https://openlibrary.org/authors/OL53305A/works.json"

# Create an empty list to store the responses
response_json = []

# Make a request for 3 pages of results


    # Get the results


    # Loop through the "entries" of the results and
    # append them to the response_json list

        # Save post's JSON


# Now we have 150 book objects, 
# which we got by making 3 requests to the API.


# preview the JSON
print(json.dumps(response_json, indent=4))

# Convert the results to a DataFrame


# Convert the results to a DataFrame, normalizing the JSON


# Export to CSV




# Dependencies
import requests
import pandas as pd

# list of TV show titles to query
tv_shows = ["Altered Carbon", "Grey's Anatomy", "This is Us", 
            "Stranger Things", "Family Law", "Good Omens",
            "Vikings", "Shameless", "Arrow", "Dirk Gently"]

# TV Maze show search base URL
base_url = "http://api.tvmaze.com/search/shows?q="

# set up lists to hold response data for name and rating
titles = []
ratings = []

all_results = []

# loop through TV show titles, make requests and parse
for show in tv_shows:
    target_url = base_url + show
    response = requests.get(target_url).json()
    titles.append(response[0]['show']['name'])
    ratings.append(response[0]['show']['rating']['average'])
    all_results.append(response[0]['show'])

# create DataFrame with titles and ratings
shows_df = pd.DataFrame({
    "title": titles,
    "rating": ratings
})

shows_df

# Use json_normalize to convert all_results to a DataFrame
all_results_df = pd.json_normalize(all_results)
all_results_df

# Export all_results_df to CSV
all_results_df.to_csv("all_results_df.csv", index=False)



# Dependencies
import requests
import pandas as pd

# list of TV show titles to query
tv_shows = ["Altered Carbon", "Grey's Anatomy", "This is Us", 
            "Stranger Things", "Family Law", "Good Omens",
            "Vikings", "Shameless", "Arrow", "Dirk Gently"]

# TV Maze show search base URL
base_url = "http://api.tvmaze.com/search/shows?q="

# set up lists to hold response data for name and rating


# loop through TV show titles, make requests and parse


# create DataFrame with titles and ratings


# Use json_normalize to convert all_results to a DataFrame


# Export all_results_df to CSV


# Dependencies
import requests
import json

# Base URL for GET requests to retrieve number/date facts
url = "http://numbersapi.com/"

# Create a list of the search choice options
search_options = ["trivia", "math", "date", "year"]

# Make a test request to check the JSON format
response = requests.get(url + "45/" +  search_options[1].lower()+ "?json").json()

# Use json.dumps with argument indent=4 to format data
print(json.dumps(response, indent=4))

# Ask the user what kind of data they would like to search for
question = ("What type of data would you like to search for? "
            + str(search_options) + "\n")
kind_of_search = input(question)

# Check that the user selection is in the search options
if kind_of_search in search_options:
    # If the kind of search is "date" take in two numbers
    if(kind_of_search.lower() == "date"):
        
        # Create a continuous loop that exits when month is valid
        while True:
            # Collect the month to search for
            month = input("What month would you like to search for? ")
            
            # Month needs to be digit and in range(1, 13)
            if not month.isdigit():
                # Print error and return month to empty string
                print("Month input not valid. Must be between 1-12.")
            elif int(month) not in range(1, 13):
                # Print error and return month to empty string
                print("Month input not valid. Must be between 1-12.")
            else:
                # Break from loop
                break

        # Create a continuous loop that exits when day is valid
        while True:
            # Collect the day to search for
            day = input("What day would you like to search for? ")

            # Day needs to be digit and in range(1, 32)
            if not day.isdigit():
                # Print error and return month to empty string
                print("Month input not valid. Must be between 1-31.")
            elif int(day) not in range(1, 32):
                # Print error and return day to empty string
                print("Day input not valid. Must be between 1-31.")
            else:
                # Break from loop
                break

        # Make an API call to the "date" API and convert response object to JSON
        response = requests.get(f"{url}{month}/{day}/{kind_of_search.lower()}?json").json()
        # Print the fact stored within the response
        print(response["text"])

    # If the kind of search is anything but "date" then take one number
    else:

        # Create a continuous loop that exits when input is number
        while True:
            # Collect the number to search for
            number = input("What number would you like to search for? ")
            
            # Check if the input is a number with isdigit()
            if number.isdigit():
                # Break from the loop
                break
            else:
                # Print an error
                print("You must enter a number.")

        # Make an API call to the API and convert response object to JSON
        response = requests.get(url + number + "/" +  kind_of_search.lower()+ "?json").json()
        # Print the fact stored within the response
        print(response["text"])
else:
    print("Error: You didn't select a valid option.")



# Dependencies
import requests
import json

# Base URL for GET requests to retrieve number/date facts


# Ask the user what kind of data they would like to search for


# Create code to return a number fact
# If the kind of search is "date" take in two numbers

  # Collect the month to search for

  # Collect the day to search for


  # Make an API call to the "date" API and convert response object to JSON

  # Print the fact stored within the response


# If the kind of search is anything but "date" then take one number


  # Collect the number to search for


  # Make an API call to the API and convert response object to JSON

  # Print the fact stored within the response




# Import dotenv package for setting environment variables 
from dotenv import load_dotenv

# Import os package
import os

# Set environment variables from the .env in the local environment
load_dotenv()

# Retrieve API key and store as Python variable
api_key = os.getenv("TMDB_API_KEY")
type(api_key)

# Test the API key with a request
import requests
import json

# Search The Movie Database for a movie title
# Documentation: https://developer.themoviedb.org/docs/search-and-query-for-details
query_url = "https://api.themoviedb.org/3/search/movie?query="

title = "Everything Everywhere All at Once"

# Execute "GET" request with url
response_data = requests.get(query_url + title + "&api_key=" + api_key)

# Format data as JSON
data = response_data.json()

# Use json.dumps with argument indent=4 to format data
print(json.dumps(data, indent=4))

# Collect the first movie id
movie_id = data["results"][0]["id"]
movie_id

# Make a request for a the full movie details
query_url = f"https://api.themoviedb.org/3/movie/{movie_id}?api_key={api_key}"

# Execute "GET" request with url
response_data = requests.get(query_url)

# Format data as JSON
data = response_data.json()

# Use json.dumps with argument indent=4 to format data
print(json.dumps(data, indent=4))



# Import dotenv package for setting environment variables 
from dotenv import load_dotenv

# Import os package
import os

# Set environment variables from the .env in the local environment


# Retrieve API key and store as Python variable

type(api_key)

# Test the API key with a request
import requests
import json

# Search The Movie Database for a movie title
# Documentation: https://developer.themoviedb.org/docs/search-and-query-for-details
query_url = "https://api.themoviedb.org/3/search/movie?query="

title = "Everything Everywhere All at Once"

# Execute "GET" request with url
response_data = requests.get(query_url + title + "&api_key=" + api_key)

# Format data as JSON
data = response_data.json()

# Use json.dumps with argument indent=4 to format data
print(json.dumps(data, indent=4))

# Collect the first movie id


# Make a request for a the full movie details
query_url = f"https://api.themoviedb.org/3/movie/{movie_id}?api_key={api_key}"

# Execute "GET" request with url


# Format data as JSON


# Use json.dumps with argument indent=4 to format data




# Initial imports
import requests
from dotenv import load_dotenv
import os
import json

load_dotenv()

api_key = os.getenv("TMDB_API_KEY")

type(api_key)

# Search The Movie Database for a movie title
# Documentation: https://developer.themoviedb.org/docs/search-and-query-for-details
search_url = "https://api.themoviedb.org/3/search/movie?query="

# Save a movie title to search for as a variable
title = "Elemental"

# Concatenate request_url, title, and api_key. Store as new variable
request_url = search_url + title + "&api_key=" + api_key

# Execute get request
response_data = requests.get(request_url).json()

# Print the JSON results
print(json.dumps(response_data, indent=4))

# Create a function that retrieves a movie's credits
def get_credits(movie_id):
    # Prepare the API URL
    credits_url = f"https://api.themoviedb.org/3/movie/{movie_id}/credits?api_key={api_key}"
    
    # Execute get request
    credits_data = requests.get(credits_url).json()

    # Print results
    print(json.dumps(credits_data, indent=4))

# Retrieve the movie ID for Elemental
movie_id = response_data["results"][0]["id"]
movie_id

# Use get_credits function for Elemental movie_id
get_credits(movie_id)

# House of Flying Daggers request

# Concatenate request_url, title, and api_key. Store as new variable
request_url = search_url + "House of Flying Daggers&api_key=" + api_key

# Execute get request
response_data = requests.get(request_url).json()

# Get movie ID
movie_id = response_data["results"][0]["id"]
movie_id

# Call get_credits function
get_credits(movie_id)

# Howl's Moving Castle request

# Concatenate request_url, title, and api_key. Store as new variable
request_url = search_url + "Howl's Moving Castle&api_key=" + api_key

# Execute get request
response_data = requests.get(request_url).json()

# Get movie ID
movie_id = response_data["results"][0]["id"]
movie_id

# Call get_credits function
get_credits(movie_id)

# The Adventures of Priscilla: Queen of the Desert request

# Concatenate request_url, title, and api_key. Store as new variable
request_url = search_url + "The Adventures of Priscilla: Queen of the Desert&api_key=" + api_key

# Execute get request
response_data = requests.get(request_url).json()

# Get movie ID
movie_id = response_data["results"][0]["id"]
movie_id

# Call get_credits function
get_credits(movie_id)

# Moana request

# Concatenate request_url, title, and api_key. Store as new variable
request_url = search_url + "Moana&api_key=" + api_key

# Execute get request
response_data = requests.get(request_url).json()

# Get movie ID
movie_id = response_data["results"][0]["id"]
movie_id

# Call get_credits function
get_credits(movie_id)



# Initial imports
import requests
from dotenv import load_dotenv
import os
import json







# Search The Movie Database for a movie title
# Documentation: https://developer.themoviedb.org/docs/search-and-query-for-details


# Save a movie title to search for as a variable


# Concatenate request_url, title, and api_key. Store as new variable


# Execute get request


# Print the JSON results


# Create a function that retrieves a movie's credits

    # Prepare the API URL

    
    # Execute get request


    # Print results


# Retrieve the movie ID for Elemental


# Use get_credits function for Elemental movie_id


# House of Flying Daggers request

# Concatenate request_url, title, and api_key. Store as new variable


# Execute get request


# Get movie ID


# Call get_credits function


# Howl's Moving Castle request

# Concatenate request_url, title, and api_key. Store as new variable


# Execute get request


# Get movie ID


# Call get_credits function


# The Adventures of Priscilla: Queen of the Desert request

# Concatenate request_url, title, and api_key. Store as new variable


# Execute get request


# Get movie ID


# Call get_credits function


# Moana request

# Concatenate request_url, title, and api_key. Store as new variable


# Execute get request


# Get movie ID


# Call get_credits function




# Import dependencies
import requests
import json
from dotenv import load_dotenv
import os

# Load environment variables and New York Times API key
load_dotenv()
api_key = os.getenv("NYT_API_KEY")
type(api_key)

# New York Times Article API URL
url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"

# Search for articles that mention granola
query = "granola"

# Build query URL
query_url = url + "api-key=" + api_key + "&q=" + query

# Request articles
articles = requests.get(query_url).json()

# The "response" property in articles contains the actual articles
# list comprehension.
articles_list = articles["response"]["docs"]
print(json.dumps(articles_list, indent=4))

# Print the web_url of each stored article
print("Your Reading List")
for article in articles_list:
    print(article["web_url"])



# Import dependencies
import requests
import json
from dotenv import load_dotenv
import os

# Load environment variables and New York Times API key
load_dotenv()
api_key = os.getenv("NYT_API_KEY")
type(api_key)

# New York Times Article API URL
url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"

# Search for articles that mention granola
query = "granola"

# Build query URL


# Request articles


# The "response" property in articles contains the actual articles
# list comprehension.


# Print the web_url of each stored article




# Import dependencies
import requests
from dotenv import load_dotenv
import os
import time

# Load environment variables and New York Times API key
load_dotenv()
api_key = os.getenv("NYT_API_KEY")

# New York Times Article API URL
url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"

# Store a search term
query = "blockbuster"

# Search for articles published between a begin and end date
begin_date = "20230101"
end_date = "20230731"

# Build URL
query_url = f"{url}api-key={api_key}&q={query}&begin_date={begin_date}&end_date={end_date}"

# Retrieve articles
articles = requests.get(query_url).json()
articles_list = articles["response"]["docs"]

for article in articles_list:
    print(f'A snippet from the article: {article["snippet"]}')
    print('---------------------------')

# BONUS: How would we get 30 results? 
# HINT: Look up the page query param

# Empty list for articles
articles_list = []

# loop through pages 0-2
for page in range(0, 3):
    query_url = f"{url}api-key={api_key}&q={query}&begin_date={begin_date}&end_date={end_date}"
    # create query with page number
    query_url = f"{query_url}&page={str(page)}"
    articles = requests.get(query_url).json()
    
    # Add a one second interval between queries to stay within API query limits
    time.sleep(1)
    # loop through the response and append each article to the list
    for article in articles["response"]["docs"]:
        articles_list.append(article)

for article in articles_list:
    print(article['snippet'])
    print('---------------------------')



# Import dependencies
import requests
from dotenv import load_dotenv
import os
import time

# Load environment variables and New York Times API key


# New York Times Article API URL


# Store a search term


# Search for articles published between a begin and end date


# Build URL


# Retrieve articles


# BONUS: How would we get 30 results? 
# HINT: Look up the page query param

# Empty list for articles


# loop through pages 0-2

    # create query with page number

    
    # Add a one second interval between queries to stay within API query limits

    # loop through the response and append each article to the list


for article in articles_list:
    print(article['snippet'])
    print('---------------------------')



# Import dependencies
import requests
import pandas as pd
from dotenv import load_dotenv
import os

# Load environment variables and OpenWeather API key
load_dotenv()
api_key = os.getenv("WEATHER_API_KEY")

# Save config information.
url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"

# Build partial query URL
query_url = f"{url}appid={api_key}&units={units}&q="

# Get weather data
weather_response = requests.get(query_url + "London")
weather_json = weather_response.json()

# Get the temperature from the response
print(f"The weather API responded with: {weather_json}.")

# List of cities
cities = ["Paris", "London", "Oslo", "Beijing"]

# set up lists to hold response info
lat = []
temp = []

# Loop through the list of cities and perform a request for data on each
for city in cities:
    response = requests.get(query_url + city).json()
    lat.append(response['coord']['lat'])
    temp.append(response['main']['temp'])

print(f"The latitude information received is: {lat}")
print(f"The temperature information received is: {temp}")

# create a DataFrame from cities, lat, and temp
weather_dict = {
    "city": cities,
    "lat": lat,
    "temp": temp
}
weather_data = pd.DataFrame(weather_dict)
weather_data



# Import dependencies
import requests
import pandas as pd
from dotenv import load_dotenv
import os

# Load environment variables and OpenWeather API key
load_dotenv()
api_key = os.getenv("WEATHER_API_KEY")

# Save config information.
url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"

# Build partial query URL


# Get weather data


# Get the temperature from the response


# List of cities


# set up lists to hold response info


# Loop through the list of cities and perform a request for data on each


# create a DataFrame from cities, lat, and temp




# Import dependencies
import requests
import pandas as pd
from dotenv import load_dotenv
import os

# Load environment variables and OpenWeather API key
load_dotenv()
api_key = os.getenv("WEATHER_API_KEY")

# Save config information.
url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"

# Build partial query URL
query_url = f"{url}appid={api_key}&units={units}&q="

# List of cities
cities = ["Paris", "London", "Oslo", "Beijing", "Mumbai", "Manila", "New York", "Seattle", "Dallas", "Taipei"]

# set up lists to hold reponse info for:
# latitude, longitude, temp, humidity, and wind speed
lat = []
lon = []
temp = []
humidity = []
wind = []

# Loop through the list of cities and perform a request for data on each
for city in cities:
    response = requests.get(query_url + city).json()
    lat.append(response['coord']['lat'])
    lon.append(response['coord']['lon'])
    temp.append(response['main']['temp'])
    humidity.append(response['main']['humidity'])
    wind.append(response['wind']['speed'])
    

print(f"The latitude information received is: {lat}")
print(f"The longitude information received is: {lon}")
print(f"The temperature information received is: {temp}")
print(f"The humidity information received is: {humidity}")
print(f"The wind speed information received is: {wind}")

# create a DataFrame from cities, latitude, longitude, temp, humidity, and wind speed
weather_dict = {
    "city": cities,
    "lat": lat,
    "lon": lon,
    "temp": temp,
    "humidity": humidity,
    "wind": wind
}
weather_data = pd.DataFrame(weather_dict)
weather_data



# Import dependencies
import requests
import pandas as pd
from dotenv import load_dotenv
import os

# Load environment variables and OpenWeather API key
load_dotenv()
api_key = os.getenv("WEATHER_API_KEY")

# Save config information.
url = "http://api.openweathermap.org/data/2.5/weather?"
units = "metric"

# Build partial query URL
query_url = f"{url}appid={api_key}&units={units}&q="

# List of cities
cities = ["Paris", "London", "Oslo", "Beijing", "Mumbai", "Manila", "New York", "Seattle", "Dallas", "Taipei"]

# set up lists to hold reponse info for:
# latitude, longitude, temp, humidity, and wind speed


# Loop through the list of cities and perform a request for data on each


# create a DataFrame from cities, latitude, longitude, temp, humidity, and wind speed




students = {
    # Name  : Age
    "James": 27,
    "Sarah": 19,
    "Jocelyn": 28
}

print(students["Mary"])

print("This line will never print.")



students = {
    # Name  : Age
    "James": 27,
    "Sarah": 19,
    "Jocelyn": 28
}

# Try to access key that doesn't exist
try:
    students["Mary"]
except KeyError:
    print("Oops, that key doesn't exist.")

# "Catching" the error lets the rest of our code execute
print("...But the program doesn't die early!")



students = {
    # Name  : Age
    "James": 27,
    "Sarah": 19,
    "Jocelyn": 28
}

print(students["Mary"])

print("This line will never print.")

# Your assignment is to get the last line to print without changing any
# of the code below. Instead, wrap each line that throws an error in a
# try/except block.

try:
    print("Infinity looks like + " + str(10 / 0) + ".")
except ZeroDivisionError:
    print("Whoops. Can't do that.")

try:
    print("I think her name was + " + name + "?")
except NameError:
    print("Oh, I forgot to define 'name'. D'oh.")

try:
    print("Your name is a nonsense number. Look: " + int("Gabriel"))
except ValueError:
    print("Drat. 'Gabriel' isn't a number?")

print("I made it through the gauntlet. The message survived!")



# Your assignment is to get the last line to print without changing any
# of the code below. Instead, wrap each line that throws an error in a
# try/except block.

print("Infinity looks like + " + str(10 / 0) + ".")

print("I think her name was + " + name + "?")

print("Your name is a nonsense number. Look: " + int("Gabriel"))

print("You made it through the gauntlet--the message survived!")

# Dependencies
import requests
import pandas as pd

# List of characters
search_characters = ['R2-D2', 'Darth Vader', 'Godzilla', 'Luke Skywalker', 'Frodo',
                     'Boba Fett', 'Iron Man', 'Jon Snow', 'Han Solo']

# Set URL for API
url = 'https://swapi.dev/api/people/?search='

# Set empty lists to hold characters height and mass
height = []
mass = []
starwars_characters = []

# Loop through each character
for character in search_characters:
    
    # Create search query, make request and store in JSON
    query = url + character
    response = requests.get(query)
    response_json = response.json()
    
    # Try to grab the height and mass of characters if they are available in the Star Wars API
    try:
        height.append(response_json['results'][0]['height'])
        mass.append(response_json['results'][0]['mass'])
        starwars_characters.append(character)
        print(f"{character} found! Appending stats")
        
    # Handle exceptions for a character that is not available in the Star Wars API
    except:
        # Print exception note
        print("Character not found")
        pass

# Create DataFrame
character_height = pd.DataFrame({
    'character': starwars_characters,
    'height': height,
    'mass': mass
})
character_height



# Dependencies
import requests
import pandas as pd

# List of characters
search_characters = ['R2-D2', 'Darth Vader', 'Godzilla', 'Luke Skywalker', 'Frodo',
                     'Boba Fett', 'Iron Man', 'Jon Snow', 'Han Solo']

# Set URL for API
url = 'https://swapi.dev/api/people/?search='

# Set empty lists to hold characters height and mass
height = []
mass = []


# Loop through each character

    
    # Create search query, make request and store in JSON

    
    # Try to grab the height and mass of characters if they are available in the Star Wars API

        
    # Handle exceptions for a character that is not available in the Star Wars API


# Create DataFrame




# Import dependencies
import requests
import json
import pandas as pd
from dotenv import load_dotenv
import os
import time

# Load environment variables and New York Times API key
load_dotenv()
api_key = os.getenv("NYT_API_KEY")

# New York Times Article API URL
url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"

# Filter for movie reviews
# section_name should be "Movies"
# type_of_material should be "Review"
filter_query = 'section_name:"Movies" AND type_of_material:"Review"'

# Use a sort filter, sort by newest
sort = "newest"

# Select the following fields to return:
# headline, web_url, snippet, source, keywords, pub_date, byline, word_count
field_list = "headline,web_url,snippet,source,keywords,pub_date,byline,word_count"

# Search for movie reviews published between a begin and end date
# Select reviews published in July 2023
begin_date = "20230701"
end_date = "20230731"

# Build URL
query_url = (
    f"{url}api-key={api_key}&begin_date={begin_date}&end_date={end_date}"
    + f'&fq={filter_query}&sort={sort}&fl={field_list}'
)

# Get the response
response = requests.get(query_url)
response

# Retrieve reviews
reviews = response.json()

# Print results in JSON format
print(json.dumps(reviews, indent=4))

# Convert results list to JSON normalized Pandas DataFrame
reviews_df = pd.json_normalize(reviews["response"]["docs"])
reviews_df

# Get the unique writer bylines as a Series
writers = reviews_df["byline.original"].drop_duplicates()
writers

# Convert the writers Series to a list
writers_list = writers.to_list()
writers_list

# Use the writers list to find the most recent articles by the same writer

# Empty list for results
results_list = []

# loop through the writers_list
for writer in writers_list:
    # Set up the query
    query_url = f"{url}api-key={api_key}&byline:{writer}&sort{sort}&fl={field_list}"
    
    # Get the results
    results = requests.get(query_url).json()
    
    # Add a 12 second interval between queries to stay within API query limits
    time.sleep(12)

    # Use a try-except clause to collect results
    try:
        # Loop through the "docs"
        for doc in results["response"]["docs"]:
            # Save byline.original, headline.main, snippet,
            # and web_url
            results_list.append({
                "byline": doc["byline"]["original"],
                "headline": doc["headline"]["main"],
                "snippet": doc["snippet"],
                "web_url": doc["web_url"]
            })
        print(f"Found articles {writer}")
    except:
        print(f"No articles {writer} found")

# Convert the results to a Pandas DataFrame
results_df = pd.DataFrame(results_list)
results_df



# Import dependencies
import requests
import json
import pandas as pd
from dotenv import load_dotenv
import os
import time

# Load environment variables and New York Times API key


# New York Times Article API URL
url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"

# Filter for movie reviews
# section_name should be "Movies"
# type_of_material should be "Review"
filter_query = 'section_name:"Movies" AND type_of_material:"Review"'

# Use a sort filter, sort by newest
sort = "newest"

# Select the following fields to return:
# headline, web_url, snippet, source, keywords, pub_date, byline, word_count
field_list = "headline,web_url,snippet,source,keywords,pub_date,byline,word_count"

# Search for movie reviews published between a begin and end date
# Select reviews published in July 2023
begin_date = "20230701"
end_date = "20230731"

# Build URL


# Get the response


# Retrieve reviews


# Print results in JSON format


# Convert results list to JSON normalized Pandas DataFrame


# Get the unique writer bylines as a Series


# Convert the writers Series to a list


# Use the writers list to find the most recent articles by the same writer

# Empty list for results


# loop through the writers_list

    # Set up the query

    
    # Get the results

    
    # Add a 12 second interval between queries to stay within API query limits


    # Use a try-except clause to collect results

        # Loop through the "docs"

            # Save byline.original, headline.main, snippet,
            # and web_url


# Convert the results to a Pandas DataFrame




# Dependencies
import requests
import pandas as pd
from census import Census
from dotenv import load_dotenv
import os

# Load environment variables and U.S. Census API key
load_dotenv()
api_key = os.getenv("CENSUS_API_KEY")

# Create an instance of the Census library
c = Census(
    api_key,
    year = 2013
)

# Run Census Search to retrieve data on all zip codes (2013 ACS5 Census)
census_data = c.acs5.get(
    (
        "NAME",
        "B19013_001E",
        "B01003_001E",
        "B01002_001E",
        "B19301_001E",
        "B17001_002E"
    ),
    {'for': 'zip code tabulation area:*'}
)

# Convert to DataFrame
census_pd = pd.DataFrame(census_data)

# Column renaming
census_pd = census_pd.rename(
    columns = {
        "B01003_001E": "Population",
        "B01002_001E": "Median Age",
        "B19013_001E": "Household Income",
        "B19301_001E": "Per Capita Income",
        "B17001_002E": "Poverty Count",
        "NAME": "Name",
        "zip code tabulation area": "Zipcode"
    }
)

# Add a Poverty Rate column (Poverty Count / Population)
census_pd["Poverty Rate"] = 100 * census_pd["Poverty Count"].astype(int) / census_pd["Population"].astype(int)

# Configure the final DataFrame
census_pd = census_pd[
    [
        "Zipcode",
        "Population",
        "Median Age",
        "Household Income",
        "Per Capita Income",
        "Poverty Count",
        "Poverty Rate"
    ]
]

# Display DataFrame length and sample data
print(f"Number of rows in the DataFrame: {len(census_pd)}")
census_pd.head()

# Save the DataFrame as a CSV
# Note: To avoid any issues later, use encoding="utf-8"
census_pd.to_csv("census_data.csv", encoding="utf-8", index=False)



# Dependencies
import requests
import pandas as pd
from census import Census
from dotenv import load_dotenv
import os

# Load environment variables and U.S. Census API key
load_dotenv()
api_key = os.getenv("CENSUS_API_KEY")

# Create an instance of the Census library
c = Census(
    api_key,
    year = 2013
)

# Run Census Search to retrieve data on all zip codes (2013 ACS5 Census)


# Convert to DataFrame


# Column renaming


# Add a Poverty Rate column (Poverty Count / Population)


# Configure the final DataFrame


# Display DataFrame length and sample data


# Save the DataFrame as a CSV
# Note: To avoid any issues later, use encoding="utf-8"




# Import dependencies
import requests
import pandas as pd
from census import Census
from dotenv import load_dotenv
import os

# Load environment variables and U.S. Census API key
load_dotenv()
api_key = os.getenv("CENSUS_API_KEY")

# Create an instance of the Census library
c = Census(
    api_key,
    year = 2020
)

# Run Census Search to retrieve data on all states (2020 ACS5 Census)
census_data = c.acs5.get(
    (
        "NAME",
        "B19013_001E",
        "B01003_001E",
        "B01002_001E",
        "B23025_002E",
        "B25077_001E"
    ), 
    {'for': 'state:*'}
)

# Convert to DataFrame
census_pd = pd.DataFrame(census_data)

# Column renaming
census_pd = census_pd.rename(
    columns = {
        "B01003_001E": "Population",
        "B01002_001E": "Median Age",
        "B19013_001E": "Household Income",
        "B23025_002E": "Employable People in the labor force",
        "B25077_001E": "Median Home Value",
        "NAME": "Name"
    }
)

# Display DataFrame length and sample data
print(f"Number of rows in the DataFrame: {len(census_pd)}")
census_pd.head()

# Save the DataFrame as a CSV
# Note: To avoid any issues later, use encoding="utf-8"
census_pd.to_csv("census_data.csv", encoding="utf-8", index=False)



# Import dependencies
import requests
import pandas as pd
from census import Census
from dotenv import load_dotenv
import os

# Load environment variables and U.S. Census API key
load_dotenv()
api_key = os.getenv("CENSUS_API_KEY")

# Create an instance of the Census library


# Run Census Search to retrieve data on all states (2020 ACS5 Census)


# Convert to DataFrame


# Column renaming


# Display DataFrame length and sample data


# Save the DataFrame as a CSV
# Note: To avoid any issues later, use encoding="utf-8"




# Import dependencies
import requests
import json
import pandas as pd

# Set up author query url
author_query_url = "https://openlibrary.org/search/authors.json?q="

author = input("Which author would you like to search for? ")

# Execute `GET` request with url
response_data = requests.get(author_query_url + author)

# Print `response_data variable`
print(response_data)

# Format data as JSON
data = response_data.json()

# Use json.dumps with argument indent=4 to format data
print(json.dumps(data, indent=4))

# Extract "key" from first result
try:
    author_key = data["docs"][0]["key"]
    print(author_key)
except:
    print("Try again.")

# Use author key to fetch author's works
author_works_url = f"https://openlibrary.org/authors/{author_key}/works.json"

# Execute `GET` request with url
response_data = requests.get(author_works_url)

# Format data as JSON
author_works_data = response_data.json()

# Use json.dumps with argument indent=4 to format data
print(json.dumps(author_works_data, indent=4))

# Print menu of results: 
print("Select the number for the book you would like to view.")
for i in range(len(author_works_data["entries"])):
    print(f'{i}: {author_works_data["entries"][i]["title"]}')


work_selection = input("What is your selection? ")

try:
    work_selection = int(work_selection)
    work_url = "https://openlibrary.org" + author_works_data["entries"][work_selection]["key"]
    print(work_url)
except:
    print("Your chosen selection could not be looked up.")

# Execute `GET` request with url
work_response_data = requests.get(work_url + ".json")
work_data = work_response_data.json()

# Dump JSON about work
print(json.dumps(work_data, indent=4))

# Create Pandas DataFrame for each work by author
#for i in range(len(author_works_data["entries"])):
#    print(f'{i}: {author_works_data["entries"][i]["title"]}')
author_df = pd.json_normalize(author_works_data["entries"])
author_df



# Import dependencies
import requests
import json
import pandas as pd



import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Load the dataset into Pandas dataframe
happiness = pd.read_csv("../Resources/2019_global_happiness.csv")

# Load the dataset reference into a Pandas dataframe
happiness_ref = pd.read_csv("../Resources/2019_global_happiness_reference.csv")

# Review the first 5 rows
happiness.head(5)

# Review the dataset reference
happiness_ref.style.set_properties(subset=['Description'], **{'width': '350px'}, **{'text-align': 'left'})

# Group dataset by country, drop year column, and calculate average values
by_country = happiness.drop(columns=['year']).groupby('country').mean()

# Plot the distribution of happiness in the dataset
happiness_dist = by_country.plot.hist(column=['happiness'],bins=200,xlim=(0,10), width=0.35, figsize=(6,4),
                                         title='Global Happiness Distribution', xticks=range(0,11), 
                                         yticks=[0,10,20], color='cadetblue')

# Bar chart that shows happiness for the 10 happiest countries
top = by_country.sort_values(by=['happiness'], ascending=False).head(15)
top_chart = top.plot.bar(y='happiness', rot=40, width=0.8, figsize=(10,3.5),ylim=(0,10),
                         title='Countries with Highest Average Happiness, 2005-2018',
                         yticks=[0,5,10], xlabel='',color='sandybrown')

# Bar chart that shows happiness for the 10 least happy countries
bottom = by_country.sort_values(by=['happiness']).head(15)
bottom_chart = bottom.plot.bar(y='happiness', rot=60, width=0.8, figsize=(10,3.5),ylim=(0,10),
                         title='Countries with Lowest Average Happiness, 2005-2018',
                         yticks=[0,5,10], xlabel='',color='lightsteelblue')

# Group dataset by year, drop country column, and calculate average values
by_year = happiness.drop(columns=['country']).groupby('year').mean()

# Average global happiness over time
happiness_time_chart = by_year.happiness.plot(figsize=(10,5),ylim=(5,6.5),yticks=(5,6.5),
                                              title='Average Global Happiness Over Time, 2005-2018',
                                              xlabel='',color='orange')

# Average global wealth over time
wealth_time_chart = by_year.wealth.plot(figsize=(10,5),ylim=(8.9,10.1),yticks=(9,10),
                                        title='Average Global Wealth Over Time, 2005-2018',
                                        xlabel='',color='green')

# Scatterplot of wealth and happiness
wealth_happiness = by_country.plot.scatter(x="wealth",y="happiness",title="Relationship between Happiness and Wealth", figsize=(8,4),
                                           xlabel="Wealth",ylabel="Happiness",color="lightcoral",
                                          xlim=(6.5,12),ylim=(3,8),xticks=(),yticks=())

# Scatterplot of happiness and generosity
generosity_happiness = by_country.plot.scatter(x="generosity",y="happiness",title="Relationship between Happiness and Generosity", figsize=(8,4),
                                           xlabel="Generosity",ylabel="Happiness",color="rosybrown",
                                           xticks=(),yticks=())

# Scatterplot of happiness and generosity
wealth_generosity = by_country.plot.scatter(x="wealth",y="generosity",title="Relationship between Wealth and Generosity", figsize=(8,4),
                                           xlabel="Wealth",ylabel="Generosity",color="darkseagreen",
                                           xticks=(),yticks=())



import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Load the dataset into Pandas dataframe
happiness = pd.read_csv("../Resources/2019_global_happiness.csv")

# Load the dataset reference into a Pandas dataframe
happiness_ref = pd.read_csv("../Resources/2019_global_happiness_reference.csv")

# Review the first 5 rows
happiness.head(5)

# Review the dataset reference
happiness_ref.style.set_properties(subset=['Description'], **{'width': '350px'}, **{'text-align': 'left'})

# Group dataset by country, drop year column, and calculate average values
by_country = happiness.drop(columns=['year']).groupby('country').mean()

# Plot the distribution of happiness in the dataset
happiness_dist = by_country.plot.hist(column=['happiness'],bins=200,xlim=(0,10), width=0.35, figsize=(6,4),
                                         title='Global Happiness Distribution', xticks=range(0,11), 
                                         yticks=[0,10,20], color='cadetblue')

# Bar chart that shows happiness for the 10 happiest countries
top = by_country.sort_values(by=['happiness'], ascending=False).head(15)
top_chart = top.plot.bar(y='happiness', rot=40, width=0.8, figsize=(10,3.5),ylim=(0,10),
                         title='Countries with Highest Average Happiness, 2005-2018',
                         yticks=[0,5,10], xlabel='',color='sandybrown')

# Bar chart that shows happiness for the 10 least happy countries
bottom = by_country.sort_values(by=['happiness']).head(15)
bottom_chart = bottom.plot.bar(y='happiness', rot=60, width=0.8, figsize=(10,3.5),ylim=(0,10),
                         title='Countries with Lowest Average Happiness, 2005-2018',
                         yticks=[0,5,10], xlabel='',color='lightsteelblue')

# Group dataset by year, drop country column, and calculate average values
by_year = happiness.drop(columns=['country']).groupby('year').mean()

# Average global happiness over time
happiness_time_chart = by_year.happiness.plot(figsize=(10,5),ylim=(5,6.5),yticks=(5,6.5),
                                              title='Average Global Happiness Over Time, 2005-2018',
                                              xlabel='',color='orange')

# Average global wealth over time
wealth_time_chart = by_year.wealth.plot(figsize=(10,5),ylim=(8.9,10.1),yticks=(9,10),
                                        title='Average Global Wealth Over Time, 2005-2018',
                                        xlabel='',color='green')

# Scatterplot of wealth and happiness
wealth_happiness = by_country.plot.scatter(x="wealth",y="happiness",title="Relationship between Happiness and Wealth", figsize=(8,4),
                                           xlabel="Wealth",ylabel="Happiness",color="lightcoral",
                                          xlim=(6.5,12),ylim=(3,8),xticks=(),yticks=())

# Scatterplot of happiness and generosity
generosity_happiness = by_country.plot.scatter(x="generosity",y="happiness",title="Relationship between Happiness and Generosity", figsize=(8,4),
                                           xlabel="Generosity",ylabel="Happiness",color="rosybrown",
                                           xticks=(),yticks=())

# Scatterplot of happiness and generosity
wealth_generosity = by_country.plot.scatter(x="wealth",y="generosity",title="Relationship between Wealth and Generosity", figsize=(8,4),
                                           xlabel="Wealth",ylabel="Generosity",color="darkseagreen",
                                           xticks=(),yticks=())



# Import Numpy for calculations and matplotlib for charting
import numpy as np
import matplotlib.pyplot as plt

# Creates a numpy array from 0 to 5 with each step being 0.1 higher than the last
x_axis = np.arange(0, 5, 0.1)
x_axis

# Creates an exponential series of values which we can then chart
e_x = [np.exp(x) for x in x_axis]
e_x

# Create a graph based upon the list and array we have created
plt.plot(x_axis, e_x)
# Show the graph that we have created
plt.show()

# Give our graph axis labels
plt.xlabel("Time With MatPlotLib")
plt.ylabel("How Cool MatPlotLib Seems")

# Have to plot our chart once again as it doesn't stick after being shown
plt.plot(x_axis, e_x)
plt.show()

# Create our x_axis numpy array
x_axis = np.arange(0, 6, 0.1)

# Creates a numpy array based on the sin of our x_axis values
sin = np.sin(x_axis)

# Creates a numpy array based on the cos of our x_axis values
cos = np.cos(x_axis)

# Plot both of these lines so that they will appear on our final chart
plt.plot(x_axis, sin)
plt.plot(x_axis, cos)

plt.show()



# Import Numpy for calculations and matplotlib for charting
import numpy as np
import matplotlib.pyplot as plt

# Creates a numpy array from 0 to 5 with each step being 0.1 higher than the last


# Creates an exponential series of values which we can then chart


# Create a graph based upon the list and array we have created

# Show the graph that we have created


# Give our graph axis labels


# Have to plot our chart once again as it doesn't stick after being shown


# Create our x_axis numpy array


# Creates a numpy array based on the sin of our x_axis values


# Creates a numpy array based on the cos of our x_axis values


# Plot both of these lines so that they will appear on our final chart




# Dependencies
import numpy as np
import matplotlib.pyplot as plt

# Set x axis to numerical value for month
x_axis_data = np.arange(1,13,1)
x_axis_data

# Average weather temp
points = [39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]

# Plot the line
plt.plot(x_axis_data, points)
plt.show()

# Convert to Celsius C = (F-32) * 0.56
points_C = [(x-32) * 0.56 for x in points]
points_C

# Plot using Celsius
plt.plot(x_axis_data, points_C)
plt.show()

# Plot both on the same chart
plt.plot(x_axis_data, points)
plt.plot(x_axis_data, points_C)
plt.show()



# Dependencies
import numpy as np
import matplotlib.pyplot as plt

# Set x axis to numerical value for month


# Average weather temp
points = [39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]

# Plot the line


# Convert to Celsius C = (F-32) * 0.56


# Plot using Celsius


# Plot both on the same chart




%matplotlib notebook

# Dependencies
import matplotlib.pyplot as plt
import numpy as np

# Generate the x values from 0 to 10 using a step of 0.1
x_axis = np.arange(0, 10, 0.1)
sin = np.sin(x_axis)
cos = np.cos(x_axis)

# Use dots or other markers for your plots, and change their colors
plt.plot(x_axis, sin, linewidth=0, marker="o", color="blue", label="Sine")
plt.plot(x_axis, cos, linewidth=0, marker="^", color="red", label="Cosine")
plt.show()

# Add a semi-transparent horizontal line at y = 0
plt.hlines(0, 0, 10, alpha=0.25)

# Add labels to the x and y axes
plt.title("Juxtaposed Sine and Cosine Curves")
plt.xlabel("Input (Sampled Real Numbers from 0 to 10)")
plt.ylabel("Value of Sine (blue) and Cosine (red)")
plt.show()

# Set your x and y limits
plt.xlim(0, 10)
plt.ylim(-1, 1)
plt.show()

# Set a grid on the plot
plt.grid()
plt.show()

# Adds a legend and sets its location to the lower right
plt.legend(loc="lower right")
plt.show()

# Save the plot and display it
plt.savefig("../Images/sin_cos_with_markers.png")
plt.show()

%matplotlib notebook

# Dependencies
import matplotlib.pyplot as plt
import numpy as np

# Generate the x values from 0 to 10 using a step of 0.1


# Use dots or other markers for your plots, and change their colors


# Add a semi-transparent horizontal line at y = 0


# Add labels to the x and y axes


# Set your x and y limits


# Set a grid on the plot


# Adds a legend and sets its location to the lower right


# Save the plot and display it


# Include this line to make plots interactive
%matplotlib notebook

# Dependencies
import matplotlib.pyplot as plt
import numpy as np

# Set x axis to numerical value for month
x_axis = np.arange(1,13,1)
x_axis

# Avearge weather temp
points_F = [39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]

# Convert to Celsius C = (F-32) * 0.56
points_C = [(x-32) * 0.56 for x in points_F]
points_C

# Create a handle for each plot
fahrenheit, = plt.plot(x_axis, points_F, marker="+",color="blue", linewidth=1, label="Fahrenheit")
celsius, = plt.plot(x_axis, points_C, marker="s", color="Red", linewidth=1, label="Celsius")
plt.show()

# Set our legend to where the chart thinks is best
plt.legend(handles=[fahrenheit, celsius], loc="best")
plt.show()

# Create labels for the X and Y axis
plt.xlabel("Months")
plt.ylabel("Degrees")
plt.show()

# Save and display the chart
plt.savefig("../Images/avg_temp.png")
plt.show()



# Include this line to make plots interactive
%matplotlib notebook

# Dependencies
import matplotlib.pyplot as plt
import numpy as np

# Set x axis to numerical value for month


# Avearge weather temp
points_F = [39, 42, 51, 62, 72, 82, 86, 84, 77, 65, 55, 44]

# Convert to Celsius C = (F-32) * 0.56


# Create a handle for each plot


# Set our legend to where the chart thinks is best


# Create labels for the X and Y axis


# Save and display the chart




%matplotlib notebook

import matplotlib.pyplot as plt
import numpy as np

# Create an array that contains the number of users each language has
users = [13000, 26000, 52000, 30000, 9000]
x_axis = np.arange(len(users))

# Tell matplotlib that we will be making a bar chart
# Users is our y axis and x_axis is, of course, our x axis
# We apply align="center" to ensure our bars line up with our tick marks
plt.bar(x_axis, users, color='r', alpha=0.5, align="center")

# Tell matplotlib where we would like to place each of our x axis headers
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, ["Java", "C++", "Python", "Ruby", "Clojure"])
plt.show()

# Sets the x limits of the current chart
plt.xlim(-0.75, len(x_axis)-0.25)
plt.show()

# Sets the y limits of the current chart
plt.ylim(0, max(users)+5000)
plt.show()

# Give our chart some labels and a tile
plt.title("Popularity of Programming Languages")
plt.xlabel("Programming Language")
plt.ylabel("Number of People Using Programming Languages")
plt.show()



%matplotlib notebook

import matplotlib.pyplot as plt
import numpy as np

# Create an array that contains the number of users each language has
users = [13000, 26000, 52000, 30000, 9000]
x_axis = np.arange(len(users))

# Tell matplotlib that we will be making a bar chart
# Users is our y axis and x_axis is, of course, our x axis
# We apply align="center" to ensure our bars line up with our tick marks


# Tell matplotlib where we would like to place each of our x axis headers


# Sets the x limits of the current chart


# Sets the y limits of the current chart


# Give our chart some labels and a tile


%matplotlib notebook

import matplotlib.pyplot as plt
import numpy as np

cities = ["San Francisco", "Omaha", "New Orleans", "Cincinnati", "Pittsburgh"]
cars_in_cities = [214.7, 564.4, 416.5, 466.7, 350.6]
x_axis = np.arange(len(cars_in_cities))

# Create a bar chart based upon the above data
plt.bar(x_axis, cars_in_cities, color="b", align="center")

# Create the ticks for our bar chart's x axis
tick_locations = [value for value in x_axis]
plt.xticks(tick_locations, cities)
plt.show()

# Set the limits of the x axis
plt.xlim(-0.75, len(x_axis)-0.25)
plt.show()

# Set the limits of the y axis
plt.ylim(0, max(cars_in_cities)+10)
plt.show()

# Give the chart a title, x label, and y label
plt.title("Density of Commuting Cars in Cities")
plt.xlabel("Cities")
plt.ylabel("Commuting Cars Per 1,000 Population Age 16+")
plt.show()

# Save an image of the chart and print it to the screen
plt.savefig("../Images/CarDensity.png")
plt.show()

%matplotlib notebook

import matplotlib.pyplot as plt
import numpy as np

cities = ["San Francisco", "Omaha", "New Orleans", "Cincinnati", "Pittsburgh"]
cars_in_cities = [214.7, 564.4, 416.5, 466.7, 350.6]
x_axis = np.arange(len(cars_in_cities))

# Create a bar chart based upon the above data


# Create the ticks for our bar chart's x axis


# Set the limits of the x axis


# Set the limits of the y axis


# Give the chart a title, x label, and y label


# Save an image of the chart and print it to the screen


%matplotlib notebook

# Import Dependencies
import random
import matplotlib.pyplot as plt
import numpy as np

# The maximum x value for our chart will be 100
x_limit = 100

# List of values from 0 to 100 each value being 1 greater than the last
x_axis = np.arange(0, x_limit, 1)

# Create a random array of data that we will use for our y values
data = [random.random() for value in x_axis]

# Tells matplotlib that we want to make a scatter plot
# The size of each point on our plot is determined by their x value
plt.scatter(x_axis, data, marker="o", facecolors="red", edgecolors="black",
            s=x_axis, alpha=0.75)
plt.show()

# The y limits of our scatter plot is 0 to 1
plt.ylim(0, 1)
plt.show()

# The x limits of our scatter plot is 0 to 100
plt.xlim(0, x_limit)
plt.show()

%matplotlib notebook

# Import Dependencies
import random
import matplotlib.pyplot as plt
import numpy as np

# The maximum x value for our chart will be 100
x_limit = 100

# List of values from 0 to 100 each value being 1 greater than the last

# Create a random array of data that we will use for our y values


# Tells matplotlib that we want to make a scatter plot
# The size of each point on our plot is determined by their x value


# The y limits of our scatter plot is 0 to 1


# The x limits of our scatter plot is 0 to 100


%matplotlib notebook

import matplotlib.pyplot as plt
import numpy as np

temp = [14.2, 16.4, 11.9, 15.2, 18.5, 22.1, 19.4, 25.1, 23.4, 18.1, 22.6, 17.2]
sales = [215, 325, 185, 332, 406, 522, 412, 614, 544, 421, 445, 408]

# Tell matplotlib to create a scatter plot based upon the above data

# Without scoop_price
#plt.scatter(temp, sales, marker="o", facecolors="red", edgecolors="black")

# BONUS: With scoop_price set to the scalar value
scoop_price = [89, 18, 10, 28, 79, 46, 29, 38, 89, 26, 45, 62]
plt.scatter(temp, sales, marker="o", facecolors="red", edgecolors="black", s=scoop_price)
plt.show()

# Set the upper and lower limits of our y axis
plt.ylim(180,620)
plt.show()

# Set the upper and lower limits of our x axis
plt.xlim(11,26)
plt.show()

# Create a title, x label, and y label for our chart
plt.title("Ice Cream Sales v Temperature")
plt.xlabel("Temperature (Celsius)")
plt.ylabel("Sales (Dollars)")
plt.show()

# Save an image of the chart and print to screen
plt.savefig("../Images/IceCreamSales.png")
plt.show()

%matplotlib notebook

import matplotlib.pyplot as plt
import numpy as np

temp = [14.2, 16.4, 11.9, 15.2, 18.5, 22.1, 19.4, 25.1, 23.4, 18.1, 22.6, 17.2]
sales = [215, 325, 185, 332, 406, 522, 412, 614, 544, 421, 445, 408]

# Tell matplotlib to create a scatter plot based upon the above data
# Without scoop_price

# BONUS: With scoop_price set to the scalar value
# scoop_price = [89, 18, 10, 28, 79, 46, 29, 38, 89, 26, 45, 62]


# Set the upper and lower limits of our y axis


# Set the upper and lower limits of our x axis


# Create a title, x label, and y label for our chart


# Save an image of the chart and print to screen


%matplotlib notebook

# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Load in csv
rain_df = pd.read_csv("../Resources/avg_rain_state.csv")
rain_df.head()

# Set x axis and tick locations
x_axis = np.arange(len(rain_df))
tick_locations = [value+0.4 for value in x_axis]

# Create a list indicating where to write x labels and set figure size to adjust for space
plt.figure(figsize=(20,4))
plt.bar(x_axis, rain_df["Inches"], color='r', alpha=0.5, align="edge")
plt.xticks(tick_locations, rain_df["State"], rotation="vertical")
plt.show()

# Set x and y limits
plt.xlim(-0.25, len(x_axis))
plt.ylim(0, max(rain_df["Inches"])+10)
plt.show()

# Set a Title and labels
plt.title("Average Rain per State")
plt.xlabel("State")
plt.ylabel("Average Amount of Rainfall in Inches")
plt.show()

# Save our graph and show the graph
plt.tight_layout()
plt.savefig("../Images/avg_state_rain.png")
plt.show()



%matplotlib notebook

# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Load in csv


# Set x axis and tick locations


# Create a list indicating where to write x labels and set figure size to adjust for space


# Set x and y limits


# Set a Title and labels


# Save our graph and show the graph


# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Load in csv
rain_df = pd.read_csv("../Resources/avg_rain_state.csv")
rain_df.head()

# Set x axis and tick locations
x_axis = np.arange(len(rain_df))
tick_locations = [value for value in x_axis]

# Create and customize a bar chart

# Create a list indicating where to write x labels and set figure size to adjust for space
plt.figure(figsize=(20,4))
plt.bar(x_axis, rain_df["Inches"], color='r', alpha=0.5, align="center")
plt.xticks(tick_locations, rain_df["State"], rotation="vertical")

# Set x and y limits
plt.xlim(-0.75, len(x_axis))
plt.ylim(0, max(rain_df["Inches"])+10)

# Set a Title and labels
plt.title("Average Rain per State")
plt.xlabel("State")
plt.ylabel("Average Amount of Rainfall in Inches")
plt.show()

# Save our graph and show the grap
plt.tight_layout()
plt.savefig("../Images/avg_state_rain.png")
plt.show()

# Filter the DataFrame down only to those columns to chart
state_and_inches = rain_df[["State","Inches"]]

# Set the index to be "State" so they will be used as labels
state_and_inches = state_and_inches.set_index("State")

state_and_inches.head()

# Use DataFrame.plot() in order to create a bar chart of the data
state_and_inches.plot(kind="bar", figsize=(20,3.5))

# Set a title for the chart
plt.title("Average Rain Per State")

plt.show()
plt.tight_layout()

# Pandas can also plot multiple columns if the DataFrame includes them
multi_plot = rain_df.plot(kind="bar", figsize=(20,5))

# PandasPlot.set_xticklabels() can be used to set the tick labels as well
multi_plot.set_xticklabels(rain_df["State"], rotation=45)

plt.show()
plt.tight_layout()

# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Load in csv
rain_df = pd.read_csv("../Resources/avg_rain_state.csv")
rain_df.head()

# Set x axis and tick locations
x_axis = np.arange(len(rain_df))
tick_locations = [value for value in x_axis]

# Create and customize a bar chart

# Create a list indicating where to write x labels and set figure size to adjust for space
plt.figure(figsize=(20,4))
plt.bar(x_axis, rain_df["Inches"], color='r', alpha=0.5, align="center")
plt.xticks(tick_locations, rain_df["State"], rotation="vertical")

# Set x and y limits
plt.xlim(-0.75, len(x_axis))
plt.ylim(0, max(rain_df["Inches"])+10)

# Set a Title and labels
plt.title("Average Rain per State")
plt.xlabel("State")
plt.ylabel("Average Amount of Rainfall in Inches")
plt.show()

# Save our graph and show the grap
plt.tight_layout()
plt.savefig("../Images/avg_state_rain.png")
plt.show()

# Filter the DataFrame down only to those columns to chart


# Set the index to be "State" so they will be used as labels




# Use DataFrame.plot() in order to create a bar chart of the data


# Set a title for the chart



# Pandas can also plot multiple columns if the DataFrame includes them


# PandasPlot.set_xticklabels() can be used to set the tick labels as well



%matplotlib notebook

# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Read CSV
settlement_data = pd.read_csv("Resources/union_settlements_1995.csv")
settlement_data.head()

# List columns
settlement_data.columns

# Get total settlements by union
union_data = settlement_data["UNION"].value_counts()

# Configure plot, figsize, title, and axis labels
figure1 = union_data.plot(kind="bar", facecolor="red", figsize=(8,6),
                                title="Major Collective Bargaining Settlements (1995)",
                                xlabel="Union",
                                ylabel="Settlements")

# Configure x-tick rotation
xticklabels = union_data.index
figure1.set_xticklabels(xticklabels, rotation=45, rotation_mode="anchor", ha="right", wrap=True)

# Show plot
plt.show()

# Resize plot to display labels
plt.tight_layout()
plt.show()



%matplotlib notebook

# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Read CSV


# List columns


# Get total settlements by union


# Configure plot, figsize, title, and axis labels

# Configure x-tick rotation

# Show plot


# Resize plot to display labels




%matplotlib notebook

# Import Dependencies
import matplotlib.pyplot as plt
import pandas as pd

# Import our data into pandas from CSV
accident_string = '../Resources/accidents.csv'
accidents_df = pd.read_csv(accident_string, low_memory=False)

accidents_df

# Create a group based on the values in the 'FUNC_SYSNAME' column
# 'FUNC_SYSNAME' stores the type of road the accident occurred
accident_road_type = accidents_df.groupby('FUNC_SYSNAME')

# Count how many times each road type appears in our group
count_road_types = accident_road_type['FUNC_SYSNAME'].count()

count_road_types

# Create a bar chart based off of the group series from before
count_chart = count_road_types.plot(kind='bar', figsize=(6,8))

# Set the xlabel and ylabel using class methods
count_chart.set_xlabel("Road Type")
count_chart.set_ylabel("Number of Accidents")

plt.show()
plt.tight_layout()



%matplotlib notebook

# Import Dependencies
import matplotlib.pyplot as plt
import pandas as pd

# Import our data into pandas from CSV


# Create a group based on the values in the 'FUNC_SYSNAME' column
# 'FUNC_SYSNAME' stores the type of road the accident occurred

# Count how many times each road type appears in our group


# Create a bar chart based off of the group series from before

# Set the xlabel and ylabel using class methods




# Import Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Import our data into pandas from CSV
file_path = '../Resources/library_usage.csv'
library_usage_df = pd.read_csv(file_path, low_memory=False)

library_usage_df.head()

# Filter data so it only includes patrons who checked out at least one item
library_loans_df = pd.DataFrame(library_usage_df.loc[library_usage_df['Total Checkouts']>0,:])

# Split up our data into groups based upon 'Patron Type Definition'
patron_groups = library_loans_df.groupby('Patron Type Definition')

# Find out how many of each patron type borrowed library items
patron_borrows = patron_groups['Total Checkouts'].count()

# Chart our data, give it a title, and label the axes
patron_chart = patron_borrows.plot(kind="bar", title="Library Usage by Patron Type")
patron_chart.set_xlabel("Patron Type")
patron_chart.set_ylabel("Number of Patrons Borrowing Items")

plt.show()
plt.tight_layout()

# Split up our data into groups based upon 'Home Library Definition' and 'Patron Type Definition'
library_groups = library_usage_df.groupby(['Home Library Definition','Patron Type Definition'])

# Create a new variable that holds the sum of our groups
sum_it_up = library_groups[['Total Checkouts']].sum()
sum_it_up.head(20)

# Make a variable called branch and store a 'Home Library Definition' in it
branch = "Anza"

# Make a variable called min_checkouts that you can change depending on how busy the library branch you've chosen is
min_checkouts = 5000

# Collect the loans of the branch above
just_one_branch = sum_it_up.loc[branch]

# filter the data to patron types with greater than the value set for min_checkouts
just_one_branch = just_one_branch.loc[just_one_branch['Total Checkouts']>min_checkouts,:]

# Create a bar chart based upon the total checkouts (or loans) of that single branch
branch_bar = just_one_branch.plot(kind="bar", y='Total Checkouts', title=("Loans of " + branch + 
                                                                          " Branch for Patron Types Over "
                                                                         + str(min_checkouts) + " Loaned Items"))
branch_bar.set_xlabel("Patron Type")
branch_bar.set_ylabel("Number of Patrons Borrowing Items")



plt.show()
plt.tight_layout()



# Import Dependencies
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Import our data into pandas from CSV
file_path = '../Resources/library_usage.csv'
library_usage_df = pd.read_csv(file_path, low_memory=False)

library_usage_df.head()

# Filter data so it only includes patrons who checked out at least one item
library_loans_df=library_usage_df.loc[library_usage_df['Total Checkouts']>0,:]
library_loans_df


# Split up our data into groups based upon 'Patron Type Definition'
patron_groups=library_loans_df.groupby('Patron Type Definition')

# Find out how many of each patron type borrowed library items
patron_borrowers=patron_groups['Total Checkouts'].count()
patron_borrowers


# Chart our data, give it a title, and label the axes
patron_chart=patron_borrowers.plot(kind="bar", title="Library Usage by Patron type")
patron_chart.set_xlabel("Patron Type")
patron_chart.set_ylabel("Number of Patrons Borrwowing Items")

plt.show()
plt.tight_layout

# Split up our data into groups based upon 'Home Library Definition' and 'Patron Type Definition'
library_groups=library_usage_df.groupby(['Home Library Definition','Patron Type Definition'])

# Create a new variable that holds the sum of our groups
sum_it_up=library_groups['Total Checkouts'].sum()
sum_it_up.head(30)


# Make a variable called branch and store a 'Home Library Definition' in it
branch='Anza'

# Make a variable called min_checkouts that you can change depending on how busy the library branch you've chosen is
min_checkouts=5000

# Collect the loans of the branch above
just_one_branch=sum_it_up.loc[branch]

# filter the data to patron types with greater than the value set for min_checkouts
# just_one_branch=just_one_branch.loc[just_one_branch['Total Checkouts']>min_checkouts,:]
# just_one_branch
# Create a bar chart based upon the total checkouts (or loans) of that single branch
branch_bar=just_one_branch.plot(kind="bar", title=(f"Loans of {branch} Branch for Patron types over {min_checkouts} Loaned Items"))
plt.show()
plt.tight_layout



# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

car_data = pd.read_csv('../Resources/mpg.csv')
car_data.head()

# Remove the rows with missing values in horsepower
car_data = car_data.loc[car_data['horsepower'] != "?"]
car_data.head()

# Set the 'car name' as our index
car_data = car_data.set_index('car name')

# Remove the 'origin' column
del car_data['origin']

car_data.head()

# Convert the "horsepower" column to numeric so the data can be used
car_data['horsepower'] = pd.to_numeric(car_data['horsepower'])

# Create a scatter plot which compares MPG to horsepower
car_data.plot(kind="scatter", x="horsepower", y="mpg", grid=True, figsize=(8,8),
              title="MPG Vs. Horsepower")
plt.show()




%matplotlib notebook

# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

car_data = pd.read_csv('../Resources/mpg.csv')
car_data.head()

# Remove the rows with missing values in horsepower


# Set the 'car name' as our index

# Remove the 'origin' column


# Convert the "horsepower" column to numeric so the data can be used


# Create a scatter plot which compares MPG to horsepower





# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Read CSV
unemployed_data_one = pd.read_csv("../Resources/unemployment_2010-2015.csv")
unemployed_data_two = pd.read_csv("../Resources/unemployment_2016-2020.csv")

# Merge our two data frames together
combined_unemployed_data = pd.merge(unemployed_data_one, unemployed_data_two, on="Country Name")
combined_unemployed_data.head()

# Delete the duplicate 'Country Code' column and rename the first one back to 'Country Code'
del combined_unemployed_data['Country Code_y']
combined_unemployed_data = combined_unemployed_data.rename(columns={"Country Code_x":"Country Code"})
combined_unemployed_data.head()

# Set the 'Country Code' to be our index for easy referencing of rows
combined_unemployed_data = combined_unemployed_data.set_index("Country Code")

# Collect the mean unemployment rates for the world
average_unemployment = combined_unemployed_data[[str(year) for year in range(2010, 2021)]].mean()

# Collect the years where data was collected
years = average_unemployment.keys()

# Plot the world average as a line chart
world_avg, = plt.plot(years, average_unemployment, color="blue", label="World Average" )

# Plot the unemployment values for a single country
country_one, = plt.plot(years, combined_unemployed_data.loc['USA',["2010","2011","2012","2013","2014","2015",
                                                                  "2016","2017","2018","2019","2020"]], 
                        color="green",label=combined_unemployed_data.loc['USA',"Country Name"])

# Create a legend for our chart
plt.legend(handles=[world_avg, country_one], loc="best")

# Show the chart
plt.show()

average_unemployment.plot(label="World Average")
combined_unemployed_data.loc['USA', "2010":"2020"].plot(label="United States")
plt.legend()
plt.show()



# Dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Read CSV
unemployed_data_one = pd.read_csv("../Resources/unemployment_2010-2015.csv")
unemployed_data_two = pd.read_csv("../Resources/unemployment_2016-2020.csv")

# Merge our two data frames together


# Delete the duplicate 'Country Code' column and rename the first one back to 'Country Code'


# Set the 'Country Code' to be our index for easy referencing of rows


# Collect the mean unemployment rates for the world

# Collect the years where data was collected


# Plot the world average as a line chart

# Plot the unemployment values for a single country

# Create a legend for our chart

# Show the chart


# Plot the same data using the Pandas plot function




import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Take in all of our traveler data and read it into pandas
travel_2016 = "../Resources/2016_travelers.csv"
travel_2017 = "../Resources/2017_travelers.csv"
travel_2018 = "../Resources/2018_travelers.csv"

travel_2016_df = pd.read_csv(travel_2016)
travel_2017_df = pd.read_csv(travel_2017)
travel_2018_df = pd.read_csv(travel_2018)


# Merge the first two datasets on "COUNTRY OF NATIONALITY" so that no data is lost (should be 44 rows)
combined_travel_df = pd.merge(travel_2016_df, travel_2017_df,
                                 how='outer', on='COUNTRY OF NATIONALITY')
combined_travel_df.head()

# Rename our _x columns to "2016 Alone", "2016 With Spouse", "2016 With Children", "2016 With Family/Relatives",
# "2016 Student Group", "2016 With Friends", "2016 With Business Associate", "2016 With Incentive Group",
# and "2016 Others"

combined_travel_df = combined_travel_df.rename(columns={"ALONE_x":"2016 Alone",
                                                        "WITH SPOUSE_x":"2016 With Spouse",
                                                        "WITH CHILDREN_x":"2016 With Children",
                                                        "WITH FAMILY/RELATIVES_x":"2016 With Family/Relatives",
                                                        "STUDENT GROUP_x":"2016 Student Group",
                                                        "WITH FRIENDS_x":"2016 With Friends",
                                                        "WITH BUSINESS ACCOCIATE_x":"2016 With Business Associate",
                                                        "WITH INCENTIVE GROUP_x":"2016 With Incentive Group",
                                                        "OTHERS_x":"2016 Others"})

# Rename our _y columns to "2016 Alone", "2016 With Spouse", "2016 With Children", "2016 With Family/Relatives",
# "2016 Student Group", "2016 With Friends", "2016 With Business Associate", "2016 With Incentive Group",
# and "2016 Others"
combined_travel_df = combined_travel_df.rename(columns={"ALONE_y":"2017 Alone",
                                                        "WITH SPOUSE_y":"2017 With Spouse",
                                                        "WITH CHILDREN_y":"2017 With Children",
                                                        "WITH FAMILY/RELATIVES_y":"2017 With Family/Relatives",
                                                        "STUDENT GROUP_y":"2017 Student Group",
                                                        "WITH FRIENDS_y":"2017 With Friends",
                                                        "WITH BUSINESS ACCOCIATE_y":"2017 With Business Associate",
                                                        "WITH INCENTIVE GROUP_y":"2017 With Incentive Group",
                                                        "OTHERS_y":"2017 Others"})

combined_travel_df.head()

# Merge our newly combined dataframe with the 2018 dataframe
combined_travel_df = pd.merge(combined_travel_df, travel_2018_df, how="outer", on="COUNTRY OF NATIONALITY")
combined_travel_df

# Rename "ALONE", "WITH SPOUSE", "WITH CHILDREN", "WITH FAMILY/RELATIVES", "STUDENT GROUP", "WITH FRIENDS",
# "WITH BUSINESS ACCOCIATE","WITH INCENTIVE GROUP", "OTHERS" to 
# "2018 Alone", "2018 With Spouse", "2018 With Children", "2018 With Family/Relatives", "2018 Student Group", 
# "2018 With Friends", "2018 With Business Associate", "2018 With Incentive Group", and "2018 Others"
combined_travel_df = combined_travel_df.rename(columns={"ALONE":"2018 Alone",
                                                        "WITH SPOUSE":"2018 With Spouse",
                                                        "WITH CHILDREN":"2018 With Children",
                                                        "WITH FAMILY/RELATIVES":"2018 With Family/Relatives",
                                                        "STUDENT GROUP":"2018 Student Group",
                                                        "WITH FRIENDS":"2018 With Friends",
                                                        "WITH BUSINESS ACCOCIATE":"2018 With Business Associate",
                                                        "WITH INCENTIVE GROUP":"2018 With Incentive Group",
                                                        "OTHERS":"2018 Others"})

combined_travel_df.head()





import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Take in all of our traveler data and read it into pandas
travel_2016 = "../Resources/2016_travelers.csv"
travel_2017 = "../Resources/2017_travelers.csv"
travel_2018 = "../Resources/2018_travelers.csv"

travel_2016_df = pd.read_csv(travel_2016)
travel_2017_df = pd.read_csv(travel_2017)
travel_2018_df = pd.read_csv(travel_2018)


# Merge the first two datasets on "COUNTRY OF NATIONALITY" so that no data is lost (should be 44 rows)


# Rename our _x columns to "2016 Alone", "2016 With Spouse", "2016 With Children", "2016 With Family/Relatives",
# "2016 Student Group", "2016 With Friends", "2016 With Business Associate", "2016 With Incentive Group",
# and "2016 Others"

# Rename our _y columns to "2016 Alone", "2016 With Spouse", "2016 With Children", "2016 With Family/Relatives",
# "2016 Student Group", "2016 With Friends", "2016 With Business Associate", "2016 With Incentive Group",
# and "2016 Others"



# Merge our newly combined dataframe with the 2018 dataframe


# Rename "ALONE", "WITH SPOUSE", "WITH CHILDREN", "WITH FAMILY/RELATIVES", "STUDENT GROUP", "WITH FRIENDS",
# "WITH BUSINESS ACCOCIATE","WITH INCENTIVE GROUP", "OTHERS" to 
# "2018 Alone", "2018 With Spouse", "2018 With Children", "2018 With Family/Relatives", "2018 Student Group", 
# "2018 With Friends", "2018 With Business Associate", "2018 With Incentive Group", and "2018 Others"




import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Take in all of our traveler data and read it into pandas
travel_2016 = "../Resources/2016_travelers.csv"
travel_2017 = "../Resources/2017_travelers.csv"
travel_2018 = "../Resources/2018_travelers.csv"

travel_2016_df = pd.read_csv(travel_2016)
travel_2017_df = pd.read_csv(travel_2017)
travel_2018_df = pd.read_csv(travel_2018)


# Merge the first two datasets on "COUNTRY OF NATIONALITY" so that no data is lost (should be 44 rows)
combined_travel_df = pd.merge(travel_2016_df, travel_2017_df,
                                 how='outer', on='COUNTRY OF NATIONALITY')
combined_travel_df.head()

# Rename our _x columns to "2016 Alone", "2016 With Spouse", "2016 With Children", "2016 With Family/Relatives",
# "2016 Student Group", "2016 With Friends", "2016 With Business Associate", "2016 With Incentive Group",
# and "2016 Others"

combined_travel_df = combined_travel_df.rename(columns={"ALONE_x":"2016 Alone",
                                                        "WITH SPOUSE_x":"2016 With Spouse",
                                                        "WITH CHILDREN_x":"2016 With Children",
                                                        "WITH FAMILY/RELATIVES_x":"2016 With Family/Relatives",
                                                        "STUDENT GROUP_x":"2016 Student Group",
                                                        "WITH FRIENDS_x":"2016 With Friends",
                                                        "WITH BUSINESS ACCOCIATE_x":"2016 With Business Associate",
                                                        "WITH INCENTIVE GROUP_x":"2016 With Incentive Group",
                                                        "OTHERS_x":"2016 Others"})

# Rename our _y columns to "2016 Alone", "2016 With Spouse", "2016 With Children", "2016 With Family/Relatives",
# "2016 Student Group", "2016 With Friends", "2016 With Business Associate", "2016 With Incentive Group",
# and "2016 Others"
combined_travel_df = combined_travel_df.rename(columns={"ALONE_y":"2017 Alone",
                                                        "WITH SPOUSE_y":"2017 With Spouse",
                                                        "WITH CHILDREN_y":"2017 With Children",
                                                        "WITH FAMILY/RELATIVES_y":"2017 With Family/Relatives",
                                                        "STUDENT GROUP_y":"2017 Student Group",
                                                        "WITH FRIENDS_y":"2017 With Friends",
                                                        "WITH BUSINESS ACCOCIATE_y":"2017 With Business Associate",
                                                        "WITH INCENTIVE GROUP_y":"2017 With Incentive Group",
                                                        "OTHERS_y":"2017 Others"})

combined_travel_df.head()

# Merge our newly combined dataframe with the 2018 dataframe
combined_travel_df = pd.merge(combined_travel_df, travel_2018_df, how="outer", on="COUNTRY OF NATIONALITY")
combined_travel_df

# Rename "ALONE", "WITH SPOUSE", "WITH CHILDREN", "WITH FAMILY/RELATIVES", "STUDENT GROUP", "WITH FRIENDS",
# "WITH BUSINESS ACCOCIATE","WITH INCENTIVE GROUP", "OTHERS" to 
# "2018 Alone", "2018 With Spouse", "2018 With Children", "2018 With Family/Relatives", "2018 Student Group", 
# "2018 With Friends", "2018 With Business Associate", "2018 With Incentive Group", and "2018 Others"
combined_travel_df = combined_travel_df.rename(columns={"ALONE":"2018 Alone",
                                                        "WITH SPOUSE":"2018 With Spouse",
                                                        "WITH CHILDREN":"2018 With Children",
                                                        "WITH FAMILY/RELATIVES":"2018 With Family/Relatives",
                                                        "STUDENT GROUP":"2018 Student Group",
                                                        "WITH FRIENDS":"2018 With Friends",
                                                        "WITH BUSINESS ACCOCIATE":"2018 With Business Associate",
                                                        "WITH INCENTIVE GROUP":"2018 With Incentive Group",
                                                        "OTHERS":"2018 Others"})

combined_travel_df.head()

# Check the mean of the columns
combined_travel_df.select_dtypes(include=[np.number]).mean()

# Reduce columns where mean of traveling companions is > 1 across all years
travel_reduced = pd.DataFrame(combined_travel_df[["COUNTRY OF NATIONALITY",
                                                  "2016 Alone","2016 With Spouse","2016 With Children",
                                                  "2016 With Family/Relatives","2016 With Friends",
                                                  "2016 With Business Associate","2017 Alone",
                                                  "2017 With Spouse","2017 With Children",
                                                  "2017 With Family/Relatives","2017 With Friends",
                                                  "2017 With Business Associate","2018 Alone",
                                                  "2018 With Spouse","2018 With Children",
                                                  "2018 With Family/Relatives","2018 With Friends",
                                                  "2018 With Business Associate"]])

# Set index to "Country of Nationality"
travel_reduced = travel_reduced.set_index("COUNTRY OF NATIONALITY")
travel_reduced



import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Take in all of our traveler data and read it into pandas
travel_2016 = "../Resources/2016_travelers.csv"
travel_2017 = "../Resources/2017_travelers.csv"
travel_2018 = "../Resources/2018_travelers.csv"

travel_2016_df = pd.read_csv(travel_2016)
travel_2017_df = pd.read_csv(travel_2017)
travel_2018_df = pd.read_csv(travel_2018)


# Merge the first two datasets on "COUNTRY OF NATIONALITY" so that no data is lost (should be 44 rows)
combined_travel_df = pd.merge(travel_2016_df, travel_2017_df,
                                 how='outer', on='COUNTRY OF NATIONALITY')
combined_travel_df.head()

# Rename our _x columns to "2016 Alone", "2016 With Spouse", "2016 With Children", "2016 With Family/Relatives",
# "2016 Student Group", "2016 With Friends", "2016 With Business Associate", "2016 With Incentive Group",
# and "2016 Others"

combined_travel_df = combined_travel_df.rename(columns={"ALONE_x":"2016 Alone",
                                                        "WITH SPOUSE_x":"2016 With Spouse",
                                                        "WITH CHILDREN_x":"2016 With Children",
                                                        "WITH FAMILY/RELATIVES_x":"2016 With Family/Relatives",
                                                        "STUDENT GROUP_x":"2016 Student Group",
                                                        "WITH FRIENDS_x":"2016 With Friends",
                                                        "WITH BUSINESS ACCOCIATE_x":"2016 With Business Associate",
                                                        "WITH INCENTIVE GROUP_x":"2016 With Incentive Group",
                                                        "OTHERS_x":"2016 Others"})

# Rename our _y columns to "2016 Alone", "2016 With Spouse", "2016 With Children", "2016 With Family/Relatives",
# "2016 Student Group", "2016 With Friends", "2016 With Business Associate", "2016 With Incentive Group",
# and "2016 Others"
combined_travel_df = combined_travel_df.rename(columns={"ALONE_y":"2017 Alone",
                                                        "WITH SPOUSE_y":"2017 With Spouse",
                                                        "WITH CHILDREN_y":"2017 With Children",
                                                        "WITH FAMILY/RELATIVES_y":"2017 With Family/Relatives",
                                                        "STUDENT GROUP_y":"2017 Student Group",
                                                        "WITH FRIENDS_y":"2017 With Friends",
                                                        "WITH BUSINESS ACCOCIATE_y":"2017 With Business Associate",
                                                        "WITH INCENTIVE GROUP_y":"2017 With Incentive Group",
                                                        "OTHERS_y":"2017 Others"})

combined_travel_df.head()

# Merge our newly combined dataframe with the 2018 dataframe
combined_travel_df = pd.merge(combined_travel_df, travel_2018_df, how="outer", on="COUNTRY OF NATIONALITY")
combined_travel_df

# Rename "ALONE", "WITH SPOUSE", "WITH CHILDREN", "WITH FAMILY/RELATIVES", "STUDENT GROUP", "WITH FRIENDS",
# "WITH BUSINESS ACCOCIATE","WITH INCENTIVE GROUP", "OTHERS" to 
# "2018 Alone", "2018 With Spouse", "2018 With Children", "2018 With Family/Relatives", "2018 Student Group", 
# "2018 With Friends", "2018 With Business Associate", "2018 With Incentive Group", and "2018 Others"
combined_travel_df = combined_travel_df.rename(columns={"ALONE":"2018 Alone",
                                                        "WITH SPOUSE":"2018 With Spouse",
                                                        "WITH CHILDREN":"2018 With Children",
                                                        "WITH FAMILY/RELATIVES":"2018 With Family/Relatives",
                                                        "STUDENT GROUP":"2018 Student Group",
                                                        "WITH FRIENDS":"2018 With Friends",
                                                        "WITH BUSINESS ACCOCIATE":"2018 With Business Associate",
                                                        "WITH INCENTIVE GROUP":"2018 With Incentive Group",
                                                        "OTHERS":"2018 Others"})

combined_travel_df.head()

# Check the mean of the columns


# Reduce columns where mean of traveling companions is > 1 across all years




import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Take in all of our traveler data and read it into pandas
travel_2016 = "../Resources/2016_travelers.csv"
travel_2017 = "../Resources/2017_travelers.csv"
travel_2018 = "../Resources/2018_travelers.csv"

travel_2016_df = pd.read_csv(travel_2016)
travel_2017_df = pd.read_csv(travel_2017)
travel_2018_df = pd.read_csv(travel_2018)


# Merge the first two datasets on "COUNTRY OF NATIONALITY" so that no data is lost (should be 44 rows)
combined_travel_df = pd.merge(travel_2016_df, travel_2017_df,
                                 how='outer', on='COUNTRY OF NATIONALITY')
combined_travel_df.head()

# Rename our _x columns to "2016 Alone", "2016 With Spouse", "2016 With Children", "2016 With Family/Relatives",
# "2016 Student Group", "2016 With Friends", "2016 With Business Associate", "2016 With Incentive Group",
# and "2016 Others"

combined_travel_df = combined_travel_df.rename(columns={"ALONE_x":"2016 Alone",
                                                        "WITH SPOUSE_x":"2016 With Spouse",
                                                        "WITH CHILDREN_x":"2016 With Children",
                                                        "WITH FAMILY/RELATIVES_x":"2016 With Family/Relatives",
                                                        "STUDENT GROUP_x":"2016 Student Group",
                                                        "WITH FRIENDS_x":"2016 With Friends",
                                                        "WITH BUSINESS ACCOCIATE_x":"2016 With Business Associate",
                                                        "WITH INCENTIVE GROUP_x":"2016 With Incentive Group",
                                                        "OTHERS_x":"2016 Others"})

# Rename our _y columns to "2016 Alone", "2016 With Spouse", "2016 With Children", "2016 With Family/Relatives",
# "2016 Student Group", "2016 With Friends", "2016 With Business Associate", "2016 With Incentive Group",
# and "2016 Others"
combined_travel_df = combined_travel_df.rename(columns={"ALONE_y":"2017 Alone",
                                                        "WITH SPOUSE_y":"2017 With Spouse",
                                                        "WITH CHILDREN_y":"2017 With Children",
                                                        "WITH FAMILY/RELATIVES_y":"2017 With Family/Relatives",
                                                        "STUDENT GROUP_y":"2017 Student Group",
                                                        "WITH FRIENDS_y":"2017 With Friends",
                                                        "WITH BUSINESS ACCOCIATE_y":"2017 With Business Associate",
                                                        "WITH INCENTIVE GROUP_y":"2017 With Incentive Group",
                                                        "OTHERS_y":"2017 Others"})

combined_travel_df.head()

# Merge our newly combined dataframe with the 2018 dataframe
combined_travel_df = pd.merge(combined_travel_df, travel_2018_df, how="outer", on="COUNTRY OF NATIONALITY")
combined_travel_df

# Rename "ALONE", "WITH SPOUSE", "WITH CHILDREN", "WITH FAMILY/RELATIVES", "STUDENT GROUP", "WITH FRIENDS",
# "WITH BUSINESS ACCOCIATE","WITH INCENTIVE GROUP", "OTHERS" to 
# "2018 Alone", "2018 With Spouse", "2018 With Children", "2018 With Family/Relatives", "2018 Student Group", 
# "2018 With Friends", "2018 With Business Associate", "2018 With Incentive Group", and "2018 Others"
combined_travel_df = combined_travel_df.rename(columns={"ALONE":"2018 Alone",
                                                        "WITH SPOUSE":"2018 With Spouse",
                                                        "WITH CHILDREN":"2018 With Children",
                                                        "WITH FAMILY/RELATIVES":"2018 With Family/Relatives",
                                                        "STUDENT GROUP":"2018 Student Group",
                                                        "WITH FRIENDS":"2018 With Friends",
                                                        "WITH BUSINESS ACCOCIATE":"2018 With Business Associate",
                                                        "WITH INCENTIVE GROUP":"2018 With Incentive Group",
                                                        "OTHERS":"2018 Others"})

combined_travel_df.head()

# Check the mean of the columns
combined_travel_df.mean()

# Reduce columns where mean of traveling companions is > 1 across all years
travel_reduced = pd.DataFrame(combined_travel_df[["COUNTRY OF NATIONALITY",
                                                  "2016 Alone","2016 With Spouse","2016 With Children",
                                                  "2016 With Family/Relatives","2016 With Friends",
                                                  "2016 With Business Associate","2017 Alone",
                                                  "2017 With Spouse","2017 With Children",
                                                  "2017 With Family/Relatives","2017 With Friends",
                                                  "2017 With Business Associate","2018 Alone",
                                                  "2018 With Spouse","2018 With Children",
                                                  "2018 With Family/Relatives","2018 With Friends",
                                                  "2018 With Business Associate"]])

# Set index to "Country of Nationality"
travel_reduced = travel_reduced.set_index("COUNTRY OF NATIONALITY")
travel_reduced

# Create a variable for each country to chart
country1 = "USA"
country2 = "THAILAND"
country3 = "PHILIPPINES"

# Set type of traveling companion
columns_to_compare = "With Spouse"

# Create a Series for each chosen country that looks for the chosen travel companion from 2016 to 2018
country1_traveler_over_time = travel_reduced.loc[country1,
                                                [f"2016 {columns_to_compare}",
                                                 f"2017 {columns_to_compare}", 
                                                 f"2018 {columns_to_compare}"]]

country2_traveler_over_time = travel_reduced.loc[country2,
                                                [f"2016 {columns_to_compare}",
                                                 f"2017 {columns_to_compare}", 
                                                 f"2018 {columns_to_compare}"]]

country3_traveler_over_time = travel_reduced.loc[country3,
                                                [f"2016 {columns_to_compare}",
                                                 f"2017 {columns_to_compare}", 
                                                 f"2018 {columns_to_compare}"]]

# Create a list of the years that we will use as our x axis
years = [2016,2017,2018]

# Plot our line that will be used to track the first country's traveling companion percentage over the years
plt.plot(years, country1_traveler_over_time, color="green", label=country1)

# Plot our line that will be used to track the second country's traveling companion percentage over the years
plt.plot(years, country2_traveler_over_time, color="blue", label=country2)

# Plot our line that will be used to track the third country's traveling companion percentage over the years
plt.plot(years, country3_traveler_over_time, color="orange", label=country3)

# Place a legend on the chart in what matplotlib believes to be the "best" location
plt.legend(loc="best")

plt.title("Traveling " + columns_to_compare + " Country Comparison")
plt.xlabel("Years")
plt.xticks(np.arange(min(years), max(years)+1, 1.0))
plt.ylabel("% Travelers")

# Print our chart to the screen
plt.show()



import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Take in all of our traveler data and read it into pandas
travel_2016 = "../Resources/2016_travelers.csv"
travel_2017 = "../Resources/2017_travelers.csv"
travel_2018 = "../Resources/2018_travelers.csv"

travel_2016_df = pd.read_csv(travel_2016)
travel_2017_df = pd.read_csv(travel_2017)
travel_2018_df = pd.read_csv(travel_2018)


# Merge the first two datasets on "COUNTRY OF NATIONALITY" so that no data is lost (should be 44 rows)
combined_travel_df = pd.merge(travel_2016_df, travel_2017_df,
                                 how='outer', on='COUNTRY OF NATIONALITY')
combined_travel_df.head()

# Rename our _x columns to "2016 Alone", "2016 With Spouse", "2016 With Children", "2016 With Family/Relatives",
# "2016 Student Group", "2016 With Friends", "2016 With Business Associate", "2016 With Incentive Group",
# and "2016 Others"

combined_travel_df = combined_travel_df.rename(columns={"ALONE_x":"2016 Alone",
                                                        "WITH SPOUSE_x":"2016 With Spouse",
                                                        "WITH CHILDREN_x":"2016 With Children",
                                                        "WITH FAMILY/RELATIVES_x":"2016 With Family/Relatives",
                                                        "STUDENT GROUP_x":"2016 Student Group",
                                                        "WITH FRIENDS_x":"2016 With Friends",
                                                        "WITH BUSINESS ACCOCIATE_x":"2016 With Business Associate",
                                                        "WITH INCENTIVE GROUP_x":"2016 With Incentive Group",
                                                        "OTHERS_x":"2016 Others"})

# Rename our _y columns to "2016 Alone", "2016 With Spouse", "2016 With Children", "2016 With Family/Relatives",
# "2016 Student Group", "2016 With Friends", "2016 With Business Associate", "2016 With Incentive Group",
# and "2016 Others"
combined_travel_df = combined_travel_df.rename(columns={"ALONE_y":"2017 Alone",
                                                        "WITH SPOUSE_y":"2017 With Spouse",
                                                        "WITH CHILDREN_y":"2017 With Children",
                                                        "WITH FAMILY/RELATIVES_y":"2017 With Family/Relatives",
                                                        "STUDENT GROUP_y":"2017 Student Group",
                                                        "WITH FRIENDS_y":"2017 With Friends",
                                                        "WITH BUSINESS ACCOCIATE_y":"2017 With Business Associate",
                                                        "WITH INCENTIVE GROUP_y":"2017 With Incentive Group",
                                                        "OTHERS_y":"2017 Others"})

combined_travel_df.head()

# Merge our newly combined dataframe with the 2018 dataframe
combined_travel_df = pd.merge(combined_travel_df, travel_2018_df, how="outer", on="COUNTRY OF NATIONALITY")
combined_travel_df

# Rename "ALONE", "WITH SPOUSE", "WITH CHILDREN", "WITH FAMILY/RELATIVES", "STUDENT GROUP", "WITH FRIENDS",
# "WITH BUSINESS ACCOCIATE","WITH INCENTIVE GROUP", "OTHERS" to 
# "2018 Alone", "2018 With Spouse", "2018 With Children", "2018 With Family/Relatives", "2018 Student Group", 
# "2018 With Friends", "2018 With Business Associate", "2018 With Incentive Group", and "2018 Others"
combined_travel_df = combined_travel_df.rename(columns={"ALONE":"2018 Alone",
                                                        "WITH SPOUSE":"2018 With Spouse",
                                                        "WITH CHILDREN":"2018 With Children",
                                                        "WITH FAMILY/RELATIVES":"2018 With Family/Relatives",
                                                        "STUDENT GROUP":"2018 Student Group",
                                                        "WITH FRIENDS":"2018 With Friends",
                                                        "WITH BUSINESS ACCOCIATE":"2018 With Business Associate",
                                                        "WITH INCENTIVE GROUP":"2018 With Incentive Group",
                                                        "OTHERS":"2018 Others"})

combined_travel_df.head()

# Check the mean of the columns
combined_travel_df.mean()

# Reduce columns where mean of traveling companions is > 1 across all years
travel_reduced = pd.DataFrame(combined_travel_df[["COUNTRY OF NATIONALITY",
                                                  "2016 Alone","2016 With Spouse","2016 With Children",
                                                  "2016 With Family/Relatives","2016 With Friends",
                                                  "2016 With Business Associate","2017 Alone",
                                                  "2017 With Spouse","2017 With Children",
                                                  "2017 With Family/Relatives","2017 With Friends",
                                                  "2017 With Business Associate","2018 Alone",
                                                  "2018 With Spouse","2018 With Children",
                                                  "2018 With Family/Relatives","2018 With Friends",
                                                  "2018 With Business Associate"]])

# Set index to "Country of Nationality"
travel_reduced = travel_reduced.set_index("COUNTRY OF NATIONALITY")
travel_reduced

# Create a variable for each country to chart


# Set type of traveling companion


# Create a Series for each chosen country that looks for the chosen travel companion from 2016 to 2018


# Create a list of the years that we will use as our x axis

# Plot our line that will be used to track the first country's traveling companion percentage over the years

# Plot our line that will be used to track the second country's traveling companion percentage over the years

# Plot our line that will be used to track the third country's traveling companion percentage over the years

# Place a legend on the chart in what matplotlib believes to be the "best" location

# Print our chart to the screen




# Dependencies
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as st
import numpy as np

# Read in the LAX temperature data
temperature_df = pd.read_csv('../Resources/lax_temperature.csv')
temperatures = temperature_df['HourlyDryBulbTemperature']

# Demonstrate calculating measures of central tendency
mean_numpy = np.mean(temperatures)
print(f"The mean temperature at the LAX airport is {mean_numpy}")

median_numpy = np.median(temperatures)
print(f"The median temperature at the LAX airport is {median_numpy}")

mode_scipy = st.mode(temperatures,keepdims=False)
print(f"The mode temperature at the LAX airport is {mode_scipy}")

# Characterize the data set using matplotlib
plt.hist(temperatures)
plt.xlabel('Temperature (Â°F)')
plt.ylabel('Counts')
plt.show()

# Demonstrate calculating the variance and standard deviation using the different modules
var_numpy = np.var(temperatures,ddof = 0)
print(f"The population variance using the NumPy module is {var_numpy}")

sd_numpy = np.std(temperatures,ddof = 0)
print(f"The population standard deviation using the NumPy module is {sd_numpy}")

# Calculate the 68-95-99.7 rule using the standard deviation
print(f"Roughly 68% of the data is between {round(mean_numpy-sd_numpy,3)} and {round(mean_numpy+sd_numpy,3)}")
print(f"Roughly 95% of the data is between {round(mean_numpy-2*sd_numpy,3)} and {round(mean_numpy+2*sd_numpy,3)}")
print(f"Roughly 99.7% of the data is between {round(mean_numpy-3*sd_numpy,3)} and {round(mean_numpy+3*sd_numpy,3)}")

# Demonstrate calculating the z-scores using SciPy
z_scipy = st.zscore(temperatures)
print(f"The z-scores using the SciPy module are {z_scipy}")

# Dependencies
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as st
import numpy as np

# Read in the LAX temperature data
temperature_df = pd.read_csv('../Resources/lax_temperature.csv')
temperatures = temperature_df['HourlyDryBulbTemperature']

# Demonstrate calculating measures of central tendency


# Characterize the data set using matplotlib


# Demonstrate calculating the variance and standard deviation using the different modules


# Calculate the 68-95-99.7 rule using the standard deviation


# Demonstrate calculating the z-scores using SciPy


# Dependencies
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Example outlier plot of reaction times
times = [96,98,100,105,85,88,95,100,101,102,97,98,5]
fig1, ax1 = plt.subplots()
ax1.set_title('Reaction Times at Baseball Batting Cage')
ax1.set_ylabel('Reaction Time (ms)')
ax1.boxplot(times)
plt.show()

# We need to sort the data to determine which could be outliers
times.sort()
print(times)



# Dependencies
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Example outlier plot of reaction times


# We need to sort the data to determine which could be outliers




# Dependencies
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Read the LAX temperature dataset into a dataframe
temperature_df = pd.read_csv('../Resources/lax_temperature.csv')

# Filter the dataset to only include the `HourlyDryBulbTemperature` column
temperatures = temperature_df['HourlyDryBulbTemperature']

# Create a boxplot to visualize the filtered dataset
fig1, ax1 = plt.subplots()
ax1.set_title('Temperatures at LAX')
ax1.set_ylabel('Temperature (Â°F)')
ax1.boxplot(temperatures)
plt.show()

# Use Pandas to calculate the quartiles, IQR, and median, and print the results
quartiles = temperatures.quantile([.25,.5,.75])
lowerq = quartiles[0.25]
upperq = quartiles[0.75]
iqr = upperq-lowerq

print(f"The lower quartile of temperatures is: {lowerq}")
print(f"The upper quartile of temperatures is: {upperq}")
print(f"The interquartile range of temperatures is: {iqr}")
print(f"The the median of temperatures is: {quartiles[0.5]} ")

# Use the IQR to determine an upper and lower bound for outliers, then print the results
lower_bound = lowerq - (1.5*iqr)
upper_bound = upperq + (1.5*iqr)
print(f"Values below {lower_bound} could be outliers.")
print(f"Values above {upper_bound} could be outliers.")

# Dependencies
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Read the LAX temperature dataset into a dataframe


# Filter the dataset to only include the `HourlyDryBulbTemperature` column


# Create a boxplot to visualize the filtered dataset


# Use Pandas to calculate the quartiles, IQR, and median, and print the results
quartiles = temperatures.quantile([.25,.5,.75])
lowerq = quartiles[0.25]
upperq = quartiles[0.75]
iqr = upperq-lowerq


# Use the IQR to determine an upper and lower bound for outliers, then print the results
# Use the IQR to determine an upper and lower bound for outliers, then print the results
lower_bound = lowerq - (1.5*iqr)
upper_bound = upperq + (1.5*iqr)
print(f"Values below {lower_bound} could be outliers.")
print(f"Values above {upper_bound} could be outliers.")

# Dependencies
import pandas as pd
import random
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import sem

# Set the seed so our data is reproducible
random.seed(42)

# Sample versus population example fuel economy
fuel_economy = pd.read_csv('../Resources/2019_fuel_economy.csv')

# First overview the data set - how many factors, etc.
print(fuel_economy.head())

# Calculate the summary statistics and plot the histogram of the entire population data
print(f"The mean MPG of all vehicles is: {round(fuel_economy['Combined_MPG'].mean(),2)}")
print(f"The standard deviation of all vehicle's MPG is: {round(fuel_economy['Combined_MPG'].std(),2)}")
plt.hist(fuel_economy['Combined_MPG'])
plt.xlabel("Fuel Economy (MPG)")
plt.ylabel("Number of Vehicles")
plt.show()

# Calculate the summary statistics and plot the histogram of the sample data using iloc
subset = fuel_economy.iloc[range(766,856)]
print(f"The mean MPG of all vehicles is: {round(subset['Combined_MPG'].mean(),2)}")
print(f"The standard deviation of all vehicle's MPG is: {round(subset['Combined_MPG'].std(),2)}")
plt.hist(subset['Combined_MPG'])
plt.xlabel("Fuel Economy (MPG)")
plt.ylabel("Number of Vehicles")
plt.show()

# Calculate the summary statistics and plot the histogram of the sample data using random sampling
subset = fuel_economy.sample(90)
print(f"The mean MPG of all vehicles is: {round(subset['Combined_MPG'].mean(),2)}")
print(f"The standard deviation of all vehicle's MPG is: {round(subset['Combined_MPG'].std(),2)}")
plt.hist(subset['Combined_MPG'])
plt.xlabel("Fuel Economy (MPG)")
plt.ylabel("Number of Vehicles")
plt.show()

# Generate a new 30 vehicle sample and calculate the SEM of the sample
sample = fuel_economy.sample(30)
print(f"The SEM value for the sample fuel economy data is {sem(sample['Combined_MPG'])}")

# Create a sample set of 10, each with 30 vehicles
vehicle_sample_set = [fuel_economy.sample(30) for x in range(0,10)]

# Generate the plot data for each sample 
means = [sample['Combined_MPG'].mean() for sample in vehicle_sample_set]
standard_errors = [sem(sample['Combined_MPG']) for sample in vehicle_sample_set]
x_axis = np.arange(0, len(vehicle_sample_set), 1) + 1

# Setting up the plot
fig, ax = plt.subplots()
ax.errorbar(x_axis, means, standard_errors, fmt="o")
ax.set_xlim(0, len(vehicle_sample_set) + 1)
ax.set_ylim(20,28)
ax.set_xlabel("Sample Number")
ax.set_ylabel("Mean MPG")
plt.show()

print(f"The smallest SEM observed was {min(standard_errors)}")
samp_index = standard_errors.index(min(standard_errors))
print(f"The sample with the smallest SEM is sample {samp_index+1}")

# Dependencies
import pandas as pd
import random
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import sem

# Set the seed so our data is reproducible
random.seed(42)

# Sample versus population example fuel economy
fuel_economy = pd.read_csv('../Resources/2019_fuel_economy.csv')

# First overview the data set - how many factors, etc.
print(fuel_economy.head())

# Calculate the summary statistics and plot the histogram of the entire population data

# Calculate the summary statistics and plot the histogram of the sample data using iloc


# Calculate the summary statistics and plot the histogram of the sample data using random sampling


# Generate a new 30 vehicle sample and calculate the SEM of the sample


# Create a sample set of 10, each with 30 vehicles


# Generate the plot data for each sample 


# Setting up the plot


# Import dependencies
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
from sklearn.datasets import fetch_california_housing
from scipy.stats import sem

# Import the California housing data set and get description
california_dataset = fetch_california_housing()

print(california_dataset.DESCR)

# Read California housing data into a Pandas dataframe
housing_data = pd.DataFrame(data=california_dataset.data,columns=california_dataset.feature_names)
housing_data['MEDV'] = california_dataset.target
housing_data.head()

# Create 25 samples, each with sample size of 20
num_samples = 25
sample_size = 20
samples = [housing_data.sample(sample_size) for x in range(0,num_samples)]

# Calculate means
means = [s['MEDV'].mean() for s in samples]
# Calculate standard error on means
sems = [sem(s['MEDV']) for s in samples]

# Plot sample means with error bars
fig, ax = plt.subplots()
ax.errorbar(np.arange(0, 25, 1)+1,means, yerr=sems, fmt="o", color="b",
            alpha=0.5, label="Mean of House Prices")
ax.set_xlim(0, 25+1)
ax.set_xlabel("Sample Number")
ax.set_ylabel("Mean of Median House Prices ($100,000)")
plt.legend(loc="best", fontsize="small", fancybox=True)
plt.show()

# Calculate the range of SEM values
print(f"The range of SEM values in the sample set is {max(sems)-min(sems)}")

# Determine which sample's mean is closest to the population mean
print(f"The smallest SEM observed was {min(sems)}")
samp_index = sems.index(min(sems))
print(f"The sample with the smallest SEM is sample {samp_index+1}")

sems

# Compare to the population mean
print(f"The mean of the sample 10 is {samples[samp_index]['MEDV'].mean()}")
print(f"The mean of the population data set is {housing_data['MEDV'].mean()}")



# Import dependencies
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
from sklearn.datasets import fetch_california_housing
from scipy.stats import sem

# Import the California housing data set and get description
california_dataset = fetch_california_housing()

print(california_dataset.DESCR)

# Read California housing data into a Pandas dataframe
housing_data = pd.DataFrame(data=california_dataset.data,columns=california_dataset.feature_names)
housing_data['MEDV'] = california_dataset.target
housing_data.head()

# Create 25 samples, each with sample size of 20
num_samples = 25
sample_size = 20


# Calculate means

# Calculate standard error on means


# Plot sample means with error bars


# Calculate the range of SEM values


# Determine which sample's mean is closest to the population mean


# Compare to the population mean




# Dependencies
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as st

# Import the WDI dataset, drop missing data
wdi_data = pd.read_csv('../Resources/WDI_2018.csv')
wdi_data = wdi_data.dropna()
wdi_data.head()

# For the first example, determine which pairs of factors are correlated. 
plt.scatter(wdi_data.iloc[:,1],wdi_data.iloc[:,8])
plt.xlabel('Income Per Capita')
plt.ylabel('Average Alcohol Consumed Per Person Per Year (L)')
plt.show()

plt.scatter(wdi_data.iloc[:,3],wdi_data.iloc[:,10])
plt.xlabel('Population Median Age')
plt.ylabel('Cell Phones Per 100 People')
plt.show()

plt.scatter(wdi_data.iloc[:,9],wdi_data.iloc[:,7])
plt.xlabel('% Population with Access to Clean Water')
plt.ylabel('Male Life Expectancy')
plt.show()

plt.scatter(wdi_data.iloc[:,1],wdi_data.iloc[:,12])
plt.xlabel('Income Per Capita')
plt.ylabel('% Measles Immunization')
plt.show()

# The next example will compute the Pearson correlation coefficient between "Income per Capita" and "Average Alcohol Consumed"
income = wdi_data.iloc[:,1]
alcohol = wdi_data.iloc[:,8]
correlation = st.pearsonr(income,alcohol)
print(f"The correlation between both factors is {round(correlation[0],2)}")

# Compare the calcualted Pearson's r to the plots
plt.scatter(income,alcohol)
plt.xlabel('Income Per Capita')
plt.ylabel('Average Alcohol Consumed Per Person Per Year (L)')
print(f"The correlation between both factors is {round(correlation[0],2)}")
plt.show()

age = wdi_data.iloc[:,3]
cell_phones = wdi_data.iloc[:,10]
correlation = st.pearsonr(age,cell_phones)
plt.scatter(age,cell_phones)
plt.xlabel('Population Median Age')
plt.ylabel('Cell Phones Per 100 People')
print(f"The correlation between both factors is {round(correlation[0],2)}")
plt.show()

water = wdi_data.iloc[:,9]
life = wdi_data.iloc[:,7]
correlation = st.pearsonr(water,life)
plt.scatter(water,life)
plt.xlabel('% Population with Access to Clean Water')
plt.ylabel('Male Life Expectancy')
print(f"The correlation between both factors is {round(correlation[0],2)}")
plt.show()

income = wdi_data.iloc[:,1]
measles = wdi_data.iloc[:,12]
correlation = st.pearsonr(income,measles)
plt.scatter(income,measles)
plt.xlabel('Income Per Capita')
plt.ylabel('% Measles Immunization')
print(f"The correlation between both factors is {round(correlation[0],2)}")
plt.show()

correlation



# Dependencies
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as st

# Import the WDI dataset, drop missing data
wdi_data = pd.read_csv('../Resources/WDI_2018.csv')
wdi_data = wdi_data.dropna()
wdi_data.head()

# For the first example, determine which pairs of factors are correlated. 








# The next example will compute the Pearson correlation coefficient between "Income per Capita" and "Average Alcohol Consumed"


# Compare the calcualted Pearson's r to the plots










# Dependencies
import pandas as pd
import sklearn.datasets as dta
import scipy.stats as st
import matplotlib.pyplot as plt

# Read in the wine recognition data set from sklearn and load into Pandas
data = dta.load_wine()
wine_data = pd.DataFrame(data.data,columns=data.feature_names)
print(data.DESCR)

# Plot flavanoids versus malic_acid on a scatterplot
flavanoids = wine_data['flavanoids']
malic_acid = wine_data['malic_acid']
plt.scatter(malic_acid,flavanoids)
plt.xlabel("Amount of Malic Acid")
plt.ylabel("Amount of Flavanoids")
plt.show()

# Calculate the correlation coefficient between malic_acid and flavanoids
print(f"The correlation coefficient between malic acid and flavanoids is {round(st.pearsonr(malic_acid,flavanoids)[0],2)}")

# Plot colour_intensity versus alcohol on a scatterplot
color_intensity = wine_data['color_intensity']
alcohol = wine_data['alcohol']
plt.scatter(alcohol,color_intensity)
plt.xlabel("Amount of Alcohol")
plt.ylabel("Intensity of Color")
plt.show()

# Calculate the correlation coefficient between alcohol and color_intensity
print(f"The correlation coefficient between alcohol and color intensity is {round(st.pearsonr(alcohol,color_intensity)[0],2)}")

# BONUS: Generate the correlation matrix and find the strongest positive and negative correlations
wine_corr = wine_data.corr()
wine_corr.unstack().sort_values()

wine_corr

import seaborn as sns



matrix = wine_data.corr().round(2)
sns.heatmap(matrix, annot=True)
plt.show()


# Dependencies
import pandas as pd
import sklearn.datasets as dta
import scipy.stats as st
import matplotlib.pyplot as plt

# Read in the wine recognition data set from sklearn and load into Pandas
data = dta.load_wine()
wine_data = pd.DataFrame(data.data,columns=data.feature_names)
print(data.DESCR)

# Plot flavanoids versus malic_acid on a scatterplot


# Calculate the correlation coefficient between malic_acid and flavanoids


# Plot colour_intensity versus alcohol on a scatterplot


# Calculate the correlation coefficient between alcohol and color_intensity


# BONUS: Generate the correlation matrix and find the strongest positive and negative correlations




# Import dependencies
from matplotlib import pyplot as plt
from scipy.stats import linregress
import numpy as np
from sklearn import datasets
import pandas as pd

# Read in the California housing dataset
california_dataset = datasets.fetch_california_housing()
housing_data = pd.DataFrame(data=california_dataset.data,columns=california_dataset.feature_names)
housing_data['MEDV'] = california_dataset.target

# Reduce the dataset to remove AveRooms outliers
housing_data_reduced = pd.DataFrame(housing_data.loc[housing_data['AveRooms']<10,:])

# Reduce the dataset to the San Diego Area (based on approx latitude & longitude area)
san_diego_housing = pd.DataFrame(housing_data_reduced.loc[((housing_data_reduced['Latitude']>32.664282) & 
                                                          (housing_data_reduced['Latitude']<32.980514) &
                                                          (housing_data_reduced['Longitude']>-117.300418) &
                                                          (housing_data_reduced['Longitude']<-117.01950)),:])

# Plot out rooms versus median house price
x_values = san_diego_housing['AveRooms']
y_values = san_diego_housing['MEDV']
plt.scatter(x_values,y_values)
plt.xlabel('Rooms in House')
plt.ylabel('Median House Prices ($100,000)')
plt.show()

# Add the linear regression equation and line to plot
x_values = san_diego_housing['AveRooms']
y_values = san_diego_housing['MEDV']
(slope, intercept, rvalue, pvalue, stderr) = linregress(x_values, y_values)
regress_values = x_values * slope + intercept
line_eq = "y = " + str(round(slope,2)) + "x + " + str(round(intercept,2))
plt.scatter(x_values,y_values)
plt.plot(x_values,regress_values,"r-")
plt.annotate(line_eq,(5.8,0.8),fontsize=15,color="red")
plt.xlabel('Rooms in House')
plt.ylabel('Median House Prices ($100,000)')
plt.show()

# Print out the r-squared value along with the plot.
x_values = san_diego_housing['AveRooms']
y_values = san_diego_housing['MEDV']
(slope, intercept, rvalue, pvalue, stderr) = linregress(x_values, y_values)
regress_values = x_values * slope + intercept
line_eq = "y = " + str(round(slope,2)) + "x + " + str(round(intercept,2))
plt.scatter(x_values,y_values)
plt.plot(x_values,regress_values,"r-")
plt.annotate(line_eq,(5.8,0.8),fontsize=15,color="red")
plt.xlabel('Rooms in House')
plt.ylabel('Median House Prices ($100,000)')
print(f"The r-squared is: {rvalue**2}")
plt.show()



# Import dependencies
from matplotlib import pyplot as plt
from scipy.stats import linregress
import numpy as np
from sklearn import datasets
import pandas as pd

# Read in the California housing dataset
california_dataset = datasets.fetch_california_housing()
housing_data = pd.DataFrame(data=california_dataset.data,columns=california_dataset.feature_names)
housing_data['MEDV'] = california_dataset.target

# Reduce the dataset to remove AveRooms outliers
housing_data_reduced = pd.DataFrame(housing_data.loc[housing_data['AveRooms']<10,:])

# Reduce the dataset to the San Diego Area (based on approx latitude & longitude area)
san_diego_housing = pd.DataFrame(housing_data_reduced.loc[((housing_data_reduced['Latitude']>32.664282) & 
                                                          (housing_data_reduced['Latitude']<32.980514) &
                                                          (housing_data_reduced['Longitude']>-117.300418) &
                                                          (housing_data_reduced['Longitude']<-117.01950)),:])

# Plot out rooms versus median house price


# Add the linear regression equation and line to plot


# Print out the r-squared value along with the plot.


# Dependencies
from matplotlib import pyplot as plt
from scipy import stats
import numpy as np
import pandas as pd

# Load vehicle data set into pandas
vehicle_data = pd.read_csv("../Resources/singapore-motor-vehicle-population.csv")
vehicle_data.head()

# Generate a scatter plot of year versus number of petrol-electric cars
year = vehicle_data.loc[(vehicle_data["type"]=="Cars") & (vehicle_data["engine"]=="Petrol-Electric"),"year"]
petrol_electric_cars = vehicle_data.loc[(vehicle_data["type"]=="Cars") & (vehicle_data["engine"]=="Petrol-Electric"),"number"]
plt.scatter(year,petrol_electric_cars)
plt.xticks(year, rotation=90)
plt.xlabel('Year')
plt.ylabel('Petrol Electric Cars')
plt.show()

# Perform a linear regression on year versus petrol-electric cars
pe_slope, pe_int, pe_r, pe_p, pe_std_err = stats.linregress(year, petrol_electric_cars)

# Create equation of line to calculate predicted number of petrol-electric cars
pe_fit = pe_slope * year + pe_int

# Plot the linear model on top of scatter plot 
year = vehicle_data.loc[(vehicle_data["type"]=="Cars") & (vehicle_data["engine"]=="Petrol-Electric"),"year"]
petrol_electric_cars = vehicle_data.loc[(vehicle_data["type"]=="Cars") & (vehicle_data["engine"]=="Petrol-Electric"),"number"]
plt.scatter(year,petrol_electric_cars)
plt.plot(year,pe_fit,"--")
plt.xticks(year, rotation=90)
plt.xlabel('Year')
plt.ylabel('Petrol Electric Cars')
plt.show()

# Repeat plotting scatter and linear model for year versus petrol cars
petrol_cars = vehicle_data.loc[(vehicle_data["type"]=="Cars") & (vehicle_data["engine"]=="Petrol"), "number"]
p_slope, p_int, p_r, p_p, p_std_err = stats.linregress(year, petrol_cars)
p_fit = p_slope * year + p_int
plt.scatter(year,petrol_cars)
plt.plot(year,p_fit,"--")
plt.xticks(year, rotation=90)
plt.xlabel('Year')
plt.ylabel('Petrol Cars')
plt.show()

# Repeat plotting scatter and linear model for year versus electric cars
diesel_cars = vehicle_data.loc[(vehicle_data["type"]=="Cars") & (vehicle_data["engine"]=="Diesel"), "number"]
d_slope, d_int, d_r, d_p, d_std_err = stats.linregress(
    year, diesel_cars)
d_fit = d_slope * year + d_int
plt.scatter(year,diesel_cars)
plt.plot(year,d_fit,"--")
plt.xticks(year, rotation=90)
plt.xlabel('Year')
plt.ylabel('Diesel Cars')
plt.show()

# Generate a facet plot of all 3 figures
fig, (ax1, ax2, ax3) = plt.subplots(3, sharex=True)
fig.suptitle("Number of Vehicles Over Time", fontsize=16, fontweight="bold")

ax1.set_xlim(min(year), max(year))
ax1.plot(year, petrol_electric_cars, linewidth=1, marker="o")
ax1.plot(year, pe_fit, "b--", linewidth=1)
ax1.set_ylabel("Petrol-Electric Cars")

ax2.plot(year, petrol_cars, linewidth=1, marker="o", color="y")
ax2.plot(year, p_fit, "y--", linewidth=1)
ax2.set_ylabel("Petrol Cars")

ax3.plot(year, diesel_cars, linewidth=1, marker="o", color="g")
ax3.plot(year, d_fit, "g--", linewidth=1)
ax3.set_ylabel("Diesel Cars")
ax3.set_xlabel("Year")

plt.show()

# Calculate the number of cars for 2024
year = 2024
print(f"The number of petrol-electic cars in 2024 will be {round(pe_slope * year + pe_int,0)}.")
print(f"The number of petrol cars in 2024 will be {round(p_slope * year + p_int,0)}.")
print(f"The number of diesel cars in 2024 will be {round(d_slope * year + d_int,0)}.")



# Dependencies
from matplotlib import pyplot as plt
from scipy import stats
import numpy as np
import pandas as pd

# Load vehicle data set into pandas
vehicle_data = pd.read_csv("../Resources/singapore-motor-vehicle-population.csv")
vehicle_data.head()

# Generate a scatter plot of year versus number of petrol-electric cars
year=vehicle_data.loc[(vehicle_data["type"]=="Cars")&(vehicle_data['engine']=="Petrol-Electric"),"year"]
petrol_electric_cars=vehicle_data.loc[(vehicle_data["type"]=="Cars")&(vehicle_data['engine']=="Petrol-Electric"),"number"]
plt.scatter(year,petrol_electric_cars)
plt.xticks(year, rotation=90)
plt.xlabel('Years')
plt.ylabel('Petrol Electric Cars')
plt.show()

# Perform a linear regression on year versus petrol-electric cars
(slope, intercept, rvalue, pvalue, stderr) = stats.linregress(year, petrol_electric_cars)

# Create equation of line to calculate predicted number of petrol-electric cars
fit=slope*year+intercept
fit

# Plot the linear model on top of scatter plot 
# Generate a scatter plot of year versus number of petrol-electric cars
year=vehicle_data.loc[(vehicle_data["type"]=="Cars")&(vehicle_data['engine']=="Petrol-Electric"),"year"]
petrol_electric_cars=vehicle_data.loc[(vehicle_data["type"]=="Cars")&(vehicle_data['engine']=="Petrol-Electric"),"number"]
plt.scatter(year,petrol_electric_cars)
plt.plot(year,fit,"--")
plt.xticks(year, rotation=90)
plt.xlabel('Years')
plt.ylabel('Petrol Electric Cars')
plt.show()

# Repeat plotting scatter and linear model for year versus petrol cars
# year=vehicle_data.loc[(vehicle_data["type"]=="Cars")&(vehicle_data['engine']=="Petrol-Electric"),"year"]
petrol_cars=vehicle_data.loc[(vehicle_data["type"]=="Cars")&(vehicle_data['engine']=="Petrol"),"number"]
(slope, intercept, rvalue, pvalue, stderr) = stats.linregress(year, petrol_cars)
plt.scatter(year,petrol_cars)
fit=slope*year+intercept
plt.plot(year,fit,"--")
plt.xticks(year, rotation=90)
plt.xlabel('Years')
plt.ylabel('Petrol Cars')
plt.show()

# Repeat plotting scatter and linear model for year versus electric cars
electric_cars=vehicle_data.loc[(vehicle_data["type"]=="Cars")&(vehicle_data['engine']=="Electric"),"number"]
(slope, intercept, rvalue, pvalue, stderr) = stats.linregress(year, petrol_cars)
plt.scatter(year,petrol_cars)
fit=slope*year+intercept
plt.plot(year,fit,"--")
plt.xticks(year, rotation=90)
plt.xlabel('Years')
plt.ylabel('Electric Cars')
plt.show()

# Generate a facet plot of all 3 figures
fig, (ax1,ax2,ax3)=plt.subplots(3,sharex=True)

ax1.plot(year,petrol_electric_cars,linewidth=1,marker="o")
ax1.plot(year,fit,"b--",linewidth=1)
ax1.set_ylabel("petrol-electric")

ax2.plot(year,petrol_cars,linewidth=1,color="y")
ax2.plot(year,fit,"y--",linewidth=1)
ax2.set_ylabel("petrol")

ax3.plot(year,electric_cars,linewidth=1,color="g")
ax3.plot(year,fit,"g--",linewidth=1)
ax3.set_ylabel("electric")
ax3.set_xlabel("year")
plt.show()

# Calculate the number of cars for 2024
year=2024
print(slope*year+intercept)



# Initial imports
import pandas as pd

# Read the S&P 500 CSV data into a DataFrame
df_sp500 = pd.read_csv("../Resources/sp500.csv")

# Display the DataFrame
df_sp500.head()

# Verify the data types using dtypes
df_sp500.dtypes

# Reviewing the time value from index position 0
df_sp500["time"][0]

# Getting the current date and time
pd.to_datetime("today")

# Transform the time column to a datetime data type
df_sp500["time"] = pd.to_datetime(
    df_sp500["time"],
    infer_datetime_format =True,
    utc = True
)

# Verify the data type transformation using the info function
df_sp500.info()

# Convert the time column to the US/Eastern timezone
df_sp500["time"] = df_sp500["time"].dt.tz_convert("US/Eastern")

# Verify the data type transformation using the info function
df_sp500.info()

# Review the DataFrame with the new timezone information
df_sp500.head()

# Initial imports
import pandas as pd

# Read the S&P 500 CSV data into a DataFrame


# Display the DataFrame


# Verify the data types using dtypes


# Reviewing the time value from index position 0


# Getting the current date and time


# Transform the time column to a datetime data type


# Verify the data type transformation using the info function


# Convert the time column to the US/Eastern timezone


# Verify the data type transformation using the info function


# Review the DataFrame with the new timezone information


# Import the required libraries and dependencies.
import pandas as pd

# Read the data from the tsla_historical.csv file into a Pandas DataFrame
df_tsla = pd.read_csv("../Resources/tsla_historical.csv")

# Display the first five rows of the DataFrame
df_tsla.head()

# Inspect the DataFrame's data types using the info function
df_tsla.info()

df_tsla["time"] = pd.to_datetime(
    df_tsla["time"],
    utc = True    
)

df_tsla["time"] = pd.to_datetime(
    df_tsla["time"], 
    format='%Y-%m-%d %H:%M:%S%z', 
    utc=True)
# https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior

# Transform the time column to a datetime data type
df_tsla["time"] = pd.to_datetime(
    df_tsla["time"],
    infer_datetime_format = True,
    utc = True    
)

# Display the first five rows of the DataFrame to confirm
# changes to the time column
df_tsla.head()

# Use the info function to confirm the change in data type 
# for the time column
df_tsla.info()

# Convert the time column to the Europe/Berlin timezone
df_tsla["time"] = df_tsla["time"].dt.tz_convert("Europe/Berlin")

# View the first five rows of the DataFrame to confirm the
# conversion of the time column
df_tsla.head()

# Use the info function to confirm the change in the time zone
# associated with the time column
df_tsla.info()



# Import the required libraries and dependencies.
import pandas as pd
from path import Path

# Read the data from the tsla_historical.csv file into a Pandas DataFrame
df_tsla = # YOUR CODE HERE


# Display the first five rows of the DataFrame
# YOUR CODE HERE


# Inspect the DataFrame's data types using the info function
# YOUR CODE HERE


# Transform the time column to a datetime data type
df_tsla["time"] = # YOUR CODE HERE


# Display the first five rows of the DataFrame to confirm
# changes to the time column
# YOUR CODE HERE


# Use the info function to confirm the change in data type 
# for the time column
# YOUR CODE HERE


# Convert the time column to the Europe/Berlin timezone
df_tsla["time"] = # YOUR CODE HERE


# View the first five rows of the DataFrame to confirm the
# conversion of the time column
# YOUR CODE HERE


# Use the info function to confirm the change in the time zone
# associated with the time column
# YOUR CODE HERE




# Initial imports
import pandas as pd

# Read the new S&P 500 CSV data into a DataFrame
df_sp500 = pd.read_csv("../Resources/sp500.csv")

# Display the DataFrame
df_sp500.head()

# Transform time column to datetime
df_sp500["time"] = pd.to_datetime(
    df_sp500["time"],
    infer_datetime_format=True,
    utc=True
)

# Convert the time column to the US/Estern timezone
df_sp500["time"] = df_sp500["time"].dt.tz_convert("US/Eastern")

# Verify data type transformation
df_sp500.info()

# Set the time column as DataFrame index
df_sp500 = df_sp500.set_index("time")

# Display the DataFrame
df_sp500.head()

# Querying individual date and time components
print(df_sp500.index.year)
print(df_sp500.index.month)
print(df_sp500.index.quarter)

# Plot the close column to examine the trend for closing prices
df_sp500['close'].plot(
    title="Historical S&P 500 Closing Prices", 
    figsize=[20, 10]
)

# Pick a single date from the DataFrame
# Display the first 20 rows
df_sp500.loc["2019-06-06"].head(20)

# Picking specific times from a datetime range
df_sp500.loc["2019-06-06 07:00:00":"2019-06-06 9:30:00"]

# Plotting pre-market hours trading (7:00 am - 9:30 am)
df_sp500.loc["2019-06-06 07:00:00":"2019-06-06 9:30:00"]["close"].plot(figsize=[15,10])

# When the stock market opens there is usually an initial push in one direction
df_sp500.loc["2019-06-06 09:30":"2019-06-06 09:45"]["close"].plot(figsize=[15,10])

# Closing prices during lunch time
df_sp500.loc["2019-06-06 11:00":"2019-06-06 12:00"]["close"].plot(figsize=[15,10])

# Closing prices just a little over an hour before market close
df_sp500.loc['2019-06-06 14:00':'2019-06-06 14:45']["close"].plot(figsize=[15,10])

# Closing prices thirty minutes before market closes
df_sp500.loc['2019-06-06 15:30':'2019-06-06 16:00']["close"].plot(figsize=[15,10])

df_sp500.loc['2019-06-06 15:30':'2019-06-06 16:00']["close"]



# Initial imports
import pandas as pd

# Read the new S&P 500 CSV data into a DataFrame
df_sp500 = pd.read_csv("../Resources/sp500.csv")

# Display the DataFrame
df_sp500.head()

# Transform time column to datetime
df_sp500["time"] = pd.to_datetime(
    df_sp500["time"],
    infer_datetime_format=True,
    utc=True
)

# Convert the time column to the US/Estern timezone
df_sp500["time"] = df_sp500["time"].dt.tz_convert("US/Eastern")

# Verify data type transformation
df_sp500.info()

# Set the time column as DataFrame index

# Display the DataFrame


# Querying individual date and time components


# Plot the close column to examine the trend for closing prices


# Pick a single date from the DataFrame
# Display the first 20 rows


# Picking specific times from a datetime range


# Plotting pre-market hours trading (7:00 am - 9:30 am)


# When the stock market opens there is usually an initial push in one direction


# Closing prices during lunch time


# Closing prices just a little over an hour before market close


# Closing prices thirty minutes before market closes






# Import the required libraries and dependencies
import pandas as pd

# Read the data from the tsla_historical.csv file into a Pandas DataFrame
df_tsla = pd.read_csv("../Resources/tsla_historical.csv")

# Display the first five rows of the DataFrame
df_tsla.head()

# Inspect the DataFrame's data types using the info function
df_tsla.info()

# Transform the time column to a datetime data type
df_tsla["time"] = pd.to_datetime(
    df_tsla["time"],
    infer_datetime_format = True,
    utc = True    
)

# Display the first five rows of the DataFrame to confirm
# changes to the time column
df_tsla.head()

# Use the info function to confirm the change in data type 
# for the time column
df_tsla.info()

# Set the time column as DataFrame index
df_tsla = df_tsla.set_index("time")

# Display the first five rows of the DataFrame
df_tsla.head()

# Plot the closing price of TSLA
df_tsla["close"].plot(
    title="Historial TSLA Closing Prices", 
    figsize=[20, 10]
)

# Select and plot the TSLA closing prices from 2020
df_tsla.loc["2020"].plot()

# Select and plot the TSLA closing prices from 2020 using DatetimeIndex attributes
df_tsla.loc[df_tsla.index.year == 2020].plot()

# Select and plot the TSLA closing prices from August and September 2020
df_tsla.loc["2020-08" : "2020-09"].plot()

# Select and plot the TSLA closing prices from August 22 to September 5, 2020
df_tsla.loc["2020-08-22" : "2020-09-05"].plot()



# Import the required libraries and dependencies
import pandas as pd
from path import Path

# Read the data from the tsla_historical.csv file into a Pandas DataFrame
df_tsla = pd.read_csv(
    Path("../Resources/tsla_historical.csv")
)

# Display the first five rows of the DataFrame
df_tsla.head()

# Inspect the DataFrame's data types using the info function
df_tsla.info()

# Transform the time column to a datetime data type
df_tsla["time"] = pd.to_datetime(
    df_tsla["time"],
    infer_datetime_format = True,
    utc = True    
)

# Display the first five rows of the DataFrame to confirm
# changes to the time column
df_tsla.head()

# Use the info function to confirm the change in data type 
# for the time column
df_tsla.info()

# Set the time column as DataFrame index
df_tsla = # YOUR CODE HERE

# Display the first five rows of the DataFrame
# YOUR CODE HERE


# Plot the closing price of TSLA
# YOUR CODE HERE


# Select and plot the TSLA closing prices from 2020
# YOUR CODE HERE


# Select and plot the TSLA closing prices from 2020 using DatetimeIndex attributes
# YOUR CODE HERE


# Select and plot the TSLA closing prices from August and September 2020
# YOUR CODE HERE


# Select and plot the TSLA closing prices from August 22 to September 5, 2020
# YOUR CODE HERE




# Initial imports
import pandas as pd
import numpy as np

# Set the file path
file_path = "../Resources/national-home-sales.csv"

# Load time series data into Pandas
df_home_sales = pd.read_csv(file_path, index_col="period_end_date", parse_dates=True)

# Display sample data
df_home_sales.head(10)

# Sort the DataFrame index in ascending order
df_home_sales = df_home_sales.sort_index()

# Display sample data
df_home_sales.head(10)

# Plot the inventory and homes_sold series
df_home_sales[["inventory", "homes_sold"]].plot()

# Select homes sold in 2014 using the year
sales_2014 = df_home_sales["homes_sold"].loc["2014"]

# Display 2014 home sales data
print(sales_2014)

# Plot 2014 home sales data
sales_2014.plot(title="Homes Sold in 2014")

# Compute the total home sales per quarter
quarterly_sales = df_home_sales["homes_sold"].groupby(by=[df_home_sales.index.quarter]).sum()

# Display total home sales per quarter
quarterly_sales

# Plot total home sales per quarter
quarterly_sales.plot()

# Compute total quarterly home sales per year
quarterly_sales_per_year = df_home_sales["homes_sold"].groupby(by=[df_home_sales.index.year, df_home_sales.index.quarter]).sum()

# Display total quarterly home sales per year
quarterly_sales_per_year

# Plot total quarterly home sales per year
quarterly_sales_per_year.plot()

quarterly_sales_per_year.index

# Initial imports
import pandas as pd
import numpy as np

# Set the file path
file_path = "../Resources/national-home-sales.csv"

# Load time series data into Pandas
df_home_sales = pd.read_csv(file_path, index_col="period_end_date", parse_dates=True)

# Display sample data
df_home_sales.head(10)

# Sort the DataFrame index in ascending order


# Display sample data


# Plot the inventory and homes_sold series


# Select homes sold in 2014 using the year


# Display 2014 home sales data


# Plot 2014 home sales data


# Compute the total home sales per quarter


# Display total home sales per quarter


# Plot total home sales per quarter


# Compute total quarterly home sales per year


# Display total quarterly home sales per year


# Plot total quarterly home sales per year


# Import necessary libraries and dependencies
import numpy as np
import pandas as pd

# Import data
sp500_path = '../Resources/sp500_stock_volume.csv'

# Read the S&P 500 volume into a DataFrame. (Make sure to declare the datetime index).
sp500_data = pd.read_csv(
    sp500_path, 
    index_col='Date',     
    parse_dates=True, 
    infer_datetime_format=True
)

# Display sample data
sp500_data.head()

# Slice the dataframe so that it just includes the volume data.
sp500_volume = sp500_data['volume']

# Declare the group level to be the day of the week (e.g., Mon, Tues,...)
group_level = sp500_volume.index.dayofweek

# Plot average daily volume according to day of the week
sp500_volume.groupby(group_level).mean().plot()

# Declare the group level to be hour of the day
group_level = sp500_volume.index.hour

# Plot average hourly volume
sp500_volume.groupby(group_level).mean().plot()

# Group the data by the calendar week in the year (week of year). 
sp500_volume.groupby(by=[sp500_volume.index.year, sp500_volume.index.isocalendar().week]).mean().plot(
    title="SP500 Mean Volume per Week",
    figsize=[10, 5]
)




# Import necessary libraries and dependencies
import numpy as np
import pandas as pd

# Import data
sp500_path = '../Resources/sp500_stock_volume.csv'

# Read the S&P 500 volume into a DataFrame. (Make sure to declare the datetime index)

# Display sample data


# Slice the dataframe so that it just includes the volume data.


# Declare the group level to be the day of the week (e.g., Mon, Tues,...)


# Plot average daily volume according to day of the week


# Declare the group level to be hour of the day


# Plot average hourly volume


# Group the data by the calendar week in the year (week of year). 




# Import the required libraries and dependencies
import pandas as pd

# Read csv file as DataFrame, with datetime index as the column date
data_path = "../Resources/bitcoin_hourly.csv"
df = pd.read_csv(
    data_path,
    infer_datetime_format=True,
    parse_dates=True,
    index_col='date'
)

# Preview the dataset
df.head()

# Plot volume to get a sense of what's typical volume for the cryptocurrency.
df['volume'].plot()

df.groupby(df.index.isocalendar().week).mean().plot(kind='bar')


# Plot hourly trends in prices and volume for the cryptocurrency
df.groupby(df.index.hour).mean().plot(kind='bar')

# Slice to one day before and after Jan 29, 2021
df.loc['2021-01-27':'2021-01-31'].plot()

# Import the required libraries and dependencies
import pandas as pd

# Read csv file as DataFrame, with datetime index as the column date
data_path = "../Resources/bitcoin_hourly.csv"
df = pd.read_csv(
    data_path,
    infer_datetime_format=True,
    parse_dates=True,
    index_col='date'
)

# Preview the dataset
df.head()

# Plot volume to get a sense of what's typical volume for the cryptocurrency.


# Use groupby and the weekofyear function on the datetime index to create a bar plot of the data


# Plot hourly trends in prices and volume for the cryptocurrency


# Slice to one day before and after Jan 29, 2021


# Initial imports
import numpy as np
import pandas as pd

# Set the file path
file_path = "../Resources/national-home-sales.csv"

# Load time series data into Pandas
df_home_sales = pd.read_csv(
    file_path,
    index_col="period_end_date",
    parse_dates=True
)

# Display sample data
df_home_sales.head(10)

# Sort the DataFrame index in ascending order
df_home_sales = df_home_sales.sort_index()

# Display sample data
df_home_sales.head(10)

# Plot the inventory and homes_sold series
df_home_sales[["inventory", "homes_sold"]].plot()

# Compute the correlation between "inventory" and "homes_sold"
df_home_sales[["inventory", "homes_sold"]].corr()



# Initial imports
import numpy as np
import pandas as pd

# Set the file path
file_path = "../Resources/national-home-sales.csv"

# Load time series data into Pandas
df_home_sales = pd.read_csv(
    file_path,
    index_col="period_end_date",
    parse_dates=True
)

# Display sample data
df_home_sales.head(10)

# Sort the DataFrame index in ascending order
df_home_sales=df_home_sales.sort_index()
# Display sample data
df_home_sales

# Plot the inventory and homes_sold series
df_home_sales[["inventory","homes_sold"]].plot()

# Compute the correlation between "inventory" and "homes_sold"
df_home_sales[["inventory","homes_sold"]].corr()



# Import the required libraries and dependencies
import numpy as np
import pandas as pd

# Read the data from the apple-price.csv file into a Pandas DataFrame
df_stock = pd.read_csv(
    "../Resources/aapl-price.csv", 
    index_col="date", 
    parse_dates=True, 
    infer_datetime_format=True
)

# Review the DataFrame
df_stock.head()

# Read the data from the apple-trends.csv file into a Pandas DataFrame
df_trends = pd.read_csv(
    "../Resources/apple-trends.csv", 
    index_col="date", 
    parse_dates=True, 
    infer_datetime_format=True
)

# Review the DataFrame
df_trends.head()

# Concatenate Apple's stock price and Google trends data
# Use a parameter of axis=1 to ensure the concatenation by columns
# Chain the dropna function to remove any rows of data that do not 
# contain information in both columns
df_apple = pd.concat([df_stock, df_trends], axis=1).dropna()
df_apple2=df_trends.merge(df_stock, on='date',how='inner').dropna()

# Review the df_apple DataFrame
df_apple.head()
# df_apple2.head()

df_apple.dtypes

df_apple2.head()

df_apple2.dtypes

# Use hvplot to visualize the time series data in the df_apple DataFrame
df_apple.plot()

# Using the df_apple DataFrame, use the loc function to select a
# range of data from March 1st, 2019 to January 31st, 2020
df_spotlight = df_apple.loc["2019-03-01":"2020-01-31"]

# Review the df_spotlight DataFrame
df_spotlight

# Visualize the df_spotlight DataFrame
df_spotlight.plot()

# Create a column which lags Google trends by one week
# Use the shift function, and move the data down by one row
df_apple["lagged_trends"] = df_apple["trend-worldwide"].shift(1)

# Create a column that contains the Apple weekly return data
# Use the pct_change function to calculate the weekly return values
df_apple["weekly_returns"] = df_apple["close"].pct_change()

# Create a column of Apple weekly rolling stock volatility 
# Chain the pct_function, the rolling function and a 4 period window, and the std function 
df_apple["weekly_volatility"] = df_apple["close"].pct_change().rolling(window=4).std()

# Display the df_apple DataFrame to confirm all columns are present
df_apple.head()

# Use the corr function to compute the correlation between the lagged Google Trends data, price returns, and stock volatility
df_apple[["lagged_trends", "weekly_returns", "weekly_volatility"]].corr()



# Import the required libraries and dependencies
import numpy as np
import pandas as pd

# Read the data from the apple-price.csv file into a Pandas DataFrame
df_stock = pd.read_csv(
    "../Resources/aapl-price.csv", 
    index_col="date", 
    parse_dates=True, 
    infer_datetime_format=True
)

# Review the DataFrame
df_stock.head()

# Read the data from the apple-trends.csv file into a Pandas DataFrame
df_trends = pd.read_csv(
    "../Resources/apple-trends.csv", 
    index_col="date", 
    parse_dates=True, 
    infer_datetime_format=True
)

# Review the DataFrame
df_trends.head()

# Concatenate Apple's stock price and Google trends data
# Use a parameter of axis=1 to ensure the concatenation by columns
# Chain the dropna function to remove any rows of data that do not 
# contain information in both columns
df_apple=pd.concat([df_stock,df_trends],axis=1).dropna()

# Review the df_apple DataFrame
df_apple.head()


# Use hvplot to visualize the time series data in the df_apple DataFrame
df_apple.plot()

# Using the df_apple DataFrame, use the loc function to select a
# range of data from March 1st, 2019 to January 31st, 2020
df_spotlight=df_apple.loc["2019-03-01":"2020-01-31"]

# Review the df_spotlight DataFrame
df_spotlight

# Visualize the df_spotlight DataFrame
df_spotlight.plot()

# Create a column which lags Google trends by one week
# Use the shift function, and move the data down by one row
df_apple['lagged_trends']=df_apple["trend-worldwide"].shift(1)
df_apple

# Create a column that contains the Apple weekly return data
# Use the pct_change function to calculate the weekly return values
df_apple['weekly_returns']=df_apple['close'].pct_change()
df_apple

# Create a column of Apple weekly rolling stock volatility 
# Chain the pct_function, the rolling function and a 4 period window, and the std function 
df_apple["weekly_volatility"]=df_apple['close'].pct_change().rolling(window=4).std()
df_apple

# Display the df_apple DataFrame to confirm all columns are present


# Use the corr function to compute the correlation between the lagged Google Trends data, price returns, and stock volatility
df_apple[["lagged_trends","weekly_returns","weekly_volatility"]].corr()



# Install the required libraries
from IPython.display import clear_output
try:
  !pip install prophet
except:
  print("Error installing libraries")
finally:
  clear_output()
  print("Libraries successfully installed")

# Import the required libraries and dependencies
import pandas as pd
from prophet import Prophet
import datetime as dt
%matplotlib inline

# Import the `files` library to allow files upload
from google.colab import files

# Install the required libraries

# Import the required libraries and dependencies

# Install the required libraries
!pip install prophet

# Import the required libraries and dependencies
import pandas as pd
from prophet import Prophet
import datetime as dt
%matplotlib inline

# Install the required libraries


# Import the required libraries and dependencies


# Install the required libraries
!pip install prophet

# Import the required libraries and dependencies
import pandas as pd
import datetime as dt
from prophet import Prophet

%matplotlib inline

# Load the data into the DataFrame
hourly_prices = pd.read_csv(
    "https://static.bc-edx.com/ai/ail-v-1-0/m8/lesson_2/datasets/hourly_grid_prices.csv",
    index_col='day-hour',
    parse_dates=True,
    infer_datetime_format=True
).dropna()

# Display the first and last five rows of the DataFrame
display(hourly_prices.head())
display(hourly_prices.tail())

# Plot the DataFrame
hourly_prices.plot()

# Reset the index of the DataFrame
prophet_df = hourly_prices.reset_index()

# Review the first and last five rows of the DataFrame
display(prophet_df.head())
display(prophet_df.tail())

# Prepare the training data to be read into a prophet model
# Rename the columns to names that Prophet recognizes
prophet_df.columns = ['ds', 'y']
prophet_df.head()

# Confirm that there are no NaN values
prophet_df = prophet_df.dropna()
prophet_df.tail()



# Install the required libraries
!pip install prophet

# Import the required libraries and dependencies
import pandas as pd
import datetime as dt
from prophet import Prophet

%matplotlib inline

# Load the data into the DataFrame
hourly_prices = pd.read_csv(
    "https://static.bc-edx.com/ai/ail-v-1-0/m8/lesson_2/datasets/hourly_grid_prices.csv",
    index_col='day-hour',
    parse_dates=True,
    infer_datetime_format=True
).dropna()

# Display the first and last five rows of the DataFrame
display(hourly_prices.head())
display(hourly_prices.tail())

# Plot the DataFrame


# Reset the index of the DataFrame


# Review the first and last five rows of the DataFrame


# Prepare the training data to be read into a prophet model
# Rename the columns to names that Prophet recognizes


# Confirm that there are no NaN values




# Import the required libraries and dependencies
import pandas as pd
import datetime as dt
import matplotlib.pyplot as plt

%matplotlib inline

# Read the data from the scarf-google-trends-data.csv file into a Pandas DataFrame
df_alpaca = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m8/lesson_2/datasets/scarf-google-trends-data.csv")

# Review the DataFrame
df_alpaca.head()

# Plot the general trends
df_alpaca.plot() 

# Create a DataFrame for Canada to include the week and canada columns
df_canada = df_alpaca[["week", "canada"]]

# Rename the columns to the Prophet model syntax 
df_canada = df_canada.rename(columns={"week":"ds", "canada":"y"})

# Review the Canada DataFrame
df_canada.head()

# Create a DataFrame for Uruguay to include the week and uruguay columns
df_uruguay = df_alpaca[["week", "uruguay"]]

# Rename the columns to the Prophet model syntax 
df_uruguay = df_uruguay.rename(columns={"week":"ds", "uruguay":"y"})

# Review the Uruguay DataFrame
df_uruguay.head()



# Import the required libraries and dependencies
import pandas as pd
import datetime as dt
import matplotlib.pyplot as plt

%matplotlib inline

# Read the data from the scarf-google-trends-data.csv file into a Pandas DataFrame
df_alpaca = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m8/lesson_2/datasets/scarf-google-trends-data.csv")

# Review the DataFrame
df_alpaca.head()

# Plot the general trends


# Create a DataFrame for Canada to include the week and canada columns


# Rename the columns to the Prophet model syntax 


# Review the Canada DataFrame


# Create a DataFrame for Uruguay to include the week and uruguay columns


# Rename the columns to the Prophet model syntax 


# Review the Uruguay DataFrame




# Install the required libraries
!pip install prophet

# Import the required libraries and dependencies
import pandas as pd
import datetime as dt
from prophet import Prophet

%matplotlib inline

# Import the `files` library to allow files upload
from google.colab import files

# Upload "hourly_grid_prices.csv" into Colab, then store in a DataFrame
uploaded = files.upload()

hourly_prices = pd.read_csv(
    "hourly_grid_prices.csv",
    index_col='day-hour',
    parse_dates=True,
    infer_datetime_format=True
).dropna()

# Display the first and last five rows of the DataFrame
display(hourly_prices.head())
display(hourly_prices.tail())

# Plot the DataFrame
hourly_prices.plot()

# Reset the index of the DataFrame
prophet_df = hourly_prices.reset_index()

# Review the first and last five rows of the DataFrame
display(prophet_df.head())
display(prophet_df.tail())

# Prepare the training data to be read into a prophet model
# Rename the columns to names that Prophet recognizes
prophet_df.columns = ['ds', 'y']
prophet_df.head()

# Confirm that there are no NaN values
prophet_df = prophet_df.dropna()
prophet_df.tail()

# Call the Prophet function and store as an object
m = Prophet()
m

# Fit the time series Prophet model
m.fit(prophet_df)

# Create a future DataFrame to hold predictions
# Make the prediction go out as far as 720 hours (30 days)
future = m.make_future_dataframe(periods=720, freq='H')

# Review the first and last 10 rows of the DataFrame
display(future.head(10))
display(future.tail(10))

# Make a forecast based on the future DataFrame
forecast = m.predict(future)

# Review the first five rows of the forecast DataFrame
display(forecast.head())
display(forecast.tail())



# Install the required libraries
!pip install prophet

# Import the required libraries and dependencies
import pandas as pd
import datetime as dt
from prophet import Prophet

%matplotlib inline

# Import the `files` library to allow files upload
from google.colab import files

# Upload "hourly_grid_prices.csv" into Colab, 
uploaded = files.upload()

# Read in the CSV file to a DataFrame

# Display the first and last five rows of the DataFrame


# Plot the DataFrame


# Reset the index of the DataFrame


# Review the first and last five rows of the DataFrame


# Prepare the training data to be read into a prophet model
# Rename the columns to names that Prophet recognizes


# Confirm that there are no NaN values


# Call the Prophet function and store as an object


# Fit the time series Prophet model


# Create a future DataFrame to hold predictions
# Make the prediction go out as far as 720 hours (30 days)


# Review the first and last 10 rows of the DataFrame


# Make a forecast based on the future DataFrame


# Review the first five rows of the forecast DataFrame




# Install the required libraries
!pip install prophet

# Import the required libraries and dependencies
import pandas as pd
import datetime as dt
from prophet import Prophet

%matplotlib inline

# Upload the scarf-google-trends-data.csv file
from google.colab import files
uploaded = files.upload()

# Read the data from the scarf-google-trends-data.csv file into a Pandas DataFrame
df_alpaca = pd.read_csv("scarf-google-trends-data.csv")

# Review the DataFrame
df_alpaca.head()

# Plot the general trends
df_alpaca.plot() 

# Create a DataFrame for Canada to include the week and canada columns
df_canada = df_alpaca[["week", "canada"]]

# Rename the columns to the Prophet model syntax 
df_canada = df_canada.rename(columns={"week":"ds", "canada":"y"})

# Review the Canada DataFrame
df_canada.head()

# Create a DataFrame for Uruguay to include the week and uruguay columns
df_uruguay = df_alpaca[["week", "uruguay"]]

# Rename the columns to the Prophet model syntax 
df_uruguay = df_uruguay.rename(columns={"week":"ds", "uruguay":"y"})

# Review the Uruguay DataFrame
df_uruguay.head()

# Create a Prophet model for Canada
model_canada = Prophet()

# Create a Prophet model for Uruguay
model_uruguay = Prophet()

# Fit the Canada Prophet model
model_canada.fit(df_canada)

# Fit the Uruguay Prophet model
model_uruguay.fit(df_uruguay)

# Forecast one year of weekly future trends data for Canada 
future_canada = model_canada.make_future_dataframe(periods=52, freq="W")

# Display the last five rows of the future_canada DataFrame
future_canada.tail()

# Forecast one year of weekly future trends data for Uruguay 
future_uruguay = model_uruguay.make_future_dataframe(periods=52, freq="W")

# Display the last five rows of the future_uruguay DataFrame
future_uruguay.tail()

# Make predictions for Canada using the future_canada DataFrame
forecast_canada = model_canada.predict(future_canada)

# Display the first five rows of the forecast_canada DataFrame
forecast_canada.head()

# Make predictions for Uruguay using the future_uruguay DataFrame
forecast_uruguay = model_uruguay.predict(future_uruguay)

# Display the first five rows of the forecast_uruguay DataFrame
forecast_uruguay.head()



# Install the required libraries
!pip install prophet

# Import the required libraries and dependencies
import pandas as pd
import datetime as dt
from prophet import Prophet

%matplotlib inline

# Upload the scarf-google-trends-data.csv file
from google.colab import files
uploaded = files.upload()

# Read the data from the scarf-google-trends-data.csv file into a Pandas DataFrame
df_alpaca = pd.read_csv("scarf-google-trends-data.csv")

# Review the DataFrame
df_alpaca.head()

# Plot the general trends
df_alpaca.plot() 

# Create a DataFrame for Canada to include the week and canada columns
df_canada = df_alpaca[["week", "canada"]]

# Rename the columns to the Prophet model syntax 
df_canada = df_canada.rename(columns={"week":"ds", "canada":"y"})

# Review the Canada DataFrame
df_canada.head()

# Create a DataFrame for Uruguay to include the week and uruguay columns
df_uruguay = df_alpaca[["week", "uruguay"]]

# Rename the columns to the Prophet model syntax 
df_uruguay = df_uruguay.rename(columns={"week":"ds", "uruguay":"y"})

# Review the Uruguay DataFrame
df_uruguay.head()

# Create a Prophet model for Canada
model_canada = Prophet()

# Create a Prophet model for Uruguay
model_uruguay = Prophet()

# Fit the Canada Prophet model
model_canada.fit(df_canada)

# Fit the Uruguay Prophet model
model_uruguay.fit(df_uruguay)

# Forecast one year of weekly future trends data for Canada 
future_canada = model_canada.make_future_dataframe(periods=52, freq="W")

# Display the last five rows of the future_canada DataFrame
future_canada.tail()

# Forecast one year of weekly future trends data for Uruguay 
future_uruguay = model_uruguay.make_future_dataframe(periods=52, freq="W")

# Display the last five rows of the future_uruguay DataFrame
future_uruguay.tail()

# Make predictions for Canada using the future_canada DataFrame
forecast_canada = model_canada.predict(future_canada)

# Display the first five rows of the forecast_canada DataFrame
forecast_canada.head()

# Make predictions for Uruguay using the future_uruguay DataFrame
forecast_uruguay = model_uruguay.predict(future_uruguay)

# Display the first five rows of the forecast_uruguay DataFrame
forecast_uruguay.head()



# Install the required libraries
!pip install prophet

# Import the required libraries and dependencies
import pandas as pd
import datetime as dt
from prophet import Prophet

%matplotlib inline

# Import the `files` library to allow files upload
from google.colab import files

# Upload "hourly_grid_prices.csv" into Colab, then store in a DataFrame
uploaded = files.upload()

hourly_prices = pd.read_csv(
    "hourly_grid_prices.csv",
    index_col='day-hour',
    parse_dates=True,
    infer_datetime_format=True
).dropna()

# Display the first and last five rows of the DataFrame
display(hourly_prices.head())
display(hourly_prices.tail())

# Plot the DataFrame
hourly_prices.plot()

# Reset the index of the DataFrame
prophet_df = hourly_prices.reset_index()

# Review the first and last five rows of the DataFrame
display(prophet_df.head())
display(prophet_df.tail())

# Prepare the training data to be read into a prophet model
# Rename the columns to names that Prophet recognizes
prophet_df.columns = ['ds', 'y']
prophet_df.head()

# Confirm that there are no NaN values
prophet_df = prophet_df.dropna()
prophet_df.tail()

# Call the Prophet function and store as an object
m = Prophet()
m

# Fit the time series Prophet model
m.fit(prophet_df)

# Create a future DataFrame to hold predictions
# Make the prediction go out as far as 720 hours (30 days)
future = m.make_future_dataframe(periods=720, freq='H')

# Review the first and last 10 rows of the DataFrame
display(future.head(10))
display(future.tail(10))

# Make a forecast based on the future DataFrame
forecast = m.predict(future)

# Review the first five rows of the forecast DataFrame
display(forecast.head())
display(forecast.tail())

# Plot the forecast using the modelâ€™s plot function
m.plot(forecast)

# Display the underlying forecast dataframe (tail)
forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()

# Reset the index to this datetime column so that our plot looks nice
forecast = forecast.set_index('ds')

# Display the DataFrame
forecast.head()

# Plot predictions for our forecast period
forecast[['yhat', 'yhat_lower', 'yhat_upper']].iloc[-720:,:].plot()

# Reset "ds" from the datetime index back to a column
forecast = forecast.reset_index()
forecast.head()

# Plot the individual time series components of the model
fig2 = m.plot_components(forecast)



# Install the required libraries
!pip install prophet

# Import the required libraries and dependencies
import pandas as pd
import datetime as dt
from prophet import Prophet

%matplotlib inline

# Import the `files` library to allow files upload
from google.colab import files

# Upload "hourly_grid_prices.csv" into Colab, then store in a DataFrame
uploaded = files.upload()

hourly_prices = pd.read_csv(
    "hourly_grid_prices.csv",
    index_col='day-hour',
    parse_dates=True,
    infer_datetime_format=True
).dropna()

# Display the first and last five rows of the DataFrame
display(hourly_prices.head())
display(hourly_prices.tail())

# Plot the DataFrame


# Reset the index of the DataFrame

# Review the first and last five rows of the DataFrame


# Prepare the training data to be read into a prophet model
# Rename the columns to names that Prophet recognizes


# Confirm that there are no NaN values


# Call the Prophet function and store as an object


# Fit the time series Prophet model


# Create a future DataFrame to hold predictions
# Make the prediction go out as far as 720 hours (30 days)


# Review the first and last 10 rows of the DataFrame


# Make a forecast based on the future DataFrame


# Review the first five rows of the forecast DataFrame


# Plot the forecast using the modelâ€™s plot function


# Display the underlying forecast dataframe (tail)


# Reset the index to this datetime column so that our plot looks nice


# Display the DataFrame


# Plot predictions for our forecast period


# Reset "ds" from the datetime index back to a column


# Plot the individual time series components of the model




# Install the required libraries
!pip install prophet

# Import the required libraries and dependencies
import pandas as pd
import datetime as dt
from prophet import Prophet

%matplotlib inline

# Upload the scarf-google-trends-data.csv file
from google.colab import files
uploaded = files.upload()

# Read the data from the scarf-google-trends-data.csv file into a Pandas DataFrame
df_alpaca = pd.read_csv("scarf-google-trends-data.csv")

# Review the DataFrame
df_alpaca.head()

# Plot the general trends
df_alpaca.plot() 

# Create a DataFrame for Canada to include the week and canada columns
df_canada = df_alpaca[["week", "canada"]]

# Rename the columns to the Prophet model syntax 
df_canada = df_canada.rename(columns={"week":"ds", "canada":"y"})

# Review the Canada DataFrame
df_canada.head()

# Create a DataFrame for Uruguay to include the week and uruguay columns
df_uruguay = df_alpaca[["week", "uruguay"]]

# Rename the columns to the Prophet model syntax 
df_uruguay = df_uruguay.rename(columns={"week":"ds", "uruguay":"y"})

# Review the Uruguay DataFrame
df_uruguay.head()

# Create a Prophet model for Canada
model_canada = Prophet()

# Create a Prophet model for Uruguay
model_uruguay = Prophet()

# Fit the Canada Prophet model
model_canada.fit(df_canada)

# Fit the Uruguay Prophet model
model_uruguay.fit(df_uruguay)

# Forecast one year of weekly future trends data for Canada 
future_canada = model_canada.make_future_dataframe(periods=52, freq="W")

# Display the last five rows of the future_canada DataFrame
future_canada.tail()

# Forecast one year of weekly future trends data for Uruguay 
future_uruguay = model_uruguay.make_future_dataframe(periods=52, freq="W")

# Display the last five rows of the future_uruguay DataFrame
future_uruguay.tail()

# Make predictions for Canada using the future_canada DataFrame
forecast_canada = model_canada.predict(future_canada)

# Display the first five rows of the forecast_canada DataFrame
forecast_canada.head()

# Make predictions for Uruguay using the future_uruguay DataFrame
forecast_uruguay = model_uruguay.predict(future_uruguay)

# Display the first five rows of the forecast_uruguay DataFrame
forecast_uruguay.head()

# Plot the Prophet predictions for Canada 
model_canada.plot(forecast_canada)

# Plot the Prophet predictions for Uruguay 
model_uruguay.plot(forecast_uruguay)

# Set the index in the forecast_canada DataFrame to the ds datetime column 
forecast_canada = forecast_canada.set_index('ds')

# Display the forecast_canada DataFrame
forecast_canada.head()

# Plot predictions for our forecast_canada DataFrame for the 52 week period 
forecast_canada[['yhat', 'yhat_lower', 'yhat_upper']].iloc[-52:,:].plot()

# Set the index in the forecast_uruguay DataFrame to the ds datetime column 
forecast_uruguay = forecast_uruguay.set_index('ds')

# Display the forecast_uruguay DataFrame
forecast_uruguay.head()

# Plot predictions for our forecast_uruguay DataFrame for the 52 week period 
forecast_uruguay[['yhat', 'yhat_lower', 'yhat_upper']].iloc[-52:,:].plot()

# Reset the index in the forecast_canada DataFrame
forecast_canada = forecast_canada.reset_index()

# Use the plot_components function to visualize the forecast results 
# for the forecast_canada DataFrame
fig_canada = model_canada.plot_components(forecast_canada)

# Reset the index in the forecast_uruguay DataFrame
forecast_uruguay = forecast_uruguay.reset_index()

# Use the plot_components function to visualize the forecast results 
# for the forecast_uruguay DataFrame
fig_uruguay = model_uruguay.plot_components(forecast_uruguay)



# Install the required libraries
!pip install prophet

# Import the required libraries and dependencies
import pandas as pd
import datetime as dt
from prophet import Prophet

%matplotlib inline

# Upload the scarf-google-trends-data.csv file
from google.colab import files
uploaded = files.upload()

# Read the data from the scarf-google-trends-data.csv file into a Pandas DataFrame
df_alpaca = pd.read_csv("scarf-google-trends-data.csv")

# Review the DataFrame
df_alpaca.head()

# Plot the general trends
 

# Create a DataFrame for Canada to include the week and canada columns

# Rename the columns to the Prophet model syntax 

# Review the Canada DataFrame


# Create a DataFrame for Uruguay to include the week and uruguay columns

# Rename the columns to the Prophet model syntax 

# Review the Uruguay DataFrame


# Create a Prophet model for Canada


# Create a Prophet model for Uruguay


# Fit the Canada Prophet model


# Fit the Uruguay Prophet model


# Forecast one year of weekly future trends data for Canada 

# Display the last five rows of the future_canada DataFrame


# Forecast one year of weekly future trends data for Uruguay 


# Display the last five rows of the future_uruguay DataFrame


# Make predictions for Canada using the future_canada DataFrame


# Display the first five rows of the forecast_canada DataFrame


# Make predictions for Uruguay using the future_uruguay DataFrame


# Display the first five rows of the forecast_uruguay DataFrame


# Plot the Prophet predictions for Canada 


# Plot the Prophet predictions for Uruguay 


# Set the index in the forecast_canada DataFrame to the ds datetime column 


# Display the forecast_canada DataFrame


# Plot predictions for our forecast_canada DataFrame for the 52 week period 


# Set the index in the forecast_uruguay DataFrame to the ds datetime column 


# Display the forecast_uruguay DataFrame


# Plot predictions for our forecast_uruguay DataFrame for the 52 week period 


# Reset the index in the forecast_canada DataFrame


# Use the plot_components function to visualize the forecast results 
# for the forecast_canada DataFrame


# Reset the index in the forecast_uruguay DataFrame


# Use the plot_components function to visualize the forecast results 
# for the forecast_uruguay DataFrame




# Install the required libraries
!pip install prophet

# Import the required libraries and dependencies
import pandas as pd
import datetime as dt
from prophet import Prophet

%matplotlib inline

# Upload the bitcoin_hourly.csv file
from google.colab import files
uploaded = files.upload()

# Read csv file as DataFrame, with the first column as row index
df = pd.read_csv("bitcoin_hourly.csv",
                 index_col="date",
                 infer_datetime_format=True,
                 parse_dates=True
                )

# Drop columns that won't be used
df = df.drop("volume", axis=1)

# Preview the dataset
df.head()

# Visually inspect the price data
df["close"].plot()

# Reset the index so that we recode the columns for Prophet
df = df.reset_index()

# Label the columns ds and y so that the syntax is recognized by Prophet
df.columns = ["ds", "y"]

# View dataframe shape, and the first and last five rows of the mercado_prophet_df DataFrame
display(df.shape)
display(df.head())
display(df.tail())

# Sort the DataFrame by `ds` in ascending order
df = df.sort_values(by=["ds"], ascending=True)

# Display sample data from head and tail
display(df.head(10))
display(df.tail(10))

# Plot the data
df.plot()

# Call the Prophet function, store as an object
model = Prophet()
model

# Fit the Prophet model.
model.fit(df)

# Create a future dataframe to hold predictions
# Make the prediction go out as far as 1000 hours (approx 40 days)
future_trends = model.make_future_dataframe(periods=1000, freq="H")

# View the last five rows of the predictions
future_trends.tail()

# Make the predictions for the trend data using the future_trends DataFrame
forecast_trends = model.predict(future_trends)

# Display the first five rows of the forecast DataFrame
forecast_trends.head()

# Plot the Prophet predictions for the Mercado trends data
model.plot(forecast_trends)

# Use the plot_components function to visualize the forecast results 
figures = model.plot_components(forecast_trends)

# At this point, it's useful to set the `datetime` index of the forecast data.
forecast_trends = forecast_trends.set_index(["ds"])
forecast_trends.head()

# From the `forecast_trends` DataFrame, plot to visualize
#  the yhat, yhat_lower, and yhat_upper columns over the last 10 days (24*10 = 240) 
forecast_trends[["yhat", "yhat_lower", "yhat_upper"]].iloc[-240:, :].plot()

# Create a `forecast_march_2021` Dataframe, which contains just forecasts for that month
# The DataFrame should include the columns yhat_upper, yhat_lower, and yhat
forecast_march_2021 = forecast_trends.loc["2021-03-01":"2021-03-31"][["yhat_upper", "yhat_lower", "yhat"]]

# Replace the column names to something less technical sounding
forecast_march_2021 = forecast_march_2021.rename(
    columns={
        "yhat_upper": "Best Case",
        "yhat_lower": "Worst Case", 
        "yhat": "Most Likely Case"
    }
)

# Review the last five rows of the DataFrame
forecast_march_2021.tail()

# Display the average forecasted price for March 2021
forecast_march_2021.mean()



# Install the required libraries
!pip install prophet

# Import the required libraries and dependencies
import pandas as pd
import datetime as dt
from prophet import Prophet

%matplotlib inline

# Upload the bitcoin_hourly.csv file
from google.colab import files
uploaded = files.upload()

# Read csv file as DataFrame, with the first column as row index
df = pd.read_csv("bitcoin_hourly.csv",
                 index_col="date",
                 infer_datetime_format=True,
                 parse_dates=True
                )

# Drop columns that won't be used
df = df.drop("volume", axis=1)

# Preview the dataset
df.head()

# Visually inspect the price data


# Reset the index so that we recode the columns for Prophet


# Label the columns ds and y so that the syntax is recognized by Prophet


# View dataframe shape, and the first and last five rows of the mercado_prophet_df DataFrame


# Sort the DataFrame by `ds` in ascending order


# Display sample data from head and tail


# Plot the data


# Call the Prophet function, store as an object


# Fit the Prophet model.


# Create a future dataframe to hold predictions
# Make the prediction go out as far as 1000 hours (approx 40 days)

# View the last five rows of the predictions


# Make the predictions for the trend data using the future_trends DataFrame


# Display the first five rows of the forecast DataFrame


# Plot the Prophet predictions for the Mercado trends data


# Use the plot_components function to visualize the forecast results 


# At this point, it's useful to set the `datetime` index of the forecast data.


# From the `forecast_trends` DataFrame, plot to visualize
#  the yhat, yhat_lower, and yhat_upper columns over the last 10 days (24*10 = 240) 


# Create a `forecast_march_2021` Dataframe, which contains just forecasts for that month
# The DataFrame should include the columns yhat_upper, yhat_lower, and yhat


# Replace the column names to something less technical sounding


# Review the last five rows of the DataFrame


# Display the average forecasted price for March 2021




# Install the required libraries
!pip install prophet

# Import the required libraries and dependencies
import pandas as pd
import datetime as dt
from prophet import Prophet

%matplotlib inline

# Upload the sp500.csv file
from google.colab import files
uploaded = files.upload()

# Read csv file as DataFrame, with the first column as row index
df = pd.read_csv("sp500.csv",
                 index_col="time",
                 infer_datetime_format=True,
                 parse_dates=True
                )

# Remove the timezone from the index for Prophet compatibility
df.index = df.index.tz_localize(None)

# Preview the dataset
df.head()

# Visually inspect the price data
df["close"].plot()

# Add whatever analysis and predictions you see fit

# YOUR CODE HERE

# Install the required libraries
!pip install prophet

# Import the required libraries and dependencies
import pandas as pd
import datetime as dt
from prophet import Prophet

%matplotlib inline

# Upload the bitcoin_hourly.csv file
from google.colab import files
uploaded = files.upload()

# Read csv file as DataFrame, with the first column as row index
df = pd.read_csv("sp500.csv",
                 index_col="time",
                 infer_datetime_format=True,
                 parse_dates=True
                )

# Remove the timezone from the index for Prophet
df.index = df.index.tz_localize(None)

# Preview the dataset
df.head()

# Visually inspect the price data
df["close"].plot()

# Reset the index so that we recode the columns for Prophet
df = df.reset_index()

# Label the columns ds and y so that the syntax is recognized by Prophet
df.columns = ["ds", "y"]

# View dataframe shape, and the first and last five rows of the mercado_prophet_df DataFrame
display(df.shape)
display(df.head())
display(df.tail())

# Sort the DataFrame by `ds` in ascending order
df = df.sort_values(by=["ds"], ascending=True)

# Display sample data from head and tail
display(df.head(10))
display(df.tail(10))

# Plot the data
df.plot()

# Call the Prophet function, store as an object
model = Prophet()
model

# Fit the Prophet model.
model.fit(df)

# Create a future dataframe to hold predictions
# Make the prediction go out as far as 1000 hours (approx 40 days)
future_trends = model.make_future_dataframe(periods=1000, freq="H")

# View the last five rows of the predictions
future_trends.tail()

# Make the predictions for the trend data using the future_trends DataFrame
forecast_trends = model.predict(future_trends)

# Display the first five rows of the forecast DataFrame
forecast_trends.head()

# Plot the Prophet predictions for the Mercado trends data
model.plot(forecast_trends)

# Use the plot_components function to visualize the forecast results 
figures = model.plot_components(forecast_trends)

# At this point, it's useful to set the `datetime` index of the forecast data.
forecast_trends = forecast_trends.set_index(["ds"])
forecast_trends.head()

# From the `forecast_trends` DataFrame, plot to visualize
#  the yhat, yhat_lower, and yhat_upper columns over the last 10 days (24*10 = 240) 
forecast_trends[["yhat", "yhat_lower", "yhat_upper"]].iloc[-240:, :].plot()

# Create a `forecast_feb_2020` Dataframe, which contains just forecasts for that month
# The DataFrame should include the columns yhat_upper, yhat_lower, and yhat
forecast_feb_2020 = forecast_trends.loc["2020-02-01":"2021-02-28"][["yhat_upper", "yhat_lower", "yhat"]]

# Replace the column names to something less technical sounding
forecast_feb_2020 = forecast_feb_2020.rename(
    columns={
        "yhat_upper": "Best Case",
        "yhat_lower": "Worst Case", 
        "yhat": "Most Likely Case"
    }
)

# Review the last five rows of the DataFrame
forecast_feb_2020.tail()

# Display the average forecasted price for March 2021
forecast_feb_2020.mean()



# Import the modules
from sklearn.datasets import make_blobs
import pandas as pd

# Generate three synthetic clusters
X, y = make_blobs(
    centers=3, 
    n_features=2,
    random_state=1  
)

# Print out the X values
X

# Get the shape of the X values
X.shape

# Print out the y values and get the shape.
print(y)
y.shape

# Transform the y variables into a single column.
y = y.reshape(-1, 1)

# Create a DataFrame with the synthetic data
df = pd.DataFrame(X, columns=["Feature 1", "Feature 2"])

# Add the y variables as the "Target" column.
df["Target"] = y

# Display the data
df

# Visualize the data
df.plot.scatter(x="Feature 1",
                y="Feature 2",
                c="Target",
                colormap="winter")

# Generate five synthetic clusters
X, y = make_blobs(
    centers=5, 
    n_features=2,
    random_state=1
)

# Transform the y variables into a single column.
y = y.reshape(-1, 1)

# Create a DataFrame with the synthetic data
df = pd.DataFrame(X, columns=["Feature 1", "Feature 2"])

# Add the y variables as the "Target" column.
df["Target"] = y

# Visualize the data
df.plot.scatter(x="Feature 1", y="Feature 2", c="Target", colormap="winter")



# Import the modules
from sklearn.datasets import make_blobs
import pandas as pd

# Generate three synthetic clusters
X, y = make_blobs(
    centers=3, 
    n_features=2,
    random_state=1  
)

# Print out the X values
X

# Get the shape of the X values
X.shape

# Print out the y values and get the shape.
print(y)
y.shape

# Transform the y variables into a single column.
y = y.reshape(-1, 1)

# Create a DataFrame with the synthetic data
df = pd.DataFrame(X, columns=["Feature 1", "Feature 2"])

# Add the y variables as the "Target" column.
df["Target"] = y

# Display the data
df

# Visualize the data
df.plot.scatter(x="Feature 1",
                y="Feature 2",
                c="Target",
                colormap="winter")

# Generate five synthetic clusters


# Transform the y variables into a single column.
y = y.reshape(-1, 1)

# Create a DataFrame with the synthetic data


# Add the y variables as the "Target" column.


# Visualize the data




# Import dependencies
import pandas as pd

# Read in the CSV file as a Pandas DataFrame
service_ratings_df = pd.read_csv("Resources/service_ratings.csv")

# Review the DataFrame
service_ratings_df.head()

# Visualize a scatter plot of the data
service_ratings_df.plot.scatter(x="mobile_app_rating",
                                y="personal_banker_rating")

# Start by importing the K-means algorithm
import sklearn
from sklearn.cluster import KMeans

# Create and initialize the K-means model instance for 2 clusters
model = KMeans(n_clusters=2, n_init='auto', random_state=1)

# Print the model
model

# Fit the data to the instance of the model
model.fit(service_ratings_df)

# Make predictions about the data clusters using the trained model
customer_ratings = model.predict(service_ratings_df)

# Print the predictions
print(customer_ratings)

# Create a copy of the DataFrame
service_rating_predictions_df = service_ratings_df.copy()

# Add a column to the DataFrame that contains the customer_ratings information
service_rating_predictions_df['customer rating'] = customer_ratings

# Review the DataFrame
service_rating_predictions_df.head()

# Plot the data points based on the customer rating
service_rating_predictions_df.plot.scatter(
    x="mobile_app_rating", 
    y="personal_banker_rating",
    c="customer rating", 
    colormap='winter')



# Import dependencies
import pandas as pd

# Read in the CSV file as a Pandas DataFrame
service_ratings_df = pd.read_csv("https://static.bc-edx.com/mbc/ai/m2/datasets/service-ratings.csv")

# Review the DataFrame
service_ratings_df.head()

# Visualize a scatter plot of the data


# Start by importing the K-means algorithm
from sklearn.cluster import KMeans

# Create and initialize the K-means model instance for 2 clusters
model = KMeans(n_clusters=2, n_init='auto',random_state=1)

# Print the model
model

# Fit the data to the instance of the model
model.fit(service_ratings_df)

# Make predictions about the data clusters using the trained model


# Print the predictions


# Create a copy of the DataFrame


# Add a column to the DataFrame that contains the customer_ratings information

# Review the DataFrame


# Plot the data points based on the customer rating





# Import the required libraries and dependencies
import pandas as pd
from sklearn.cluster import KMeans

# Read in the CSV file and create the Pandas DataFrame
customers_df = pd.read_csv("Resources/customer-shopping-scaled.csv")

# Review the DataFrame
customers_df.head()

# Check for null values and the data types. 
customers_df.info()

# Build the encodeMethod helper function
# Hotel/Restuarant/Cafe purchases should encode to 1
# Retail purchases should encode to 2
def encodeMethod(purchase):
    """
    This function encodes the method of purchases to 2 for "Retail"
    and 1 for Hotel/Restuarant/Cafe.
    """
    if purchase == "HotelRestCafe":
        return 1
    else:
        return 2

# Edit the "Method" column using the encodeMethod function
customers_df["Method"] = customers_df["Method"].apply(encodeMethod)

# # Review the DataFrame
customers_df.head()

# Initialize the K-Means model; n_clusters=2 and n_init='auto'
model_k2 = KMeans(n_clusters=2, n_init='auto')

# Fit the model
model_k2.fit(customers_df)

# Predict the model segments (clusters)
customer_segments_k2 = model_k2.predict(customers_df)

# View the customer segments
print(customer_segments_k2)

# Initialize the K-Means model; n_clusters=3 and n_init='auto'
model_k3 = KMeans(n_clusters=3, n_init='auto')

# Fit the model
model_k3.fit(customers_df)

# Predict the model segments (clusters)
customer_segments_k3 = model_k3.predict(customers_df)

# View the customer segments
print(customer_segments_k3)

# Create a copy of the original DataFrame and name it as customer_predictions
customer_predictions = customers_df.copy()

# Create a new column in the DataFrame with the predicted clusters with k=2
customer_predictions["Customer Segment (k=2)"] = customer_segments_k2

# Create a new column in the DataFrame with the predicted clusters with k=3
customer_predictions["Customer Segment (k=3)"] = customer_segments_k3

# Review the DataFrame
customer_predictions.head()

# Create a scatter plot with x="Frozen" and y="Grocery" with k=2 segments
customer_predictions.plot.scatter(
    x="Frozen", 
    y="Grocery", 
    c="Customer Segment (k=2)",
    title = "Scatter Plot by Shopping Segment - k=2",
    colormap='winter'
)

# Create a scatter plot with x="Frozen" and y="Grocery" with k=3 segments
customer_predictions.plot.scatter(
    x="Frozen", 
    y="Grocery", 
    c="Customer Segment (k=3)",
    title = "Scatter Plot by Shopping Segment - k=3",
    colormap='winter'
)



# Import the required libraries and dependencies
import pandas as pd
from sklearn.cluster import KMeans

# Read in the CSV file and create the Pandas DataFrame

# Review the DataFrame


# Check for null values and the data types. 


# Build the encodeMethod helper function
# Hotel/Restuarant/Cafe purchases should encode to 1
# Retail purchases should encode to 2
def encodeMethod(purchase):
    """
    This function encodes the method of purchases to 2 for "Retail"
    and 1 for Hotel/Restuarant/Cafe.
    """
    if purchase == "HotelRestCafe":
        return 1
    else:
        return 2

# Edit the "Method" column using the encodeMethod function


# # Review the DataFrame


# Initialize the K-Means model; n_clusters=2 and n_init='auto'


# Fit the model


# Predict the model segments (clusters)


# View the customer segments


# Initialize the K-Means model; n_clusters=3 and n_init='auto'


# Fit the model


# Predict the model segments (clusters)


# View the customer segments


# Create a copy of the original DataFrame and name it as customer_predictions


# Create a new column in the DataFrame with the predicted clusters with k=2


# Create a new column in the DataFrame with the predicted clusters with k=3


# Review the DataFrame


# Create a scatter plot with x="Frozen" and y="Grocery" with k=2 segments


# Create a scatter plot with x="Frozen" and y="Grocery" with k=3 segments




# Initial imports
import pandas as pd
from sklearn.cluster import KMeans

# Load the data into a pandas DataFrame.
customers_shopping_df = pd.read_csv("Resources/customer-shopping-scaled.csv")
customers_shopping_df.head()

# Use the encodeMethod helper function to encode Hotel/Restuarant/Cafe purchases to 1 and Retail purchases to 2.
def encodeMethod(purchase):
    """
    This function encodes the method of purchases to 1 for "HotelRestCafe"
    and 2 for "Retail".
    """
    if purchase == "HotelRestCafe":
        return 1
    else:
        return 2

# Edit the "Method" column using the encodeMethod function
customers_shopping_df["Method"] = customers_shopping_df["Method"].apply(encodeMethod)

# # Review the DataFrame
customers_shopping_df.head()

# Create an empty list to store the inertia values
inertia = []

# Create a list with the number of k-values to try
k = list(range(1, 11))

# Create a for loop to compute the inertia with each possible value of k and add the values to the inertia list.
for i in k:
    model = KMeans(n_clusters=i, n_init='auto', random_state=1)
    model.fit(customers_shopping_df)
    inertia.append(model.inertia_)

# Create a dictionary with the data to plot the elbow curve
elbow_data = {
    "k": k,
    "inertia": inertia
}

# Create a DataFrame with the data to plot the elbow curve
df_elbow = pd.DataFrame(elbow_data)

# Display the DataFrame
df_elbow

# Plot the Elbow curve
df_elbow.plot.line(x="k",
                   y="inertia",
                   title="Elbow Curve",
                   xticks=k)

# Determine the rate of decrease between each k value. 
k = elbow_data["k"]
inertia = elbow_data["inertia"]
for i in range(1, len(k)):
    percentage_decrease = (inertia[i-1] - inertia[i]) / inertia[i-1] * 100
    print(f"Percentage decrease from k={k[i-1]} to k={k[i]}: {percentage_decrease:.2f}%")

# Define the model with 4 clusters
model = KMeans(n_clusters=4, n_init='auto', random_state=1)

# Fit the model
model.fit(customers_shopping_df)

# Make predictions
k_4 = model.predict(customers_shopping_df)

# Create a copy of the DataFrame
customers_predictions_df = customers_shopping_df.copy()

# Add a class column with the labels
customers_predictions_df['customer_segment'] = k_4

# Review the DataFrame
customers_predictions_df

# Create a scatter plot with x="Frozen" and y="Grocery" with k=4 segments
customers_predictions_df.plot.scatter(
    x="Frozen", 
    y="Grocery", 
    c="customer_segment",
    title = "Scatter Plot by Shopping Segment - k=4",
    colormap='viridis'
)



# Initial imports
import pandas as pd
from sklearn.cluster import KMeans

# Load the data into a pandas DataFrame.


# Use the encodeMethod helper function to encode Hotel/Restuarant/Cafe purchases to 1 and Retail purchases to 2.
def encodeMethod(purchase):
    """
    This function encodes the method of purchases to 1 for "HotelRestCafe"
    and 2 for "Retail".
    """
    if purchase == "HotelRestCafe":
        return 1
    else:
        return 2

# Edit the "Method" column using the encodeMethod function
customers_shopping_df["Method"] = customers_shopping_df["Method"].apply(encodeMethod)

# # Review the DataFrame
customers_shopping_df.head()

# Create an empty list to store the inertia values


# Create a list with the number of k-values to try


# Create a for loop to compute the inertia with each possible value of k and add the values to the inertia list.


# Create a dictionary with the data to plot the elbow curve


# Create a DataFrame with the data to plot the elbow curve


# Display the DataFrame


# Plot the Elbow curve


# Determine the rate of decrease between each k value. 


# Define the model with 4 clusters


# Fit the model


# Make predictions


# Create a copy of the DataFrame


# Add a class column with the labels


# Review the DataFrame


# Create a scatter plot with x="Frozen" and y="Grocery" with k=4 segments, use colormap='viridis'.




# Import the modules
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Read in the CSV file as a Pandas DataFrame
spread_df = pd.read_csv("Resources/stock_data.csv",
    index_col="date", 
    parse_dates=True, 
    infer_datetime_format=True
)

# Review the DataFrame
spread_df.head()

# Create a a list to store inertia values
inertia = []

# Create a a list to store the values of k
k = list(range(1, 11))

# Create a for-loop where each value of k is evaluated using the K-means algorithm
# Fit the model using the spread_df DataFrame
# Append the value of the computed inertia from the `inertia_` attribute of the KMeans model instance
for i in k:
    k_model = KMeans(n_clusters=i, n_init='auto', random_state=1)
    k_model.fit(spread_df)
    inertia.append(k_model.inertia_)

# Create a Dictionary that holds the list values for k and inertia
elbow_data = {"k": k, "inertia": inertia}

# Create a DataFrame using the elbow_data Dictionary
df_elbow = pd.DataFrame(elbow_data)

# Review the DataFrame
df_elbow

# Plot the elbow curve using Pandas plot.
df_elbow.plot.line(
    x="k", 
    y= "inertia", 
    title="Elbow Curve", 
    xticks=k
)

# Define the model with the lower value of k clusters
# Use a random_state of 1 to generate the model
model = KMeans(n_clusters=3, n_init='auto', random_state=1)

# Fit the model
model.fit(spread_df)

# Make predictions
k_lower = model.predict(spread_df)

# Create a copy of the DataFrame and name it as spread_df_predictions
spread_df_predictions = spread_df.copy()

# Add a class column with the labels to the spread_df_predictions DataFrame
spread_df_predictions['clusters_lower'] = k_lower

spread_df_predictions

# Visualize the data
spread_df_predictions.plot.scatter(
    x="hi_low_spread",
    y="close",
    c="clusters_lower",
    colormap="winter")

# Define the model with the higher value of k clusters
# Use a random_state of 1 to generate the model
model = KMeans(n_clusters=4, n_init='auto', random_state=1)

# Fit the model
model.fit(spread_df)

# Make predictions
k_higher = model.predict(spread_df)

# Add a class column with the labels to the spread_df_predictions DataFrame
spread_df_predictions['clusters_higher'] = k_higher

# Plot the clusters
spread_df_predictions.plot.scatter(
    x="hi_low_spread",
    y="close",
    c="clusters_lower",
    colormap="winter")



# Import the modules
import pandas as pd
import hvplot.pandas
from pathlib import Path
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Read in the CSV file as a Pandas DataFrame
spread_df = pd.read_csv(
    Path("../Resources/stock_data.csv"),
    index_col="date", 
    parse_dates=True, 
    infer_datetime_format=True
)

# Review the DataFrame
spread_df.head()

# Create a a list to store inertia values


# Create a a list to store the values of k


# Create a for-loop where each value of k is evaluated using the K-means algorithm
# Fit the model using the spread_df DataFrame
# Append the value of the computed inertia from the `inertia_` attribute of the KMeans model instance


# Create a Dictionary that holds the list values for k and inertia


# Create a DataFrame using the elbow_data Dictionary


# Review the DataFrame


# Plot the DataFrame


# Define the model with the lower value of k clusters
# Use a random_state of 1 to generate the model


# Fit the model


# Make predictions


# Create a copy of the DataFrame and name it as spread_df_predictions


# Add a class column with the labels to the spread_df_predictions DataFrame


# Plot the clusters


# Define the model with the higher value of k clusters
# Use a random_state of 1 to generate the model
model = KMeans(n_clusters=4, random_state=1)

# Fit the model
model.fit(spread_df)

# Make predictions
k_higher = model.predict(spread_df)

# Add a class column with the labels to the spread_df_predictions DataFrame
spread_df_predictions['clusters_higher'] = k_higher

# Plot the clusters
spread_df_predictions.hvplot.scatter(
    x="hi_low_spread",
    y="close",
    by="clusters_higher"
).opts(yformatter="%.0f")



# Import the modules
import pandas as pd
from sklearn.cluster import KMeans

# Read in the CSV file as a pandas DataFrame and make the "year" column the index.
used_car_sales_df = pd.read_csv("Resources/used-car-sales-data.csv", index_col="year")

# Review the DataFrame
used_car_sales_df.head()

# Create a a list to store inertia values
inertia = []

# Create a a list to store the values of k
k = list(range(1, 11))

# Create a for-loop where each value of k is evaluated using the K-means algorithm
# Fit the model using the spread_df DataFrame
# Append the value of the computed inertia from the `inertia_` attribute of the K-means model instance
for i in k:
    k_model = KMeans(n_clusters=i, n_init='auto', random_state=2)
    k_model.fit(used_car_sales_df)
    inertia.append(k_model.inertia_)

# Create a dictionary that holds the list values for k and inertia
elbow_data = {"k": k, "inertia": inertia}

# Create a DataFrame using the elbow_data dictionary
df_elbow = pd.DataFrame(elbow_data)

# Review the DataFrame
df_elbow.head()

# Plot the elbow curve
df_elbow.plot.line(x="k",
                   y="inertia",
                   title="Elbow Curve",
                   xticks=k)



# Import the modules
import pandas as pd
from sklearn.cluster import KMeans

# Read in the CSV file as a pandas DataFrame and make the "year" column the index.


# Review the DataFrame


# Create a a list to store inertia values


# Create a a list to store the values of k


# Create a for-loop where each value of k is evaluated using the K-means algorithm
# Fit the model using the spread_df DataFrame
# Append the value of the computed inertia from the `inertia_` attribute of the K-means model instance


# Create a dictionary that holds the list values for k and inertia


# Create a DataFrame using the elbow_datadictionary


# Review the DataFrame


# Plot the elbow curve




# Import the modules
import pandas as pd
from sklearn.cluster import KMeans

# Read in the CSV file as a pandas DataFrame and set the "year" column as the index.
used_car_sales_df = pd.read_csv("Resources/used-car-sales-data.csv", index_col="year")

# Review the DataFrame
used_car_sales_df.head()

# Create a list to store inertia values
inertia = []

# Create a list to store the values of k
k = list(range(1, 11))

# Create a for-loop where each value of k is evaluated using the K-means algorithm
# Fit the model using the DataFrame
# Append the value of the computed inertia from the `inertia_` attribute of the KMeans model instance
for i in k:
    k_model = KMeans(n_clusters=i, n_init='auto', random_state=2)
    k_model.fit(used_car_sales_df)
    inertia.append(k_model.inertia_)

# Create a Dictionary that holds the list values for k and inertia
elbow_data = {"k": k, "inertia": inertia}

# Create a DataFrame using the elbow_data Dictionary
df_elbow = pd.DataFrame(elbow_data)

# Review the DataFrame
df_elbow.head()

# Plot the Elbow curve
df_elbow.plot.line(x="k",
                   y="inertia",
                   title="Elbow Curve",
                   xticks=k)

# Define the model with the lower value of k clusters
# Use a random_state of 1 to generate the model
model = KMeans(n_clusters=3, n_init='auto', random_state=1)

# Fit the model
model.fit(used_car_sales_df)

# Make predictions
car_sales_segment_3 = model.predict(used_car_sales_df)

# Create a copy of the DataFrame and name it as used_car_sales_predictions_df
used_car_sales_predictions_df = used_car_sales_df.copy()

# Add a class column with the labels to the used_car_sales_predictions_df DataFrame
used_car_sales_predictions_df['car_sales_segment_3'] = car_sales_segment_3

# Display the updated DataFrame.
used_car_sales_predictions_df.head(3)

# Plot the clusters with "selling_price" vs. "km_driven".
used_car_sales_predictions_df.plot.scatter(
    x="selling_price",
    y="km_driven",
    c="car_sales_segment_3",
    colormap="winter")

# Define the model with the higher value of k clusters
# Use a random_state of 1 to generate the model
model = KMeans(n_clusters=4, n_init='auto', random_state=1)

# Fit the model
model.fit(used_car_sales_df)

# Make predictions
car_sales_segment_4 = model.predict(used_car_sales_df)

# Add a class column with the labels to the used_car_sales_predictions_df DataFrame
used_car_sales_predictions_df['car_sales_segment_4'] = car_sales_segment_4

# Plot the clusters with "selling_price" vs. "km_driven".
used_car_sales_predictions_df.plot.scatter(
    x="selling_price",
    y="km_driven",
    c="car_sales_segment_4",
    colormap="winter")



# Import the modules
import pandas as pd
from sklearn.cluster import KMeans

# Read in the CSV file as a pandas DataFrame and set the "year" column as the index.
used_car_sales_df = pd.read_csv("Resources/used-car-sales-data.csv", index_col="year")

# Review the DataFrame
used_car_sales_df.head()

# Create a list to store inertia values
inertia = []

# Create a list to store the values of k
k = list(range(1, 11))

# Create a for-loop where each value of k is evaluated using the K-means algorithm
# Fit the model using the DataFrame
# Append the value of the computed inertia from the `inertia_` attribute of the KMeans model instance
for i in k:
    k_model = KMeans(n_clusters=i, n_init='auto', random_state=2)
    k_model.fit(used_car_sales_df)
    inertia.append(k_model.inertia_)

# Create a Dictionary that holds the list values for k and inertia
elbow_data = {"k": k, "inertia": inertia}

# Create a DataFrame using the elbow_data Dictionary
df_elbow = pd.DataFrame(elbow_data)

# Review the DataFrame
df_elbow.head()

# Plot the Elbow curve
df_elbow.plot.line(x="k",
                   y="inertia",
                   title="Elbow Curve",
                   xticks=k)

# Define the model with the lower value of k clusters
# Use a random_state of 1 to generate the model


# Fit the model


# Make predictions


# Create a copy of the DataFrame and name it as used_car_sales_predictions_df


# Add a class column with the labels to the used_car_sales_predictions_df DataFrame


# Display the updated DataFrame.


# Plot the clusters with "selling_price" vs. "km_driven".


# Define the model with the higher value of k clusters
# Use a random_state of 1 to generate the model


# Fit the model


# Make predictions

# Add a class column with the labels to the used_car_sales_predictions_df DataFrame


# Plot the clusters with "selling_price" vs. "km_driven".




# Import the modules
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Read in the CSV file and create the Pandas DataFrame
customers_df = pd.read_csv("Resources/customer-shopping-data.csv")

# Review the DataFrame
customers_df.head()

# Check the DataFrame data types
customers_df.dtypes

# Get the column names.
customers_df.columns

# Scaling the numeric columns: 'Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen' columns
customers_scaled = StandardScaler().fit_transform(customers_df[['Fresh','Milk','Grocery',
                                                                'Frozen','Detergents_Paper','Delicassen']])
# Display the arrays. 
customers_scaled

# Creating a DataFrame with with the scaled data
customers_transformed_df = pd.DataFrame(customers_scaled, columns=['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen'])

# Display sample data
customers_transformed_df.head()

# Transform the "Method" column using get_dummies()
purchase_method = pd.get_dummies(customers_df["Method"])

# Display the transformed data
purchase_method.head()

# Concatenate the df_shopping_transformed and the card_dummies DataFrames
customers_transformed_df = pd.concat([customers_transformed_df, purchase_method], axis=1)

# Display concatenated DataFrame
customers_transformed_df.head()



# Import the modules
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Read in the CSV file and create the Pandas DataFrame


# Review the DataFrame


# Check the DataFrame data types


# Get the column names.


# Scaling the numeric columns: 'Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen' columns

# Display the arrays. 


# Creating a DataFrame with with the scaled data


# Display sample data


# Transform the "Method" column using get_dummies()

# Display the transformed data


# Concatenate the df_shopping_transformed and the card_dummies DataFrames


# Display concatenated DataFrame




# Import the required modules
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Read in the CSV file as a pandas Dataframe
ccinfo_df = pd.read_csv("Resources/cc_info_default.csv")

# Review the first five rows of the DataFrame
display(ccinfo_df.head())
# Review the last five rows of the DataFrame
display(ccinfo_df.tail())

# Review the info of the DataFrame
ccinfo_df.info()

# Verify the categories of the "education" column
ccinfo_df["education"].value_counts()

# Transform the education column using get_dummies
education_encode = pd.get_dummies(ccinfo_df["education"])

# Display the transformed data
education_encode.tail()

# Concatenate the df_shopping_transformed and the card_dummies DataFrames
ccinfo_df = pd.concat([ccinfo_df, education_encode], axis=1)

# Drop the original education column
ccinfo_df = ccinfo_df.drop(columns=["education"])

# Display the DataFrame
ccinfo_df.head()

# Encoding the marriage column using a custom function
def encode_marriage(marriage):
    """
    This function encodes marital status by setting yes as 1 and no as 0.
    """
    if marriage == "yes":
        return 1
    else:
        return 0

# Call the encode_marriage function on the marriage column
ccinfo_df["marriage"] = ccinfo_df["marriage"].apply(encode_marriage)

# Review the DataFrame 
ccinfo_df.head()

# Scaling the numeric columns
ccinfo_data_scaled = StandardScaler().fit_transform(ccinfo_df[["limit_bal", "bill_amt", "pay_amt"]])

# Review the scaled data
ccinfo_data_scaled

# Create a DataFrame of the scaled data
ccinfo_data_scaled = pd.DataFrame(ccinfo_data_scaled, columns=["limit_bal", "bill_amt", "pay_amt"])

# Replace the original data with the columns of information from the scaled Data
ccinfo_df["limit_bal"] = ccinfo_data_scaled["limit_bal"]
ccinfo_df["bill_amt"] = ccinfo_data_scaled["bill_amt"]
ccinfo_df["pay_amt"] = ccinfo_data_scaled["pay_amt"]

# Review the DataFrame
ccinfo_df.head()

# Create a a list to store inertia values and the values of k
inertia = []
k = list(range(1, 11))

# Create a for loop where each value of k is evaluated using the K-means algorithm
# Fit the model using the service_ratings DataFrame
# Append the value of the computed inertia from the `inertia_` attribute of the KMeans model instance
for i in k:
    k_model = KMeans(n_clusters=i, n_init='auto', random_state=0)
    k_model.fit(ccinfo_df)
    inertia.append(k_model.inertia_)
    

# Define a DataFrame to hold the values for k and the corresponding inertia
elbow_data = {"k": k, "inertia": inertia}
df_elbow = pd.DataFrame(elbow_data)

# Review the DataFrame
df_elbow.head()

# Plot the elbow curve
df_elbow.plot.line(x="k",
                   y="inertia",
                   title="Elbow Curve",
                   xticks=k)

# Define the model with 3 clusters
model = KMeans(n_clusters=3, n_init='auto', random_state=3)

# Fit the model
model.fit(ccinfo_df)

# Make predictions
k_3 = model.predict(ccinfo_df)

# Create a copy of the preprocessed data
ccinfo_predictions_df = ccinfo_df.copy()

# Add a class column with the labels
ccinfo_predictions_df['customer_segments'] = k_3

# Plot the clusters
ccinfo_predictions_df.plot.scatter(
    x="limit_bal",
    y="age",
    c="customer_segments",
    colormap="winter")



# Import the required modules
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Read in the CSV file as a pandas Dataframe

# Review the first five rows of the DataFrame
display(ccinfo_df.head())
# Review the last five rows of the DataFrame
display(ccinfo_df.tail())

# Review the info of the DataFrame


# Verify the categories of the "education" column


# Transform the education column using get_dummies


# Display the last five rows of the transformed data


# Concatenate the df_shopping_transformed and the card_dummies DataFrames


# Drop the original education column


# Display the DataFrame


# Encoding the marriage column using a custom function


# Call the encode_marriage function on the marriage column


# Review the DataFrame 


# Scaling the numeric columns


# Review the scaled data


# Create a DataFrame of the scaled data


# Replace the original data with the columns of information from the scaled Data


# Review the DataFrame


# Create a a list to store inertia values and the values of k


# Create a for loop where each value of k is evaluated using the K-means algorithm
# Fit the model using the service_ratings DataFrame
# Append the value of the computed inertia from the `inertia_` attribute of the KMeans model instance


# Define a DataFrame to hold the values for k and the corresponding inertia

# Review the DataFrame


# Plot the elbow curve


# Define the model with 3 clusters


# Fit the model


# Make predictions


# Create a copy of the preprocessed data


# Add a class column with the labels


# Plot the clusters




# Import the required libraries and dependencies
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Read the CSV file into a pandas DataFrame
# Set the index using the Ticker column
df_stocks = pd.read_csv("Resources/stock_data.csv", index_col="Ticker")

# Review the DataFrame
df_stocks.head()

# Get the information on the data types
df_stocks.info()

# Use the StandardScaler module and fit_transform function to 
# scale all columns with numerical values
stock_data_scaled = StandardScaler().fit_transform(df_stocks[["MeanOpen", "MeanHigh",
                                                              "MeanLow", "MeanClose",
                                                              "MeanVolume", "MeanPercentReturn"]])

# Display the first five rows of the scaled data
stock_data_scaled[0:5]

# Create a DataFrame called with the scaled data
# The column names should match those referenced in the StandardScaler step
df_stocks_scaled = pd.DataFrame(
    stock_data_scaled,
    columns=["MeanOpen", "MeanHigh", "MeanLow", "MeanClose", "MeanVolume", "MeanPercentReturn"]
)

# Create a Ticker column in the df_stocks_scaled DataFrame
# using the index of the original df_stocks DataFrame
df_stocks_scaled["Ticker"] = df_stocks.index

# Set the newly created Ticker column as index of the df_stocks_scaled DataFrame
df_stocks_scaled = df_stocks_scaled.set_index("Ticker")

# Review the DataFrame
df_stocks_scaled.head()

# Encode the Sector column
sector_encoded_df = pd.get_dummies(df_stocks["Sector"])

# Review the DataFrame
sector_encoded_df.head()

# Concatenate the Sector encoded data with the scaled data DataFrame
df_stocks_scaled = pd.concat([df_stocks_scaled, sector_encoded_df], axis=1)

# Display the concatenated DataFrame
df_stocks_scaled.head()

# Initialize the K-Means model with n_clusters=3,  n_init='auto', and random_state=1
model = KMeans(n_clusters=3, n_init='auto', random_state=1)

# Fit the model for the df_stocks_scaled DataFrame
model.fit(df_stocks_scaled)

# Predict the model segments (clusters)
stock_clusters = model.predict(df_stocks_scaled)

# View the stock segments
print(stock_clusters)

# Create a copy of the concatenated DataFrame
df_stocks_scaled_predictions = df_stocks_scaled.copy()

# Create a new column in the copy of the concatenated DataFrame with the predicted clusters
df_stocks_scaled_predictions["StockCluster"] = stock_clusters

# Review the DataFrame
df_stocks_scaled_predictions



# Import the required libraries and dependencies
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Read the CSV file into a pandas DataFrame
# Set the index using the Ticker column


# Review the DataFrame


# Get the information on the data types


# Use the StandardScaler module and fit_transform function to 
# scale all columns with numerical values


# Display the first five rows of the scaled data


# Create a DataFrame called with the scaled data
# The column names should match those referenced in the StandardScaler step

# Create a Ticker column in the df_stocks_scaled DataFrame
# using the index of the original df_stocks DataFrame


# Set the newly created Ticker column as index of the df_stocks_scaled DataFrame


# Review the DataFrame


# Encode the Sector column


# Review the DataFrame


# Concatenate the Sector encoded data with the scaled data DataFrame


# Display the concatenated DataFrame


# Initialize the K-Means model with n_clusters=3,  n_init='auto', and random_state=1


# Fit the model for the df_stocks_scaled DataFrame


# Predict the model segments (clusters)


# View the stock segments


# Create a copy of the concatenated DataFrame


# Create a new column in the copy of the concatenated DataFrame with the predicted clusters


# Review the DataFrame




# Import the dependencies
import numpy as np
np.random.seed(0)
import pandas as pd
from sklearn import datasets

# Create a simulated dataset for illustration.
X, y = datasets.make_moons(n_samples=(500), noise=0.05, random_state=1)
X[0:10]

# Import the alternative algorithms for clustering.
from sklearn.cluster import KMeans, AgglomerativeClustering, Birch
# Use the Kmeans algorithm.
k_model = KMeans(n_clusters=3, n_init='auto', random_state=0)
k_model.fit(X)
predictions = k_model.predict(X)

# Use the Birch algorithm.
birch_model = Birch(n_clusters=2)
birch_model.fit(X)
birch_predictions = birch_model.predict(X)

# Use the AgglomerativeClustering algorithm.
agglo_model = AgglomerativeClustering(n_clusters=3)
agglo_predictions = agglo_model.fit_predict(X)

# Make predictions for the Birch algorithm. 
predictions_df = pd.DataFrame(X)
predictions_df['birch-labels'] = birch_predictions
predictions_df

#  Rename the non-string columns 0 and 1, "No" and "Yes" to avoid the Holoviews warning for future versions. 
predictions_df.rename({0: 'feature_0', 1: 'feature_1'}, axis=1, inplace=True)
predictions_df

# Plot the clusters
predictions_df.plot.scatter(
    x="feature_0",
    y="feature_1",
    c="birch-labels",
    colormap="winter")

# Estimate scores for two clusters for the Birch model. 
birch_model_two_clusters = Birch(n_clusters=2)
birch_model_two_clusters.fit(X)
birch_predictions_2 = birch_model_two_clusters.predict(X)

# Estimate scores for 3 clusters for the Birch model. 
birch_model_three_clusters = Birch(n_clusters=3)
birch_model_three_clusters.fit(X)
birch_predictions_3 = birch_model_three_clusters.predict(X)

# Use the Calinski-Harabasz index or variance ratio criterion to define two clusters. 
from sklearn import metrics
labels = birch_model_two_clusters.labels_
score = metrics.calinski_harabasz_score(X, labels)  
score

# Use the Calinski-Harabasz index or variance ratio criterion to define three clusters. 
labels = birch_model_three_clusters.labels_
score = metrics.calinski_harabasz_score(X, labels)  
score



# Import the dependencies
import numpy as np
np.random.seed(0)
import pandas as pd
from sklearn import datasets

# Create a simulated dataset for illustration.
X, y = datasets.make_moons(n_samples=(500), noise=0.05, random_state=1)
X[0:10]

# Import the alternative algorithms for clustering.
from sklearn.cluster import KMeans, AgglomerativeClustering, Birch
# Use the Kmeans algorithm.


# Use the Birch algorithm.


# Use the AgglomerativeClustering algorithm.


# Make predictions for the Birch algorithm. 


#  Rename the non-string columns 0 and 1, "No" and "Yes" to avoid the Holoviews warning for future versions. 


# Plot the clusters


# Estimate scores for two clusters for the Birch model. 


# Estimate scores for 3 clusters for the Birch model. 


# Use the Calinski-Harabasz index or variance ratio criterion to define two clusters. 
from sklearn import metrics


# Use the Calinski-Harabasz index or variance ratio criterion to define three clusters. 




# Import the required modules
import pandas as pd
from sklearn.preprocessing import StandardScaler

# Read in the CSV file as a Pandas Dataframe
ccinfo_df = pd.read_csv("Resources/cc_info_default.csv")

# Review the first five rows of the DataFrame
ccinfo_df.head()

# Check for null values
ccinfo_df.info()

# Verify the categories of the "education" column
ccinfo_df["education"].value_counts()

# Transform the education column using get_dummies
education_dummies = pd.get_dummies(ccinfo_df["education"])

# Display the transformed data
education_dummies.tail()

# Concatenate the df_shopping_transformed and the card_dummies DataFrames
ccinfo_df = pd.concat([ccinfo_df, education_dummies], axis=1)

# Drop the original education column
ccinfo_df = ccinfo_df.drop(columns=["education"])

# Display the DataFrame
ccinfo_df.head()

# Encoding the marriage column using a custom function
def encode_marriage(marriage):
    """
    This function encodes marital status by setting yes as 1 and no as 0.
    """
    if marriage == "yes":
        return 1
    else:
        return 0

# Call the encode_marriage function on the marriage column
ccinfo_df["marriage"] = ccinfo_df["marriage"].apply(encode_marriage)

# Review the DataFrame 
ccinfo_df.head()

# Scaling the numeric columns
ccinfo_data_scaled = StandardScaler().fit_transform(ccinfo_df[["limit_bal", "bill_amt", "pay_amt"]])

# Review the scaled data
ccinfo_data_scaled

# Create a DataFrame of the scaled data
ccinfo_data_scaled = pd.DataFrame(ccinfo_data_scaled, columns=["limit_bal", "bill_amt", "pay_amt"])

# Replace the original data with the columns of information from the scaled Data
ccinfo_df["limit_bal"] = ccinfo_data_scaled["limit_bal"]
ccinfo_df["bill_amt"] = ccinfo_data_scaled["bill_amt"]
ccinfo_df["pay_amt"] = ccinfo_data_scaled["pay_amt"]

# Review the DataFrame
ccinfo_df.head()

# Import the KMeans, Birch, and AgglomerativeClustering modules from SKLearn
from sklearn.cluster import KMeans, AgglomerativeClustering, Birch

# Create a a list to store inertia values and the values of k
inertia = []
k = list(range(1, 11))

# Create a for-loop where each value of k is evaluated using the K-means algorithm
# Fit the model using the service_ratings DataFrame
# Append the value of the computed inertia from the `inertia_` attribute of the KMeans model instance
for i in k:
    k_model = KMeans(n_clusters=i, n_init='auto', random_state=0)
    k_model.fit(ccinfo_df)
    inertia.append(k_model.inertia_)

# Define a DataFrame to hold the values for k and the corresponding inertia
elbow_data = {"k": k, "inertia": inertia}
df_elbow = pd.DataFrame(elbow_data)

# Review the DataFrame
df_elbow

# Plot the Elbow curve
df_elbow.plot.line(x="k",
                   y="inertia",
                   title="Elbow Curve",
                   xticks=k)

# Define the model with 3 clusters
model = KMeans(n_clusters=3, n_init='auto', random_state=3)

# Fit the model
model.fit(ccinfo_df)

# Make predictions
kmeans_predictions = model.predict(ccinfo_df)

# Fit a AgglomerativeClustering Model with three clusters
agglo_model = AgglomerativeClustering(n_clusters=3)

# Make predictions with the AgglomerativeClustering model
agglo_predictions = agglo_model.fit_predict(ccinfo_df)

# Previewing the predicted customer classifications for AgglomerativeClustering
agglo_predictions[-10:]

# Fit a Birch Model with three clusters.
birch_model = Birch(n_clusters=3)
birch_model.fit(ccinfo_df)

# Make predictions with the Birch model
birch_predictions = birch_model.predict(ccinfo_df)

# Previewing the predicted customer classifications for BIRCH
birch_predictions[-10:]

# Create a copy of the preprocessed data
ccinfo_predictions_df = ccinfo_df.copy()
# Add class columns with the labels to the new DataFrame
ccinfo_predictions_df["kmeans-segments"] = kmeans_predictions
ccinfo_predictions_df["agglomerative-segments"] = agglo_predictions
ccinfo_predictions_df["birch-segments"] = birch_predictions
ccinfo_predictions_df[['kmeans-segments','agglomerative-segments', 'birch-segments']].head(3)

# Plot the kmeans clusters using the limit_bal and age columns. 
ccinfo_predictions_df.plot.scatter(
    x="limit_bal",
    y="age",
    c="kmeans-segments",
    colormap="winter")

# Plot the agglomerative clusters using the limit_bal and age columns. 
ccinfo_predictions_df.plot.scatter(
    x="limit_bal",
    y="age",
    c="agglomerative-segments",
    colormap="winter")

# Plot the birch clusters using the limit_bal and age columns. 
ccinfo_predictions_df.plot.scatter(
    x="limit_bal",
    y="age",
    c="birch-segments",
    colormap="winter")

# Create a list to store values and the values of k
score_kmeans = []
score_agglomerative = []
score_birch = []

# Create a list to set the range of k values to test
k = list(range(2, 11))

from sklearn import metrics
# For each model, we iterate through the different cluster count (`i`). 
# Then, calculate the variance ratio for each algorithm, given that specified cluster count.

for i in k:
    # Kmeans variance and score
    k_model = KMeans(n_clusters=i, n_init='auto',random_state=0)
    k_model.fit(ccinfo_df)
    labels = k_model.labels_
    score = metrics.calinski_harabasz_score(ccinfo_df, labels)    
    score_kmeans.append(score)
    
    # AgglomerativeClustering variance and score
    agglo_model = AgglomerativeClustering(n_clusters=i)
    agglo_predictions = agglo_model.fit_predict(ccinfo_df)
    labels = agglo_model.labels_
    score = metrics.calinski_harabasz_score(ccinfo_df, labels)    
    score_agglomerative.append(score)    
    
    # Birch variance and score
    birch_model = Birch(n_clusters=i)
    birch_model.fit(ccinfo_df)
    labels = birch_model.labels_
    score = metrics.calinski_harabasz_score(ccinfo_df, labels)    
    score_birch.append(score)

# Display the scores. 
display(score_kmeans)
display(score_agglomerative)
display(score_birch)



# Import the required modules
import pandas as pd
from sklearn.preprocessing import StandardScaler

# Read in the CSV file as a Pandas Dataframe
ccinfo_df = pd.read_csv("Resources/cc_info_default.csv")

# Review the first five rows of the DataFrame
ccinfo_df.head()

# Check for null values


# Verify the categories of the "education" column


# Transform the education column using get_dummies


# Display the transformed data


# Concatenate the df_shopping_transformed and the card_dummies DataFrames


# Drop the original education column


# Display the DataFrame


# Encoding the marriage column using a custom function
def encode_marriage(marriage):
    """
    This function encodes marital status by setting yes as 1 and no as 0.
    """
    if marriage == "yes":
        return 1
    else:
        return 0

# Call the encode_marriage function on the marriage column


# Review the DataFrame 


# Scaling the numeric columns


# Review the scaled data


# Create a DataFrame of the scaled data


# Replace the original data with the columns of information from the scaled Data


# Review the DataFrame


# Import the KMeans, Birch, and AgglomerativeClustering modules from SKLearn
from sklearn.cluster import KMeans, AgglomerativeClustering, Birch

# Create a a list to store inertia values and the values of k
inertia = []
k = list(range(1, 11))

# Create a for-loop where each value of k is evaluated using the K-means algorithm
# Fit the model using the service_ratings DataFrame
# Append the value of the computed inertia from the `inertia_` attribute of the KMeans model instance


# Define a DataFrame to hold the values for k and the corresponding inertia


# Review the DataFrame
df_elbow

# Plot the Elbow curve


# Define the model with 3 clusters


# Fit the model


# Make predictions


# Fit a AgglomerativeClustering Model with three clusters


# Make predictions with the AgglomerativeClustering model


# Previewing the predicted customer classifications for AgglomerativeClustering


# Fit a Birch Model with three clusters.


# Make predictions with the Birch model


# Previewing the predicted customer classifications for BIRCH


# Create a copy of the preprocessed data

# Add class columns with the labels to the new DataFrame


# Plot the kmeans clusters using the limit_bal and age columns. 


# Plot the agglomerative clusters using the limit_bal and age columns. 


# Plot the birch clusters using the limit_bal and age columns. 


# Create a list to store values and the values of k
score_kmeans = []
score_agglomerative = []
score_birch = []

# Create a list to set the range of k values to test


from sklearn import metrics
# For each model, we iterate throught the different cluster count (`i`). 
# Then, calculate the variance ratio for each algorithm, given that specified cluster count.
for i in k:
    # Kmeans variance and score

    
    # AgglomerativeClustering variance and score

    
    # Birch variance and score


# Display the scores. 




# Import the required libraries and dependencies
import pandas as pd
from sklearn.cluster import KMeans, AgglomerativeClustering, Birch
from sklearn.preprocessing import StandardScaler

# Read the CSV file into a Pandas DataFrame
rate_df = pd.read_csv("Resources/global_carry_trades.csv")

# Review the DataFrame
rate_df.head()

# Use the StandardScaler module and fit_transform function to 
# scale all columns with numerical values
rate_df_scaled = StandardScaler().fit_transform(rate_df[["interest_differential" , "next_month_currency_return"]])

# Diplay the first three rows of the scaled data
rate_df_scaled[0:5]

# Create a DataFrame called with the scaled data
# The column names should match those referenced in the StandardScaler step
rate_df_scaled = pd.DataFrame(
    rate_df_scaled,
    columns=["interest_differential" , "next_month_currency_return"])
rate_df_scaled

# Encode (convert to dummy variables) the "IMF Country Code" column
countries_encoded = pd.get_dummies(rate_df['IMF Country Code'])

# Review the DataFrame
countries_encoded.head()

# Concatenate the scaled data DataFrame with the "IMF Country Code" encoded dummies 
rate_df_scaled = pd.concat([rate_df_scaled, countries_encoded], axis=1)

# Display the combined DataFrame.
rate_df_scaled.head()

# Initialize the K-Means model with n_clusters=3
model = KMeans(n_clusters=3, n_init='auto', random_state=1)

# Fit the model for the rate_df_scaled DataFrame
model.fit(rate_df_scaled)

# Save the predicted model clusters to a new DataFrame.
country_clusters = model.predict(rate_df_scaled)

# View the country clusters
print(country_clusters)

# Create a copy of the concatenated DataFrame
rate_scaled_predictions = rate_df_scaled.copy()

# Create a new column in the copy of the concatenated DataFrame with the predicted clusters
rate_scaled_predictions["CountryCluster"] = country_clusters

# Review the DataFrame
rate_scaled_predictions.head()

# Group the saved DataFrame by cluster using `groupby` to calculate average currency returns
rate_scaled_predictions.groupby(by=['CountryCluster'])['next_month_currency_return'].mean()

# Create a scatter plot of the interest differential and next months currency return.
rate_scaled_predictions.plot.scatter(
    x="interest_differential",
    y="next_month_currency_return",
    c="CountryCluster",
    colormap='winter')

# Initialize a Birch model with n_clusters=5
birch_model = Birch(n_clusters=5)

# Fit the model for the rate_df_scaled DataFrame
birch_model.fit(rate_df_scaled)

# Predict the model segments (clusters)
country_clusters = birch_model.predict(rate_df_scaled)

# View the stock segments
print(country_clusters)

# Create a copy of the concatenated DataFrame
rate_scaled_predictions = rate_df_scaled.copy()

# Create a new column in the copy of the concatenated DataFrame with the predicted clusters
rate_scaled_predictions["CountryCluster"] = country_clusters

# Review the DataFrame
rate_scaled_predictions.head()

# Create a scatter plot of the interest differential and next months currency return.
rate_scaled_predictions.plot.scatter(
    x="interest_differential",
    y="next_month_currency_return",
    c="CountryCluster",
    colormap='winter')



# Import the required libraries and dependencies
import pandas as pd
from sklearn.cluster import KMeans, AgglomerativeClustering, Birch
from sklearn.preprocessing import StandardScaler

# Read the CSV file into a Pandas DataFrame
rate_df = pd.read_csv("Resources/global_carry_trades.csv")

# Review the DataFrame
rate_df.head()

# Use the StandardScaler module and fit_transform function to 
# scale all columns with numerical values


# Diplay the first three rows of the scaled data


# Create a DataFrame called with the scaled data
# The column names should match those referenced in the StandardScaler step


# Encode (convert to dummy variables) the "IMF Country Code" column


# Review the DataFrame


# Concatenate the scaled data DataFrame with the "IMF Country Code" encoded dummies 


# Display the combined DataFrame.


# Initialize the K-Means model with n_clusters=3


# Fit the model for the rate_df_scaled DataFrame


# Save the predicted model clusters to a new DataFrame.


# View the country clusters


# Create a copy of the concatenated DataFrame


# Create a new column in the copy of the concatenated DataFrame with the predicted clusters


# Review the DataFrame


# Group the saved DataFrame by cluster using `groupby` to calculate average currency returns



# Create a scatter plot of the interest differential and next months currency return.


# Initialize a Birch model with n_clusters=5


# Fit the model for the rate_df_scaled DataFrame


# Predict the model segments (clusters)


# View the stock segments


# Create a copy of the concatenated DataFrame


# Create a new column in the copy of the concatenated DataFrame with the predicted clusters


# Review the DataFrame


# Create a scatter plot of the interest differential and next months currency return.




# Required imports
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Read in the CSV file as a Pandas Dataframe
ccinfo_default_df = pd.read_csv("Resources/ccinfo_transformed.csv")
# Show the DataFrame.
ccinfo_default_df.head()

# Plot the clusters using the "limit_bal" and "age" columns
ccinfo_default_df.plot.scatter(
    x="limit_bal",
    y="age",
    c="customer_segments",
    colormap='winter')

# Plot the clusters using the "bill_amt" and "pay_amt" columns
ccinfo_default_df.plot.scatter(
    x="bill_amt",
    y="pay_amt",
    c="customer_segments",
    colormap='winter')

# Drop the customer_segments column so it isn't considered a feature. 
cc_df_clean = ccinfo_default_df.drop(['customer_segments'], axis=1)
cc_df_clean.head()

# Import the PCA module
from sklearn.decomposition import PCA

# Instantiate the PCA instance and declare the number of PCA variables
pca = PCA(n_components=2)

# Fit the PCA model on the transformed credit card DataFrame
ccinfo_pca = pca.fit_transform(cc_df_clean)

# Review the first 5 rows of list data
ccinfo_pca[:5]

# Calculate the PCA explained variance ratio
pca.explained_variance_ratio_

# Create the PCA DataFrame
ccinfo_pca_df = pd.DataFrame(
    ccinfo_pca,
    columns=["PCA1", "PCA2"]
)

# Review the PCA DataFrame
ccinfo_pca_df.head(10)

# Create a a list to store inertia values and the values of k
inertia = []
k = list(range(1, 11))

# Append the value of the computed inertia from the `inertia_` attribute of teh KMeans model instance
for i in k:
    k_model = KMeans(n_clusters=i, n_init='auto', random_state=1)
    k_model.fit(ccinfo_pca_df)
    inertia.append(k_model.inertia_)

# Define a DataFrame to hold the values for k and the corresponding inertia
elbow_data = {"k": k, "inertia": inertia}
df_elbow = pd.DataFrame(elbow_data)

# Review the DataFrame
df_elbow.head()

# Plot the Elbow Curve
df_elbow.plot.line(
    x="k", 
    y="inertia", 
    title="Elbow Curve", 
    xticks=k
)

# Determine the rate of decrease between each k value
k = df_elbow["k"]
inertia = df_elbow["inertia"]
for i in range(1, len(k)):
    percentage_decrease = (inertia[i-1] - inertia[i]) / inertia[i-1] * 100
    print(f"Percentage decrease from k={k[i-1]} to k={k[i]}: {percentage_decrease:.2f}%")

# Define the model with 3 clusters
model = KMeans(n_clusters=3, n_init='auto', random_state=0)

# Fit the model
model.fit(ccinfo_pca_df)

# Make predictions
k_3 = model.predict(ccinfo_pca_df)

# Create a copy of the PCA DataFrame
ccinfo_pca_predictions_df = ccinfo_pca_df.copy()

# Add a class column with the labels
ccinfo_pca_predictions_df["customer_segments"] = k_3

# Plot the clusters
ccinfo_pca_predictions_df.plot.scatter(
    x="PCA1",
    y="PCA2",
    c="customer_segments",
    colormap='winter')

# What columns contribute the most to the explained variance?
# Calculate the PCA explained variance ratio
pca.explained_variance_ratio_

# Determine which feature has the stronger influence on each principal component. 
# Use the columns from the original DataFrame. FYI: The data has already been scaled and fitted.
pca_component_weights = pd.DataFrame(pca.components_.T, columns=['PCA1', 'PCA2'], index=cc_df_clean.columns)
pca_component_weights



# Required imports
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Read in the CSV file as a Pandas Dataframe
ccinfo_default_df = pd.read_csv("Resources/ccinfo_transformed.csv")
# Show the DataFrame.
ccinfo_default_df.head()

# Plot the clusters using the "limit_bal" and "age" columns


# Plot the clusters using the "bill_amt" and "pay_amt" columns


# Drop the customer_segments column so it isn't considered a feature. 


# Import the PCA module


# Instantiate the PCA instance and declare the number of PCA variables


# Fit the PCA model on the transformed credit card DataFrame


# Review the first 5 rows of list data


# Calculate the PCA explained variance ratio


# Create the PCA DataFrame


# Review the PCA DataFrame
c

# Create a a list to store inertia values and the values of k


# Append the value of the computed inertia from the `inertia_` attribute of teh KMeans model instance


# Define a DataFrame to hold the values for k and the corresponding inertia


# Review the DataFrame


# Plot the Elbow Curve


# Determine the rate of decrease between each k value


# Define the model with 3 clusters


# Fit the model


# Make predictions


# Create a copy of the PCA DataFrame


# Add a class column with the labels


# Plot the clusters


# What columns contribute the most to the explained variance?
# Calculate the PCA explained variance ratio


# Determine which feature has the stronger influence on each principal component. 
# Use the columns from the original DataFrame. FYI: The data has already been scaled and fitted.






# Import the modules
import pandas as pd
from sklearn.cluster import KMeans

# Read the csv file into a pandas DataFrame
customers_transformed_df = pd.read_csv("Resources/customers.csv")

# Review the DataFrame
customers_transformed_df.head()

# Import the PCA module
from sklearn.decomposition import PCA

# Instantiate the PCA instance and declare the number of PCA variables
pca=PCA(n_components=2)

# Fit the PCA model on the transformed credit card DataFrame
customers_pca = pca.fit_transform(customers_transformed_df)

# Review the first 5 rows of the array of list data
customers_pca[:5]

# Calculate the PCA explained variance ratio
pca.explained_variance_ratio_

# Create the PCA DataFrame
customers_pca_df = pd.DataFrame(
    customers_pca,
    columns=["PCA1", "PCA2"]
)

# Review the PCA DataFrame
customers_pca_df.head()

# Create a a list to store inertia values and the values of k
inertia = []
k = list(range(1, 11))

# Create a for-loop where each value of k is evaluated using the K-means algorithm
# Fit the model using the service_ratings DataFrame
# Append the value of the computed inertia from the `inertia_` attribute of the KMeans model instance
for i in k:
    k_model = KMeans(n_clusters=i, n_init='auto', random_state=0)
    k_model.fit(customers_pca_df)
    inertia.append(k_model.inertia_)

# Define a DataFrame to hold the values for k and the corresponding inertia
elbow_data = {"k": k, "inertia": inertia}

# Create the DataFrame from the elbow data
df_elbow = pd.DataFrame(elbow_data)

# Review the DataFrame
df_elbow.head()

# Plot the DataFrame
df_elbow.plot.line(
    x="k", 
    y="inertia", 
    title="Elbow Curve", 
    xticks=k
)

# Define the model Kmeans model using the optimal value of k for the number of clusters.
model = KMeans(n_clusters=3, n_init='auto', random_state=0)

# Fit the model
model.fit(customers_pca_df)

# Make predictions
k_3 = model.predict(customers_pca_df)

# Create a copy of the customers_pca_df DataFrame
customer_pca_predictions_df = customers_pca_df.copy()

# Add a class column with the labels
customer_pca_predictions_df["customer_segments"] = k_3

# Plot the clusters
customer_pca_predictions_df.plot.scatter(
    x="PCA1",
    y="PCA2",
    c="customer_segments",
    colormap='winter')

# Define the model Kmeans model using k=3 clusters
model = KMeans(n_clusters=3, n_init='auto', random_state=0)

# Fit the model
model.fit(customers_transformed_df)

# Make predictions
k_3 = model.predict(customers_transformed_df)

# Create a copy of the customers_transformed_df DataFrame
customers_transformed_predictions_df = customers_transformed_df.copy()

# Add a class column with the labels
customers_transformed_predictions_df["customer_segments"] = k_3

# Plot the clusters using the first any two feature columns
customers_transformed_predictions_df.plot.scatter(
    x="feature_1",
    y="feature_2",
    c="customer_segments",
    colormap='winter')

# Determine which feature has the stronger influence on each principal component. 
# Use the columns from the original DataFrame. 
pca_component_weights = pd.DataFrame(pca.components_.T, columns=['PCA1', 'PCA2'], index=customers_transformed_df.columns)
pca_component_weights

# Plot the features that are the most influencial for each component. 
customers_transformed_predictions_df.plot.scatter(
    x="feature_9",
    y="feature_7",
    c="customer_segments",
    colormap='winter')

# Plot the clusters using the most influencial features for each component. 
customers_transformed_predictions_df.plot.scatter(
    x="feature_9",
    y="feature_10",
    c="customer_segments",
    colormap='winter')



# Import the modules
import pandas as pd
from sklearn.cluster import KMeans

# Read the csv file into a pandas DataFrame
customers_transformed_df = pd.read_csv("Resources/customers.csv")

# Review the DataFrame
customers_transformed_df.head()

# Import the PCA module
from sklearn.decomposition import PCA

# Instantiate the PCA instance and declare the number of PCA variables


# Fit the PCA model on the transformed credit card DataFrame


# Review the first 5 rows of the array of list data


# Calculate the PCA explained variance ratio


# Create the PCA DataFrame


# Review the PCA DataFrame


# Create a a list to store inertia values and the values of k


# Create a for-loop where each value of k is evaluated using the K-means algorithm
# Fit the model using the service_ratings DataFrame
# Append the value of the computed inertia from the `inertia_` attribute of the KMeans model instance


# Define a DataFrame to hold the values for k and the corresponding inertia

# Create the DataFrame from the elbow data


# Review the DataFrame


# Plot the DataFrame


# Define the model Kmeans model using the optimal value of k for the number of clusters.


# Fit the model


# Make predictions


# Create a copy of the customers_pca_df DataFrame


# Add a class column with the labels


# Plot the clusters


# Define the model Kmeans model using k=3 clusters

# Fit the model


# Make predictions


# Create a copy of the customers_transformed_df DataFrame


# Add a class column with the labels


# Plot the clusters using the first two feature columns


# Determine which feature has the stronger influence on each principal component. 
# Use the columns from the original DataFrame. 


# Plot the clusters using the most influencial features for each component. 


# Plot the clusters using the most influencial features for each component. 




# Import the required libraries and dependencies
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Read the CSV file into a Pandas DataFrame
# Set the index using the Ticker column
df_stocks = pd.read_csv("Resources/stock_data.csv", index_col="Ticker")

# Review the DataFrame
df_stocks.head()

# Get the shape of the DataFrame
df_stocks.shape

# Get the information on the DataFrame
df_stocks.info()

# Use the standard scaler, fit_transform to scale the numerical columns. 
stock_data_scaled = StandardScaler().fit_transform(df_stocks[["MeanOpen", "MeanHigh", "MeanLow", "MeanClose", "MeanVolume", "MeanPercentReturn"]])

# Create a DataFrame called with the scaled data
# The column names should match those referenced in the StandardScaler step
df_stocks_scaled = pd.DataFrame(
    stock_data_scaled,
    columns=["MeanOpen", "MeanHigh", "MeanLow", "MeanClose", "MeanVolume", "MeanPercentReturn"]
)

# Create a Ticker column in the df_stocks_scaled DataFrame
# using the index of the original df_stocks DataFrame
df_stocks_scaled["Ticker"] = df_stocks.index

# Set the newly created Ticker column as index of the df_stocks_scaled DataFrame
df_stocks_scaled = df_stocks_scaled.set_index("Ticker")

# Review the DataFrame
df_stocks_scaled.head()

# Encode the Sector column
sector_encoded_df = pd.get_dummies(df_stocks["Sector"])

# Review the DataFrame
sector_encoded_df.head()

# Concatenate the `Sector` encoded DataFrame with the scaled data DataFrame
scaled_encoded_stocks = pd.concat([df_stocks_scaled, sector_encoded_df], axis=1)

# Display the sample data
scaled_encoded_stocks.head()

# Initialize the K-Means model with n_clusters=3
model = KMeans(n_clusters=3, n_init='auto', random_state=1)

# Fit the model for the scaled_encoded_stocks DataFrame
model.fit(scaled_encoded_stocks)

# Create a copy of the scaled_encoded_stocks DataFrame and name it as stocks_scaled_predictions
stocks_scaled_predictions = scaled_encoded_stocks.copy()

# Predict the model segments (clusters)
stock_clusters = model.predict(stocks_scaled_predictions)

# Create a new column in the DataFrame with the predicted clusters
stocks_scaled_predictions["StockCluster"] = stock_clusters

# Review the DataFrame
stocks_scaled_predictions.head()

# Create a scatter plot with x="MeanOpen" and y="MeanPercentReturn"
stocks_scaled_predictions.plot.scatter(
    x="MeanOpen",
    y="MeanPercentReturn",
    c="StockCluster",
    colormap='winter')

# Create the PCA model instance where n_components=2
pca = PCA(n_components=2)

# Fit the scaled_encoded_stocks data to the PCA
stocks_pca_data = pca.fit_transform(scaled_encoded_stocks)

# Review the first five rose of the PCA data
# using bracket notation ([0:5])
stocks_pca_data[:5]

# Calculate the explained variance
pca.explained_variance_ratio_

# Creating a DataFrame with the PCA data
df_stocks_pca = pd.DataFrame(stocks_pca_data, columns=["PCA1", "PCA2"])

# Copy the tickers names from the original data
df_stocks_pca["Ticker"] = df_stocks.index

# Set the Ticker column as index
df_stocks_pca = df_stocks_pca.set_index("Ticker")

# Review the DataFrame
df_stocks_pca.head()

# Initialize the K-Means model with n_clusters=3
model = KMeans(n_clusters=3, n_init='auto', random_state=1)

# Fit the model for the df_stocks_pca DataFrame
model.fit(df_stocks_pca)

# Predict the model segments (clusters)
stock_clusters = model.predict(df_stocks_pca)

# Create a copy of the df_stocks_pca DataFrame and name it as df_stocks_pca_predictions
df_stocks_pca_predictions = df_stocks_pca.copy()

# Create a new column in the DataFrame with the predicted clusters
df_stocks_pca_predictions["StockCluster"] = stock_clusters

# Review the DataFrame
df_stocks_pca_predictions.head()

# Create the scatter plot with x="PCA1" and y="PCA2"
df_stocks_pca_predictions.plot.scatter(
    x="PCA1",
    y="PCA2",
    c="StockCluster",
    colormap='winter')

# Use the columns from the scaled and encoded DataFrame in step 2.
pca_component_weights = pd.DataFrame(pca.components_.T, columns=['PCA1', 'PCA2'], index=scaled_encoded_stocks.columns)
pca_component_weights

# Plot the features that have the strongest influence on each component. 
stocks_scaled_predictions.plot.scatter(
    x="MeanOpen",
    y="MeanVolume",
    c="StockCluster",
    colormap='winter')



# Import the required libraries and dependencies
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Read the CSV file into a Pandas DataFrame
# Set the index using the Ticker column
df_stocks = pd.read_csv("Resources/stock_data.csv", index_col="Ticker")

# Review the DataFrame
df_stocks.head()

# Get the shape of the DataFrame
df_stocks.shape

# Get the information on the DataFrame
df_stocks.info()

# Use the standard scaler, fit_transform to scale the numerical columns. 
stock_data_scaled = StandardScaler().fit_transform(df_stocks[["MeanOpen", "MeanHigh", "MeanLow", "MeanClose", "MeanVolume", "MeanPercentReturn"]])

# Create a DataFrame called with the scaled data
# The column names should match those referenced in the StandardScaler step
df_stocks_scaled = pd.DataFrame(
    stock_data_scaled,
    columns=["MeanOpen", "MeanHigh", "MeanLow", "MeanClose", "MeanVolume", "MeanPercentReturn"]
)

# Create a Ticker column in the df_stocks_scaled DataFrame
# using the index of the original df_stocks DataFrame
df_stocks_scaled["Ticker"] = df_stocks.index

# Set the newly created Ticker column as index of the df_stocks_scaled DataFrame
df_stocks_scaled = df_stocks_scaled.set_index("Ticker")

# Review the DataFrame
df_stocks_scaled.head()

# Encode the Sector column
sector_encoded_df = pd.get_dummies(df_stocks["Sector"])

# Review the DataFrame
sector_encoded_df.head()

# Concatenate the `Sector` encoded DataFrame with the scaled data DataFrame
scaled_encoded_stocks = pd.concat([df_stocks_scaled, sector_encoded_df], axis=1)

# Display the sample data
scaled_encoded_stocks.head()

# Initialize the K-Means model with n_clusters=3


# Fit the model for the scaled_encoded_stocks DataFrame


# Create a copy of the scaled_encoded_stocks DataFrame and name it as stocks_scaled_predictions


# Predict the model segments (clusters)


# Create a new column in the DataFrame with the predicted clusters


# Review the DataFrame


# Create a scatter plot with x="MeanOpen" and y="MeanPercentReturn"


# Create the PCA model instance where n_components=2


# Fit the scaled_encoded_stocks data to the PCA


# Review the first five rose of the PCA data
# using bracket notation ([0:5])


# Calculate the explained variance


# Creating a DataFrame with the PCA data

# Copy the tickers names from the original data


# Set the Ticker column as index


# Review the DataFrame


# Initialize the K-Means model with n_clusters=3


# Fit the model for the df_stocks_pca DataFrame


# Predict the model segments (clusters)


# Create a copy of the df_stocks_pca DataFrame and name it as df_stocks_pca_predictions


# Create a new column in the DataFrame with the predicted clusters


# Review the DataFrame


# Create the scatter plot with x="PCA1" and y="PCA2"


# Use the columns from the scaled and encoded DataFrame in step 2.


# Plot the features that have the strongest influence on each component. 




# Import required libraries
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# Read salary data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/salary-data.csv"
df_salary = pd.read_csv(file_path)

# Display sample data
df_salary.head()

# Create a scatter plot with the salary information
salary_plot = df_salary.plot.scatter(
    x="years_experience",
    y="salary",
    title="Expected Salary Based on Years of Experience"
)
salary_plot

# Reformat data of the independent variable X as a single-column array
X = df_salary["years_experience"].values.reshape(-1, 1)

# Display sample data
X[:5]

# The shape of X is 30 samples, with a single feature (column)
X.shape

# Create an array for the dependent variable y
y = df_salary["salary"]

# Create a model with scikit-learn
model = LinearRegression()

# Fit the data into the model
model.fit(X, y)

# Display the slope
print(f"Model's slope: {model.coef_}")

# Display the y-intercept
print(f"Model's y-intercept: {model.intercept_}")

# Display the model's best fit line formula
print(f"Model's formula: y = {model.intercept_} + {model.coef_[0]}X")

# Display the formula to predict the salary for a person with 7 years of experience
print(f"Model's formula: y = {model.intercept_} + {model.coef_[0]} * 7")

# Predict the salary for a person with 7 years of experience
y_7 = model.intercept_ + model.coef_[0] * 7

# Display the prediction
print(f"Predicted salary for a person with 7 years of experience: ${y_7:.2f}")

# Make predictions using the X set
predicted_y_values = model.predict(X)

# Create a copy of the original data
df_salary_predicted = df_salary.copy()

# Add a column with the predicted salary values
df_salary_predicted["salary_predicted"] = predicted_y_values

# Display sample data
df_salary_predicted.head()

# Create a line plot of the predicted salary values
best_fit_line = df_salary_predicted.plot.line(
    x = "years_experience",
    y = "salary_predicted",
    color = "red"
)
best_fit_line

# Plot salary scatter and best fit line together
salary_plot = df_salary_predicted.plot.scatter(
    x="years_experience",
    y="salary",
    title="Expected Salary Based on Years of Experience"
)

# Create a line plot of the predicted salary values
best_fit_line = df_salary_predicted.plot.line(
    x = "years_experience",
    y = "salary_predicted",
    color = "red",
    ax=salary_plot
)
salary_plot



# Import required libraries
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# Read salary data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/salary-data.csv"
df_salary = pd.read_csv(file_path)

# Display sample data
df_salary.head()

# Create a scatter plot with the salary information
salary_plot = df_salary.plot.scatter(
    x="years_experience",
    y="salary",
    title="Expected Salary Based on Years of Experience"
)
salary_plot

# Reformat data of the independent variable X as a single-column array


# Display sample data


# The shape of X is 30 samples, with a single feature (column)


# Create an array for the dependent variable y


# Create a model with scikit-learn


# Fit the data into the model


# Display the slope


# Display the y-intercept


# Display the model's best fit line formula


# Display the formula to predict the salary for a person with 7 years of experience

# Predict the salary for a person with 7 years of experience

# Display the prediction


# Make predictions using the X set


# Create a copy of the original data

# Add a column with the predicted salary values

# Display sample data


# Create a line plot of the predicted salary values
best_fit_line = df_salary_predicted.plot.line(
    x = "years_experience",
    y = "salary_predicted",
    color = "red"
)
best_fit_line

# Plot salary scatter and best fit line together
salary_plot = df_salary_predicted.plot.scatter(
    x="years_experience",
    y="salary",
    title="Expected Salary Based on Years of Experience"
)

# Create a line plot of the predicted salary values
best_fit_line = df_salary_predicted.plot.line(
    x = "years_experience",
    y = "salary_predicted",
    color = "red",
    ax=salary_plot
)
salary_plot



# Import required libraries
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# Read the sales data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/sales.csv"
df_sales = pd.read_csv(file_path)

# Display sample data
df_sales.head()

# Create a scatter plot with the sales information
sales_plot = df_sales.plot.scatter(
    x="ads",
    y="sales",
    title="Sales per Number of Ads"
)
sales_plot

# Create the X set by using the `reshape` function to format the ads data as a single column array.
X = df_sales["ads"].values.reshape(-1, 1)

# Display sample data
X[:5]

# Create an array for the dependent variable y with the sales data
y = df_sales["sales"]

# Create a model with scikit-learn
model = LinearRegression()

# Fit the data into the model
model.fit(X, y)

# Display the slope
print(f"Model's slope: {model.coef_}")

# Display the y-intercept
print(f"Model's y-intercept: {model.intercept_}")

# Display the model's best fit line formula
print(f"Model's formula: y = {model.intercept_} + {model.coef_[0]}X")

# Make predictions using the X set
predicted_y_values = model.predict(X)

# Create a copy of the original data
df_sales_predicted = df_sales.copy()

# Add a column with the predicted sales values
df_sales_predicted["sales_predicted"] = predicted_y_values

# Display sample data
df_sales_predicted.head()

# Create a line plot of the predicted salary values
best_fit_line = df_sales_predicted.plot.line(
    x = "ads",
    y = "sales_predicted",
    color = "red"
)
best_fit_line

# Superpose the original data and the best fit line
# Create a scatter plot with the sales information
sales_plot = df_sales_predicted.plot.scatter(
    x="ads",
    y="sales",
    title="Sales per Number of Ads"
)

best_fit_line = df_sales_predicted.plot.line(
    x = "ads",
    y = "sales_predicted",
    color = "red",
    ax=sales_plot
)
sales_plot

# Display the formula to predict the sales with 100 ads
print(f"Model's formula: y = {model.intercept_} + {model.coef_[0]} * 100")

# Predict the sales with 100 ads
y_100 = model.intercept_ + model.coef_[0] * 100

# Display the prediction
print(f"Predicted sales with 100 ads: ${y_100:.2f}")

# Create an array to predict sales for 100, 150, 200, 250, and 300 ads
X_ads = np.array([100, 150, 200, 250, 300])

# Format the array as a one-column array
X_ads = X_ads.reshape(-1,1)

# Display sample data
X_ads

# Predict sales for 100, 150, 200, 250, and 300 ads
predicted_sales = model.predict(X_ads)

# Create a DataFrame for the predicted sales
df_predicted_sales = pd.DataFrame(
    {
        "ads": X_ads.reshape(1, -1)[0],
        "predicted_sales": predicted_sales
    }
)

# Display data
df_predicted_sales



# Import required libraries
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# Read the sales data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/sales.csv"
df_sales = pd.read_csv(file_path)

# Display sample data
df_sales.head()

# Create a scatter plot with the sales information
sales_plot = df_sales.plot.scatter(
    x="ads",
    y="sales",
    title="Sales per Number of Ads"
)
sales_plot

# Create the X set by using the `reshape` function to format the ads data as a single column array.


# Display sample data


# Create an array for the dependent variable y with the sales data


# Create a model with scikit-learn


# Fit the data into the model


# Display the slope


# Display the y-intercept


# Display the model's best fit line formula


# Make predictions using the X set


# Create a copy of the original data


# Add a column with the predicted sales values


# Display sample data


# Create a line plot of the predicted salary values


# Superpose the original data and the best fit line
# Create a scatter plot with the sales information


# Display the formula to predict the sales with 100 ads


# Predict the sales with 100 ads


# Display the prediction


# Create an array to predict sales for 100, 150, 200, 250, and 300 ads


# Format the array as a one-column array


# Display sample data


# Predict sales for 100, 150, 200, 250, and 300 ads


# Create a DataFrame for the predicted sales


# Display data


# Import required libraries
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# Read salary data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/salary-data.csv"
df_salary = pd.read_csv(file_path)

# Display sample data
df_salary.head()

# Reformat data of the independent variable X as a single-column array
X = df_salary["years_experience"].values.reshape(-1, 1)

# Display sample data
X[:5]

# The shape of X is 30 samples, with a single feature (column)
X.shape

# Create an array for the dependent variable y
y = df_salary["salary"]

# Create a model with scikit-learn
model = LinearRegression()

# Fit the data into the model
model.fit(X, y)

# Make predictions using the X set
predicted_y_values = model.predict(X)

# Create a copy of the original data
df_salary_predicted = df_salary.copy()

# Add a column with the predicted salary values
df_salary_predicted["salary_predicted"] = predicted_y_values

# Display sample data
df_salary_predicted.head()

# Import relevant metrics - score, r2, mse, rmse - from Scikit-learn
from sklearn.metrics import mean_squared_error, r2_score

# Compute the metrics for the linear regression model
score = round(model.score(X, y, sample_weight=None),5)
r2 = round(r2_score(y, predicted_y_values),5)
mse = round(mean_squared_error(y, predicted_y_values),4)
rmse = round(np.sqrt(mse),4)

# Print relevant metrics.
print(f"The score is {score}.")
print(f"The r2 is {r2}.")
print(f"The mean squared error is {mse}.")
print(f"The root mean squared error is {rmse}.")



# Import required libraries
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# Read salary data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/salary-data.csv"
df_salary = pd.read_csv(file_path)

# Display sample data
df_salary.head()

# Reformat data of the independent variable X as a single-column array
X = df_salary["years_experience"].values.reshape(-1, 1)

# Display sample data
X[:5]

# The shape of X is 30 samples, with a single feature (column)
X.shape

# Create an array for the dependent variable y
y = df_salary["salary"]

# Create a model with scikit-learn
model = LinearRegression()

# Fit the data into the model
model.fit(X, y)

# Make predictions using the X set
predicted_y_values = model.predict(X)

# Create a copy of the original data
df_salary_predicted = df_salary.copy()

# Add a column with the predicted salary values
df_salary_predicted["salary_predicted"] = predicted_y_values

# Display sample data
df_salary_predicted.head()

# Import relevant metrics - score, r2, mse, rmse - from Scikit-learn
from sklearn.metrics import mean_squared_error, r2_score

# Compute the metrics for the linear regression model


# Print relevant metrics.




# Import required libraries
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# Read the sales data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/sales.csv"
df_sales = pd.read_csv(file_path)

# Display sample data
df_sales.head()

# Create the X set by using the `reshape` function to format the ads data as a single column array.
X = df_sales["ads"].values.reshape(-1, 1)

# Display sample data
X[:5]

# Create an array for the dependent variable y with the sales data
y = df_sales["sales"]

# Create a model with scikit-learn
model = LinearRegression()

# Fit the data into the model
model.fit(X, y)

# Make predictions using the X set
predicted_y_values = model.predict(X)

# Create a copy of the original data
df_sales_predicted = df_sales.copy()

# Add a column with the predicted sales values
df_sales_predicted["sales_predicted"] = predicted_y_values

# Display sample data
df_sales_predicted.head()

# Import relevant metrics - score, r2, mse, rmse - from Scikit-learn
from sklearn.metrics import mean_squared_error, r2_score

# Compute the metrics for the linear regression model
score = round(model.score(X, y, sample_weight=None),5)
r2 = round(r2_score(y, predicted_y_values),5)
mse = round(mean_squared_error(y, predicted_y_values),4)
rmse = round(np.sqrt(mse),4)

# Print relevant metrics.
print(f"The score is {score}.")
print(f"The r2 is {r2}.")
print(f"The mean squared error is {mse}.")
print(f"The root mean squared error is {rmse}.")



# Import required libraries
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# Read the sales data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/sales.csv"
df_sales = pd.read_csv(file_path)

# Display sample data
df_sales.head()

# Create the X set by using the `reshape` function to format the ads data as a single column array.
X = df_sales["ads"].values.reshape(-1, 1)

# Display sample data
X[:5]

# Create an array for the dependent variable y with the sales data
y = df_sales["sales"]

# Create a model with scikit-learn
model = LinearRegression()

# Fit the data into the model
model.fit(X, y)

# Make predictions using the X set
predicted_y_values = model.predict(X)

# Create a copy of the original data
df_sales_predicted = df_sales.copy()

# Add a column with the predicted sales values
df_sales_predicted["sales_predicted"] = predicted_y_values

# Display sample data
df_sales_predicted.head()

# Import relevant metrics - score, r2, mse, rmse - from Scikit-learn
from sklearn.metrics import mean_squared_error, r2_score

# Compute the metrics for the linear regression model


# Print relevant metrics.




# Import required libraries
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# Read the electricity generation data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/electricity-generation.csv"
df_electricity = pd.read_csv(file_path)

# Display sample data
df_electricity.head()

# Create a scatter plot with the total electricity generation by year
electricity_plot = df_electricity.plot.scatter(
    x="Year",
    y="Total",
    title="Total electricity generation by year (GHz)"
)
electricity_plot

# The first three years look like significant outliers.
# Reduce the DataFrame to just the "Year" and "Total" columns
# and only from 2003 onwards

df = pd.DataFrame(df_electricity.loc[df_electricity["Year"]>=2003,["Year","Total"]])\
        .reset_index().drop(columns="index")
df.head()

# Create the X set
X = df["Year"].values.reshape(-1, 1)

# Display sample data
X[:5]

# Create an array for the dependent variable y with the total electricity generation data
y = df["Total"]

# Create a model with scikit-learn
model = LinearRegression()

# Fit the data into the model
model.fit(X, y)

# Display the slope
print(f"Model's slope: {model.coef_}")

# Display the y-intercept
print(f"Model's y-intercept: {model.intercept_}")

# Display the model's best fit line formula
print(f"Model's formula: y = {model.intercept_} + {model.coef_[0]}X")

# Make predictions using the X set
predicted_y_values = model.predict(X)

# Create a copy of the original data
df_electricity_predicted = df.copy()

# Add a column with the predicted electricity values
df_electricity_predicted["electricity_predicted"] = predicted_y_values

# Display sample data
df_electricity_predicted.head()

# Create a line plot of the predicted total electricity generation values
best_fit_line = df_electricity_predicted.plot.line(
    x = "Year",
    y = "electricity_predicted",
    color = "red"
)
best_fit_line

# Superpose the original data and the best fit line
# Create a scatter plot with the electricity information
electricity_plot = df_electricity_predicted.plot.scatter(
    x="Year",
    y="Total",
    title="Electricity Generation by Year (GHz)"
)

# Create a line plot of the predicted total electricity generation values
best_fit_line = df_electricity_predicted.plot.line(
    x = "Year",
    y = "electricity_predicted",
    color = "red",
    ax=electricity_plot
)
electricity_plot

# Display the formula to predict the electricity generation for 2023
print(f"Model's formula: y = {model.intercept_} + {model.coef_[0]} * 2023")

# Predict the electricity generation for 2023
y_2023 = model.intercept_ + model.coef_[0] * 2023

# Display the prediction
print(f"Predicted electricity generation for 2023: {y_2023:.2f}")

# Create an array to predict electricity generation for the years 2020, 2021, 2022, and 2023
X_years = np.array([2020, 2021, 2022, 2023])

# Format the array as a one-column array
X_years = X_years.reshape(-1,1)

# Display sample data
X_years

# Predict electricity generation for the years 2020, 2021, 2022, and 2023
predicted_electricity = model.predict(X_years)

# Create a DataFrame for the predicted electricity generation
df_predicted_electricity = pd.DataFrame(
    {
        "Year": X_years.reshape(1, -1)[0],
        "predicted_electricity": predicted_electricity
    }
)

# Display data
df_predicted_electricity

# Import relevant metrics - score, r2, mse, rmse - from Scikit-learn
from sklearn.metrics import mean_squared_error, r2_score

# Compute the metrics for the linear regression model
score = round(model.score(X, y, sample_weight=None),5)
r2 = round(r2_score(y, predicted_y_values),5)
mse = round(mean_squared_error(y, predicted_y_values),4)
rmse = round(np.sqrt(mse),4)

# Print relevant metrics.
print(f"The score is {score}.")
print(f"The r2 is {r2}.")
print(f"The mean squared error is {mse}.")
print(f"The root mean squared error is {rmse}.")



# Import required libraries
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# Read the electricity generation data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/electricity-generation.csv"
df_electricity = pd.read_csv(file_path)

# Display sample data
df_electricity.head()

# Create a scatter plot with the total electricity generation by year
electricity_plot = df_electricity.plot.scatter(
    x="Year",
    y="Total",
    title="Total electricity generation by year (GHz)"
)
electricity_plot

# The first three years look like significant outliers.
# Reduce the DataFrame to just the "Year" and "Total" columns
# and only from 2003 onwards

df = pd.DataFrame(df_electricity.loc[df_electricity["Year"]>=2003,["Year","Total"]])\
        .reset_index().drop(columns="index")
df.head()

# Create the X set


# Display sample data


# Create an array for the dependent variable y with the total electricity generation data


# Create a model with scikit-learn


# Fit the data into the model


# Display the slope


# Display the y-intercept


# Display the model's best fit line formula


# Make predictions using the X set


# Create a copy of the original data


# Add a column with the predicted electricity values


# Display sample data


# Create a line plot of the predicted total electricity generation values


# Superpose the original data and the best fit line
# Create a scatter plot with the electricity information


# Create a line plot of the predicted total electricity generation values


# Display the formula to predict the electricity generation for 2023


# Predict the electricity generation for 2023


# Display the prediction


# Create an array to predict electricity generation for the years 2020, 2021, 2022, and 2023


# Format the array as a one-column array


# Display sample data


# Predict electricity generation for the years 2020, 2021, 2022, and 2023


# Create a DataFrame for the predicted electricity generation


# Display data


# Import relevant metrics - score, r2, mse, rmse - from Scikit-learn
from sklearn.metrics import mean_squared_error, r2_score

# Compute the metrics for the linear regression model


# Print relevant metrics.




import pandas as pd

# Import the data
# Note: NA values in this dataset are represented as "?"
car_data = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/car-data.csv", na_values="?")
car_data

# Check dtypes
car_data.dtypes

# Object features should be converted to numbers.
# num-of-doors and num-of-cylinders are both numbers written as text.
# Create a dictionary of the text and integers that should be converted
str_to_int = {"eight": 8, 
              "five": 5,
              "four": 4,
              "six": 6,
              "three": 3,
              "twelve": 12,
              "two": 2}

# Fix the columns using the Pandas replace() method
car_data[["num-of-doors","num-of-cylinders"]] = car_data[["num-of-doors","num-of-cylinders"]].replace(str_to_int, regex=False)
car_data

# Check dtypes
car_data.dtypes

# Encode using pd.get_dummies()
car_data_dummies = pd.get_dummies(car_data)
car_data_dummies.head()

# Check column names
car_data_dummies.columns

# Use Pandas .astype("category").cat.codes for single column category encoding
columns_to_encode = ["make",
                     "fuel-type",
                     "aspiration",
                     "body-style",
                     "drive-wheels",
                     "engine-location",
                     "engine-type",
                     "fuel-system"]

# Copy car_data
car_data_cat_codes = car_data.copy()

# Loop through columns_to_encode and convert the columns to category codes
for column in columns_to_encode:
    car_data_cat_codes[column] = car_data_cat_codes[column].astype("category").cat.codes

car_data_cat_codes.head()

# Check dtypes
car_data_cat_codes.dtypes

# OneHotEncoder
# Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html
from sklearn.preprocessing import OneHotEncoder

# Create an instance of OneHotEncoder()
enc = OneHotEncoder(handle_unknown='ignore')

# Fit the encoder to the data
enc.fit(car_data[columns_to_encode])

# Transform the data
car_data_ohe = enc.transform(car_data[columns_to_encode])

# Default output is sparse matrix
car_data_ohe

# Get new feature names
enc.get_feature_names_out()

# Set up the OneHotEncoder so it will transform to Pandas
ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
ohe.set_output(transform="pandas")

# Fit and transform the OneHotEncoder to the columns to encode
car_data_ohe = ohe.fit_transform(car_data[columns_to_encode])
car_data_ohe.head()

# LabelEncoder
# Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html
from sklearn.preprocessing import LabelEncoder

# Create an instance of the label encoder
le = LabelEncoder()

# Copy car_data
car_data_label_encoded = car_data.copy()

# Fit and transform the label encoder for each column
for column in columns_to_encode:
    car_data_label_encoded[column] = le.fit_transform(car_data_label_encoded[column])

car_data_label_encoded.head()

# Check dtypes
car_data_label_encoded.dtypes





import pandas as pd

# Import the data
# Note: NA values in this dataset are represented as "?"
car_data = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/car-data.csv", na_values="?")
car_data

# Check dtypes
car_data.dtypes

# Object features should be converted to numbers.
# num-of-doors and num-of-cylinders are both numbers written as text.
# Create a dictionary of the text and integers that should be converted
str_to_int = {"eight": 8, 
              "five": 5,
              "four": 4,
              "six": 6,
              "three": 3,
              "twelve": 12,
              "two": 2}

# Fix the columns using the Pandas replace() method


# Check dtypes
car_data.dtypes

# Encode using pd.get_dummies()


# Check column names


# Use Pandas .astype("category").cat.codes for single column category encoding
columns_to_encode = ["make",
                     "fuel-type",
                     "aspiration",
                     "body-style",
                     "drive-wheels",
                     "engine-location",
                     "engine-type",
                     "fuel-system"]

# Copy car_data


# Loop through columns_to_encode and convert the columns to category codes


# Check dtypes


# OneHotEncoder
# Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html
from sklearn.preprocessing import OneHotEncoder

# Create an instance of OneHotEncoder()


# Fit the encoder to the data


# Transform the data


# Default output is sparse matrix


# Get new feature names


# Set up the OneHotEncoder so it will transform to Pandas


# Fit and transform the OneHotEncoder to the columns to encode


# LabelEncoder
# Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html
from sklearn.preprocessing import LabelEncoder

# Create an instance of the label encoder


# Copy car_data


# Fit and transform the label encoder for each column


# Check dtypes




import pandas as pd
from sklearn.model_selection import train_test_split

# Import the data
car_data = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/car-data-encoded.csv")
car_data

# Get the features (everything except the "price" column)
X = car_data.copy().drop(columns="price")
X.head()

# Get the target column
y = car_data["price"]
y.head()

# Use the Sklearn `train_test_split()` function to split the data into training and testing data
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Display X_train
X_train

# Display X_test
X_test

# Display y_train
y_train

# Display y_test
y_test

# Check columns
car_data.columns

# Features to include: "make", "fuel-type", "num-of-doors", "drive-wheels",
# "length", "width", "height", "engine-size", "fuel-system", "city-mpg"
features = ["make", "fuel-type", "num-of-doors", "drive-wheels", 
            "length", "width", "height", "engine-size", 
            "fuel-system", "city-mpg"]
X = car_data[features]
X.head()

# Now split the data into training and testing sets again
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Preview X_train
X_train.head()



import pandas as pd
from sklearn.model_selection import train_test_split

# Import the data
car_data = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/car-data-encoded.csv")
car_data

# Get the features (everything except the "price" column)
X = car_data.copy().drop(columns="price")
X.head()

# Get the target column
y = car_data["price"]
y.head()

# Use the Sklearn `train_test_split()` function to split the data into training and testing data


# Display X_train


# Display X_test


# Display y_train


# Display y_test


# Check columns


# Features to include: "make", "fuel-type", "num-of-doors", "drive-wheels",
# "length", "width", "height", "engine-size", "fuel-system", "city-mpg"
features = ["make", "fuel-type", "num-of-doors", "drive-wheels", 
            "length", "width", "height", "engine-size", 
            "fuel-system", "city-mpg"]


# Now split the data into training and testing sets again


# Preview X_train




import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Import the data
car_data = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/car-data-encoded.csv")
car_data

# Drop na rows
car_data = car_data.dropna()

# Features to include: "make", "fuel-type", "num-of-doors", "drive-wheels",
# "length", "width", "height", "engine-size", "fuel-system", "city-mpg"
features = ["make", "fuel-type", "num-of-doors", "drive-wheels", 
            "length", "width", "height", "engine-size", 
            "fuel-system", "city-mpg"]
X = car_data[features]
X.head()

# Set the target variable y
y = car_data["price"]

# Now split the data into training and testing sets again
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Create the model
model = LinearRegression()

# Fit the model to the training data. 
model.fit(X_train, y_train)

# Calculate the mean_squared_error and the r-squared value
# for the testing data

# Use our model to make predictions
predicted = model.predict(X_test)

# Score the predictions with mse and r2
mse = mean_squared_error(y_test, predicted)
r2 = r2_score(y_test, predicted)

print(f"mean squared error (MSE): {mse}")
print(f"R-squared (R2): {r2}")

# Call the `score()` method on the model to show the R2 score
model.score(X_test, y_test)



import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Import the data
car_data = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/car-data-encoded.csv")
car_data

# Drop na rows
car_data = car_data.dropna()

# Features to include: "make", "fuel-type", "num-of-doors", "drive-wheels",
# "length", "width", "height", "engine-size", "fuel-system", "city-mpg"
features = ["make", "fuel-type", "num-of-doors", "drive-wheels", 
            "length", "width", "height", "engine-size", 
            "fuel-system", "city-mpg"]
X = car_data[features]
X.head()

# Set the target variable y
y = car_data["price"]

# Now split the data into training and testing sets again
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Create the model


# Fit the model to the training data. 


# Calculate the mean_squared_error and the r-squared value
# for the testing data

# Use our model to make predictions


# Score the predictions with mse and r2


# Call the `score()` method on the model to show the R2 score




import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# Read the CSV file into a Pandas DataFrame
Lp100km = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/liters-per-100km.csv')
Lp100km.head()

# Plot the cylinders & L/100km to find out if a linear trend exists
Lp100km.plot.scatter(x='cylinders', y='L/100km')

# Plot the displacement & L/100km to find out if a linear trend exists

Lp100km.plot.scatter(x='displacement', y='L/100km')

# Plot the horesepower & L/100km to find out if a linear trend exists

Lp100km.plot.scatter(x='horsepower', y='L/100km')

# Plot the weight (kg) & L/100km to find out if a linear trend exists

Lp100km.plot.scatter(x='weight (kg)', y='L/100km')

# Plot the acceleration & L/100km to find out if a linear trend exists

Lp100km.plot.scatter(x='acceleration', y='L/100km')

# Assign the variable X to the two features that appear to have the most linear relationship with L/100km
# Note: scikit-learn requires a two-dimensional array of values
# so we use reshape() to create this

X = Lp100km[["weight (kg)", "displacement"]].values.reshape(-1, 2)
y = Lp100km["L/100km"].values.reshape(-1, 1)

print("Shape: ", X.shape, y.shape)

# Use the Sklearn `train_test_split()` function to split the data into training and testing data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Create the model
model = LinearRegression()

# Fit the model to the training data. 
model.fit(X_train, y_train)

# Calculate the mean_squared_error and the r-squared value
# for the testing data

from sklearn.metrics import mean_squared_error, r2_score

# Use our model to make predictions
predicted = model.predict(X_test)

# Score the predictions with mse and r2
mse = mean_squared_error(y_test, predicted)
r2 = r2_score(y_test, predicted)

print(f"mean squared error (MSE): {mse}")
print(f"R-squared (R2): {r2}")

# Call the `score()` method on the model to show the R2 score
model.score(X_test, y_test)



import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# Read the CSV file into a Pandas DataFrame
Lp100km = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/liters-per-100km.csv')
Lp100km.head()

# Plot the cylinders & L/100km to find out if a linear trend exists
Lp100km.plot.scatter(x='cylinders', y='L/100km')

# Plot the displacement & L/100km to find out if a linear trend exists

Lp100km.plot.scatter(x='displacement', y='L/100km')

# Plot the horesepower & L/100km to find out if a linear trend exists

Lp100km.plot.scatter(x='horsepower', y='L/100km')

# Plot the weight (kg) & L/100km to find out if a linear trend exists

Lp100km.plot.scatter(x='weight (kg)', y='L/100km')

# Plot the acceleration & L/100km to find out if a linear trend exists

Lp100km.plot.scatter(x='acceleration', y='L/100km')

# Assign the variable X to the two features that appear to have the most linear relationship with L/100km
# Note: scikit-learn requires a two-dimensional array of values
# so we use reshape() to create this



# Use the Sklearn `train_test_split()` function to split the data into training and testing data
from sklearn.model_selection import train_test_split


# Create the model


# Fit the model to the training data. 


# Calculate the mean_squared_error and the r-squared value
# for the testing data

from sklearn.metrics import mean_squared_error, r2_score

# Use our model to make predictions


# Score the predictions with mse and r2


# Call the `score()` method on the model to show the R2 score




import pandas as pd

# Import the data with the following arguments:
# delimiter=";", encoding="latin_1"
rent_data = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/apartments-for-rent.csv", delimiter=";", encoding="latin_1")
rent_data

# Check dtypes
rent_data.dtypes

# Drop columns: "id", "title", "body", "currency", "price_display", "address"
columns_to_drop = ["id", "title", "body", "currency", "price_display", "address"]
rent_data_cleaned = rent_data.copy().drop(columns=columns_to_drop)
rent_data_cleaned

# Get a list of the amenities
amenities = rent_data_cleaned["amenities"].dropna().drop_duplicates().to_list()
amenities

# Join the amenities list items, separated by a comma
amenities_joined = ','.join(amenities)
amenities_joined

# Get single amenities in a list with no duplicates
amenities_list = list(dict.fromkeys(amenities_joined.split(",")))
amenities_list

# Add columns for each amenity
rent_data_cleaned[amenities_list] = 0
rent_data_cleaned[amenities_list]

# Update the amenities columns
# Loop through the amenities list
for amenity in amenities_list:
    rent_data_cleaned.loc[rent_data_cleaned["amenities"].str.contains(amenity, na=False),amenity] = 1
rent_data_cleaned

# Drop the (combined) amenities column
rent_data_cleaned = rent_data_cleaned.drop(columns="amenities")
rent_data_cleaned.head()

# Check the dtypes again
rent_data_cleaned.dtypes

# Create a list of the columns to encode
columns_to_encode = ["category", 
                     "fee", 
                     "has_photo", 
                     "pets_allowed", 
                     "price_type", 
                     "cityname", 
                     "state", 
                     "source"]

# Encode using pd.get_dummies()
rent_data_dummies = pd.get_dummies(rent_data_cleaned)
rent_data_dummies.head()

# Check column names
rent_data_dummies.columns

# Use Pandas .astype("category").cat.codes for single column category encoding
# Copy rent_data_cleaned
rent_data_cat_codes = rent_data_cleaned.copy()

# Loop through columns_to_encode and convert the columns to category codes
for column in columns_to_encode:
    rent_data_cat_codes[column] = rent_data_cat_codes[column].astype("category").cat.codes

rent_data_cat_codes.head()

# Check dtypes
rent_data_cat_codes.dtypes

# OneHotEncoder
# Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html
from sklearn.preprocessing import OneHotEncoder

# Set up the OneHotEncoder so it will transform to Pandas
ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
ohe.set_output(transform="pandas")

# Fit and transform the OneHotEncoder to the columns to encode
rent_data_ohe = ohe.fit_transform(rent_data_cleaned[columns_to_encode])
rent_data_ohe.head()

# LabelEncoder
# Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html
from sklearn.preprocessing import LabelEncoder

# Create an instance of the label encoder
le = LabelEncoder()

# Copy rent_data_cleaned
rent_data_label_encoded = rent_data_cleaned.copy()

# Fit and transform the label encoder for each column
for column in columns_to_encode:
    rent_data_label_encoded[column] = le.fit_transform(rent_data_label_encoded[column])

rent_data_label_encoded.head()

# Check dtypes
rent_data_label_encoded.dtypes

# Import module
from sklearn.model_selection import train_test_split

# Choose one of the encoded datasets to split into training and testing sets
# Get the features data, dropping the target "price" and other features you don't want to include
X = rent_data_label_encoded.copy().drop(columns=["price", 
                                                 "latitude", 
                                                 "longitude", 
                                                 "cityname", 
                                                 "state", 
                                                 "fee", 
                                                 "source"])
X.head()

# Get the target variable "price" and assign it to y
y = rent_data_label_encoded["price"]
y.head()

# Use the Sklearn `train_test_split()` function to split the data into training and testing data
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Display X_train
X_train

# Display X_test
X_test

# Display y_train
y_train

# Display y_test
y_test



import pandas as pd

# Import the data with the following arguments:
# delimiter=";", encoding="latin_1"
rent_data = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/apartments-for-rent.csv", delimiter=";", encoding="latin_1")
rent_data

# Check dtypes


# Drop columns: "id", "title", "body", "currency", "price_display", "address"
columns_to_drop = ["id", "title", "body", "currency", "price_display", "address"]


# Get a list of the amenities
amenities = rent_data_cleaned["amenities"].dropna().drop_duplicates().to_list()
amenities

# Join the amenities list items, separated by a comma
amenities_joined = ','.join(amenities)
amenities_joined

# Get single amenities in a list with no duplicates
amenities_list = list(dict.fromkeys(amenities_joined.split(",")))
amenities_list

# Add columns for each amenity
rent_data_cleaned[amenities_list] = 0
rent_data_cleaned[amenities_list]

# Update the amenities columns
# Loop through the amenities list
for amenity in amenities_list:
    rent_data_cleaned.loc[rent_data_cleaned["amenities"].str.contains(amenity, na=False),amenity] = 1
rent_data_cleaned

# Drop the (combined) amenities column
rent_data_cleaned = rent_data_cleaned.drop(columns="amenities")
rent_data_cleaned.head()

# Check the dtypes again
rent_data_cleaned.dtypes

# Create a list of the columns to encode
columns_to_encode = ["category", 
                     "fee", 
                     "has_photo", 
                     "pets_allowed", 
                     "price_type", 
                     "cityname", 
                     "state", 
                     "source"]

# Encode using pd.get_dummies()


# Check column names


# Use Pandas .astype("category").cat.codes for single column category encoding
# Copy rent_data_cleaned


# Loop through columns_to_encode and convert the columns to category codes


# Check dtypes


# OneHotEncoder
# Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html
from sklearn.preprocessing import OneHotEncoder

# Set up the OneHotEncoder so it will transform to Pandas


# Fit and transform the OneHotEncoder to the columns to encode


# LabelEncoder
# Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html
from sklearn.preprocessing import LabelEncoder

# Create an instance of the label encoder


# Copy rent_data_cleaned


# Fit and transform the label encoder for each column


# Check dtypes


# Import module
from sklearn.model_selection import train_test_split

# Choose one of the encoded datasets to split into training and testing sets
# Get the features data, dropping the target "price" and other features you don't want to include


# Get the target variable "price" and assign it to y


# Use the Sklearn `train_test_split()` function to split the data into training and testing data


# Display X_train


# Display X_test


# Display y_train


# Display y_test




import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score

# Read the CSV file into a Pandas DataFrame
Lp100km = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/liters-per-100km.csv')
Lp100km.head()

# Assign the "weight (kg)" feature to X1
# Assign the "weight (kg)" and "cylinders" features to X2
# Note: Scikit-learn requires a two-dimensional array of values
# so we use reshape() to create this

X1 = Lp100km["weight (kg)"].values.reshape(-1, 1)
X2 = Lp100km[["weight (kg)", "cylinders"]].values.reshape(-1, 2)
y = Lp100km["L/100km"].values.reshape(-1, 1)

# Use the Sklearn `train_test_split()` function to split the data into training and testing data
from sklearn.model_selection import train_test_split
X1_train, X1_test, X2_train, X2_test, y_train, y_test = train_test_split(X1, X2, y, random_state=42)

# Create the models
lr1 = LinearRegression()
lr2 = LinearRegression()

# Fit the first model to the training data with a single X feature. 
lr1.fit(X1_train, y_train)

# Fit the second model to the training data with two X features.
lr2.fit(X2_train, y_train)

# Calculate the mean_squared_error and the r-squared value
# for the testing data

from sklearn.metrics import mean_squared_error, r2_score

# Use our model to make predictions
predicted1 = lr1.predict(X1_test)
predicted2 = lr2.predict(X2_test)

# Score the predictions with mse and r2
mse1 = mean_squared_error(y_test, predicted1)
r21 = r2_score(y_test, predicted1)
mse2 = mean_squared_error(y_test, predicted2)
r22 = r2_score(y_test, predicted2)

print(f"Single Feature:")
print(f"mean squared error (MSE): {mse1}")
print(f"R-squared (R2): {r21}")
print("---------------------")
print(f"Two Features:")
print(f"mean squared error (MSE): {mse2}")
print(f"R-squared (R2): {r22}")
print("---------------------")
print(f"Difference: {r21-r22}")

# Provided code to create the adjusted r-squared function
def r2_adj(x, y, model):
    r2 = model.score(x,y)
    n_cols = x.shape[1]
    return 1 - (1 - r2) * (len(y) - 1) / (len(y) - n_cols - 1)

# Calculate the adjusted r-squared value of the model
adj_score1 = r2_adj(X1_test, y_test, lr1)
adj_score2 = r2_adj(X2_test, y_test, lr2)
print(f"1 Feature Adjusted R2: {adj_score1}")
print(f"2 Feature Adjusted R2: {adj_score2}")
print(f"Difference: {adj_score1-adj_score2}")

# Examine linear regression on the single feature data using cross validation
cv_scores = cross_val_score(LinearRegression(), X1_train, y_train, scoring = "r2")
print(f"All scores: {cv_scores}")
print(f"Mean score: {cv_scores.mean()}")
print(f"Standard Deviation: {cv_scores.std()}")



import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score

# Read the CSV file into a Pandas DataFrame
Lp100km = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/liters-per-100km.csv')
Lp100km.head()

# Assign the "weight (kg)" feature to X1
# Assign the "weight (kg)" and "cylinders" features to X2
# Note: Scikit-learn requires a two-dimensional array of values
# so we use reshape() to create this

X1 = Lp100km["weight (kg)"].values.reshape(-1, 1)
X2 = Lp100km[["weight (kg)", "cylinders"]].values.reshape(-1, 2)
y = Lp100km["L/100km"].values.reshape(-1, 1)

# Use the Sklearn `train_test_split()` function to split the data into training and testing data
from sklearn.model_selection import train_test_split
X1_train, X1_test, X2_train, X2_test, y_train, y_test = train_test_split(X1, X2, y, random_state=42)

# Create the models


# Fit the first model to the training data with a single X feature. 


# Fit the second model to the training data with two X features.


# Calculate the mean_squared_error and the r-squared value
# for the testing data

from sklearn.metrics import mean_squared_error, r2_score

# Use our model to make predictions


# Score the predictions with mse and r2



# Provided code to create the adjusted r-squared function
def r2_adj(x, y, model):
    r2 = model.score(x,y)
    n_cols = x.shape[1]
    return 1 - (1 - r2) * (len(y) - 1) / (len(y) - n_cols - 1)

# Calculate the adjusted r-squared value of the model



# Examine linear regression on the single feature data using cross validation





import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Import the data
car_data = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/car-data-encoded.csv")
car_data.head()

# Drop rows with missing values
car_data = car_data.dropna()

# Create a one column X variable with only horsepower
X_one_col = pd.DataFrame(car_data['horsepower'], columns = ['horsepower'])
X_one_col.head()

# Create another variable X__multi_col by creating columns
# containing a single value, which are therefore useless to
# the model
import numpy as np
np.random.RandomState(13)

X_multi_col = X_one_col.copy()
X_multi_col['ones'] = 1
X_multi_col['twos'] = 2
X_multi_col['threes'] = 3
X_multi_col['fours'] = 4
X_multi_col['fives'] = 5
X_multi_col['sixes'] = 6
X_multi_col['sevens'] = 7
X_multi_col['eights'] = 8
X_multi_col.head()

# Set the target variable y
y = car_data["price"].values.reshape(-1, 1)

# Now split the data into training and testing sets again
X_one_col_train, X_one_col_test, X_multi_col_train, X_multi_col_test, y_train, y_test = train_test_split(X_one_col, X_multi_col, y, random_state=13)

# Create the models
lr1 = LinearRegression()
lr2 = LinearRegression()

# Fit the first model to the full training data. 
lr1.fit(X_one_col_train, y_train)

# Fit the second model to the select training data.
lr2.fit(X_multi_col_train, y_train)

# Use .coef_ to view the coefficients of the model
# Note the coefficients; the added columns aren't being used!
lr2.coef_

# Calculate the mean_squared_error and the r-squared value
# for the testing data

# Use our models to make predictions
predicted1 = lr1.predict(X_one_col_test)
predicted2 = lr2.predict(X_multi_col_test)

# Score the predictions with mse and r2
mse1 = round(mean_squared_error(y_test, predicted1), 2)
r21 = round(r2_score(y_test, predicted1), 2)
mse2 = round(mean_squared_error(y_test, predicted2), 2)
r22 = round(r2_score(y_test, predicted2), 2)

print(f"One Column Test:")
print(f"mean squared error (MSE): {mse1}")
print(f"R-squared (R2): {r21}")
print("---------------------")
print(f"Multi Column Test:")
print(f"mean squared error (MSE): {mse2}")
print(f"R-squared (R2): {r22}")
print("---------------------")
print(f"Difference: {r21-r22}")

# Provided code to create the adjusted r-squared function
def r2_adj(x, y, model):
    r2 = model.score(x,y)
    n_cols = x.shape[1]
    return 1 - (1 - r2) * (len(y) - 1) / (len(y) - n_cols - 1)

# Calculate the adjusted r-squared value of the model
adj_score1 = round(r2_adj(X_one_col_test, y_test, lr1), 2)
adj_score2 = round(r2_adj(X_multi_col_test, y_test, lr2), 2)
print(f"One Column Adjusted R2: {adj_score1}")
print(f"Multi Column Adjusted R2: {adj_score2}")
print(f"Difference: {round(adj_score1-adj_score2, 2)}")

# Examine linear regression on the better training data using cross validation
from sklearn.model_selection import cross_val_score
cv_scores = cross_val_score(LinearRegression(), X_one_col_train, y_train, scoring = "r2")
print(f"All scores: {cv_scores}")
print(f"Mean score: {cv_scores.mean()}")
print(f"Standard Deviation: {cv_scores.std()}")



import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Import the data
car_data = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/car-data-encoded.csv")
car_data.head()

# Drop rows with missing values
car_data = car_data.dropna()

# Create a one column X variable with only horsepower
X_one_col = pd.DataFrame(car_data['horsepower'], columns = ['horsepower'])
X_one_col.head()

# Create another variable X__multi_col by creating columns
# containing a single value, which are therefore useless to
# the model
import numpy as np
np.random.RandomState(13)

X_multi_col = X_one_col.copy()
X_multi_col['ones'] = 1
X_multi_col['twos'] = 2
X_multi_col['threes'] = 3
X_multi_col['fours'] = 4
X_multi_col['fives'] = 5
X_multi_col['sixes'] = 6
X_multi_col['sevens'] = 7
X_multi_col['eights'] = 8
X_multi_col.head()

# Set the target variable y
y = car_data["price"].values.reshape(-1, 1)

# Now split the data into training and testing sets again
X_one_col_train, X_one_col_test, X_multi_col_train, X_multi_col_test, y_train, y_test = train_test_split(X_one_col, X_multi_col, y, random_state=13)

# Create the models

# Fit the first model to the full training data. 


# Fit the second model to the select training data.


# Use .coef_ to view the coefficients of the model
# Note the coefficients; the added columns aren't being used!


# Calculate the mean_squared_error and the r-squared value
# for the testing data

# Use our models to make predictions




# Score the predictions with mse and r2




print(f"All Features:")
print(f"mean squared error (MSE): {mse1}")
print(f"R-squared (R2): {r21}")
print("---------------------")
print(f"Select Features:")
print(f"mean squared error (MSE): {mse2}")
print(f"R-squared (R2): {r22}")
print("---------------------")
print(f"Difference: {r21-r22}")

# Provided code to create the adjusted r-squared function
def r2_adj(x, y, model):
    r2 = model.score(x,y)
    n_cols = x.shape[1]
    return 1 - (1 - r2) * (len(y) - 1) / (len(y) - n_cols - 1)

# Calculate the adjusted r-squared value of the model

#YOUR CODE HERE


print(f"All Features Adjusted R2: {adj_score1}")
print(f"Select Features Adjusted R2: {adj_score2}")
print(f"Difference: {round(adj_score1-adj_score2, 2)}")

# Examine linear regression on the better training data using cross validation
from sklearn.model_selection import cross_val_score

# YOUR CODE HERE



print(f"All scores: {cv_scores}")
print(f"Mean score: {cv_scores.mean()}")
print(f"Standard Deviation: {cv_scores.std()}")



import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# Import the data
car_data = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/car-data-encoded.csv")
car_data.head()

# Drop rows with null values
car_data = car_data.dropna()

# Get the features (everything except the "price" column)
X = car_data.copy().drop(columns="price")
X.head()

# Get the target column
y = car_data["price"].values.reshape(-1,1)
y[0:5]

# Use the Sklearn `train_test_split()` function to split the data into training and testing data
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

import statsmodels.api as sm

# Use the statsmodels package to create and fit a linear regression
lr = sm.OLS(y_train, X_train).fit()

# Show the p-values of all columns sorted in ascending order
lr.pvalues.sort_values()

# Create an X variable with all features and another with
# only features that meet the 0.05 threshold.

X_full = X
X_sel = X[['make', 'drive-wheels', 'stroke', 
           'curb-weight', 'aspiration', 'normalized-losses']]

# Split the data into training and testing sets
X_full_train, X_full_test, X_sel_train, X_sel_test, y_train, y_test = train_test_split(X_full, X_sel, y)

# Train two models using the different X variables

# Create the models
lr1 = LinearRegression()
lr2 = LinearRegression()

# Fit the models
lr1.fit(X_full_train, y_train)
lr2.fit(X_sel_train, y_train)

# Provided code to create the adjusted r-squared function
def r2_adj(x, y, model):
    r2 = model.score(x,y)
    n_cols = x.shape[1]
    return 1 - (1 - r2) * (len(y) - 1) / (len(y) - n_cols - 1)

# Compare the adjusted r-squared of the two models
adj_score1 = r2_adj(X_full_test, y_test, lr1)
adj_score2 = r2_adj(X_sel_test, y_test, lr2)
print(f"1 Feature Adjusted R2: {adj_score1}")
print(f"2 Feature Adjusted R2: {adj_score2}")



import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# Import the data
car_data = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/car-data-encoded.csv")
car_data.head()

# Drop rows with null values
car_data = car_data.dropna()

# Get the features (everything except the "price" column)
X = car_data.copy().drop(columns="price")
X.head()

# Get the target column
y = car_data["price"].values.reshape(-1,1)
y[0:5]

# Use the Sklearn `train_test_split()` function to split the data into training and testing data
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

import statsmodels.api as sm

# Use the statsmodels package to create and fit a linear regression



# Show the p-values of all columns sorted in ascending order


# Create an X variable with all features and another with
# only features that meet the 0.05 threshold.



# Split the data into training and testing sets


# Train two models using the different X variables

# Create the models


# Fit the models


# Provided code to create the adjusted r-squared function
def r2_adj(x, y, model):
    r2 = model.score(x,y)
    n_cols = x.shape[1]
    return 1 - (1 - r2) * (len(y) - 1) / (len(y) - n_cols - 1)

# Compare the adjusted r-squared of the two models


import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# Import the data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_2/datasets/rent-data-cleaned.csv").dropna()
df.head()

# Get the features (everything except the "price" column)
X = df.copy().drop(columns="price")
X.head()

# Get the target column
y = df["price"].values.reshape(-1,1)
y[0:5]

# Use the Sklearn `train_test_split()` function to split the data into training and testing data
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

import statsmodels.api as sm

# Use the statsmodels package to create and fit a linear regression
lr = sm.OLS(y_train, X_train).fit()

# Create a variable to hold the p-values of all columns sorted in ascending order
p_values = lr.pvalues.sort_values()
p_values

# Use loc to filter to columns with p-values below 0.05
select_cols = p_values.loc[p_values < 0.05]

# Show the index of the results
select_cols.index

# Create an X variable with all features and another with
# only features that meet the 0.05 threshold.

# Hint: Use the index from the previous cell

X_full = X
X_sel = X[select_cols.index]

# Split the data into training and testing sets
X_full_train, X_full_test, X_sel_train, X_sel_test, y_train, y_test = train_test_split(X_full, X_sel, y)

# Train two models using the different X variables

# Create the models
lr1 = LinearRegression()
lr2 = LinearRegression()

# Fit the models
lr1.fit(X_full_train, y_train)
lr2.fit(X_sel_train, y_train)

# Provided code to create the adjusted r-squared function
def r2_adj(x, y, model):
    r2 = model.score(x,y)
    n_cols = x.shape[1]
    return 1 - (1 - r2) * (len(y) - 1) / (len(y) - n_cols - 1)

# Compare the adjusted r-squared of the two models
adj_score1 = r2_adj(X_full_test, y_test, lr1)
adj_score2 = r2_adj(X_sel_test, y_test, lr2)
print(f"1 Feature Adjusted R2: {adj_score1}")
print(f"2 Feature Adjusted R2: {adj_score2}")



import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# Import the data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_2/datasets/rent-data-cleaned.csv").dropna()
df.head()

# Get the features (everything except the "price" column)
X = df.copy().drop(columns="price")
X.head()

# Get the target column
y = df["price"].values.reshape(-1,1)
y[0:5]

# Use the Sklearn `train_test_split()` function to split the data into training and testing data
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

import statsmodels.api as sm

# Use the statsmodels package to create and fit a linear regression


# Create a variable to hold the p-values of all columns sorted in ascending order


# Use loc to filter to columns with p-values below 0.05

# Show the index of the results


# Create an X variable with all features and another with
# only features that meet the 0.05 threshold.

# Hint: Use the index from the previous cell



# Split the data into training and testing sets


# Train two models using the different X variables

# Create the models


# Fit the models


# Provided code to create the adjusted r-squared function
def r2_adj(x, y, model):
    r2 = model.score(x,y)
    n_cols = x.shape[1]
    return 1 - (1 - r2) * (len(y) - 1) / (len(y) - n_cols - 1)

# Compare the adjusted r-squared of the two models




import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# Import the data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/car-data-encoded.csv").dropna()
df.head()

# Get the features (everything except the "price" column)
X = df.copy().drop(columns="price")
X.head()

# Get the target column
y = df["price"].values.reshape(-1,1)
y[0:5]

# Use the Sklearn `train_test_split()` function to split the data into training and testing data
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Create a function to calculate VIF
from statsmodels.stats.outliers_influence import variance_inflation_factor

def calc_vif(X):
    vif = pd.DataFrame()
    vif["variables"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

    return(vif)

# Calculate vif for the dataframe

calc_vif(X).sort_values("VIF")

# Investigate the engine-location column to see why it returned a VIF of NaN
X['engine-location']

# Use value_counts to confirm
X['engine-location'].value_counts()

# Create another X variable by dropping engine-location 
# and the 4 columns with the highest VIF scores

X_vif = X.drop(columns=['engine-location', 'width', 'wheel-base', 'length', 'height'])

# Recalculate the VIF scores
calc_vif(X_vif).sort_values('VIF')

# Split the data into training and testing sets
X_full_train, X_full_test, X_vif_train, X_vif_test, y_train, y_test = train_test_split(X, X_vif, y, random_state=14)

# Train two models using the different X variables

# Create the models
lr1 = LinearRegression()
lr2 = LinearRegression()

# Fit the models
lr1.fit(X_full_train, y_train)
lr2.fit(X_vif_train, y_train)

# Provided code to create the adjusted r-squared function
def r2_adj(x, y, model):
    r2 = model.score(x,y)
    n_cols = x.shape[1]
    return 1 - (1 - r2) * (len(y) - 1) / (len(y) - n_cols - 1)

# Compare the adjusted r-squared of the two models
adj_score1 = r2_adj(X_full_test, y_test, lr1)
adj_score2 = r2_adj(X_vif_test, y_test, lr2)
print(f"1 Feature Adjusted R2: {adj_score1}")
print(f"2 Feature Adjusted R2: {adj_score2}")



import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# Import the data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/car-data-encoded.csv").dropna()
df.head()

# Get the features (everything except the "price" column)
X = df.copy().drop(columns="price")
X.head()

# Get the target column
y = df["price"].values.reshape(-1,1)
y[0:5]

# Use the Sklearn `train_test_split()` function to split the data into training and testing data
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Create a function to calculate VIF
from statsmodels.stats.outliers_influence import variance_inflation_factor

def calc_vif(X):
    vif = pd.DataFrame()
    vif["variables"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

    return(vif)

# Calculate vif for the dataframe



# Investigate the engine-location column to see why it returned a VIF of NaN


# Use value_counts to confirm


# Create another X variable by dropping engine-location 
# and the 4 columns with the highest VIF scores


# Recalculate the VIF scores


# Split the data into training and testing sets


# Train two models using the different X variables

# Create the models


# Fit the models


# Provided code to create the adjusted r-squared function
def r2_adj(x, y, model):
    r2 = model.score(x,y)
    n_cols = x.shape[1]
    return 1 - (1 - r2) * (len(y) - 1) / (len(y) - n_cols - 1)

# Compare the adjusted r-squared of the two models




from sklearn.datasets import make_regression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error

# Create some synthesized data
X, y = make_regression(n_samples = 5000, n_features=1, n_targets=1, noise=2, random_state=0)

# Split the data into training and testing sets.
X_train, X_test, y_train, y_test = train_test_split(X, y)

# Scale the training data
scaler = StandardScaler()
X_train_transformed = scaler.fit_transform(X_train)

# Create and train a Ridge model with an alpha of 1
model = Ridge(alpha=1)
model.fit(X_train_transformed, y_train)

# Scale the X testing data and use the model to make predictions
X_test_transformed = scaler.transform(X_test)
y_predicted = model.predict(X_test_transformed)

# Calculate the mean_squared_error
mean_squared_error(y_test, y_predicted)

# Create a RidgeCV model and train it with the scaled data
# Use values of 0.001, 0.01, 0.1, 1, and 10 for alpha
from sklearn.linear_model import RidgeCV
model_cv = RidgeCV(alphas=[0.001, 0.01, 0.1, 1, 10])
model_cv = model_cv.fit(X_train_transformed, y_train)

# Display the alpha of the best model
model_cv.alpha_

# Create a model using the best alpha
model2 = Ridge(alpha=0.01)

# Train the model
model2.fit(X_train_transformed, y_train)

# Create predictions and calculate the mean squared error
y_predicted2 = model2.predict(X_test_transformed)
mean_squared_error(y_test, y_predicted2)



from sklearn.datasets import make_regression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error

# Create some synthesized data
X, y = make_regression(n_samples = 5000, n_features=1, n_targets=1, noise=2, random_state=0)

# Split the data into training and testing sets.
X_train, X_test, y_train, y_test = train_test_split(X, y)

# Scale the training data
scaler = StandardScaler()
X_train_transformed = scaler.fit_transform(X_train)

# Create and train a Ridge model with an alpha of 1

# Scale the X testing data and use the model to make predictions


# Calculate the mean_squared_error


# Create a RidgeCV model and train it with the scaled data
# Use values of 0.001, 0.01, 0.1, 1, and 10 for alpha
from sklearn.linear_model import RidgeCV


# Display the alpha of the best model


# Create a model using the best alpha


# Train the model


# Create predictions and calculate the mean squared error




from sklearn.datasets import make_regression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error
import pandas as pd

df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_2/datasets/real-estate-evaluation.csv')
df.head()

# Separate the data into features and target 
X = df.drop('Y house price of unit area', axis=1)
y = df['Y house price of unit area']

# Check the features shape 
X.shape

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

# Scale the training data
scaler = StandardScaler()
X_train_transformed = scaler.fit_transform(X_train)

# Create and train the model
model = Ridge(alpha=1)
model.fit(X_train_transformed, y_train)

# Scale the testing data and create predictions
X_test_transformed = scaler.transform(X_test)
y_predicted = model.predict(X_test_transformed)

# Assess the MSE
mean_squared_error(y_test, y_predicted)

# Use RidgeCV to optimize for alpha
from sklearn.linear_model import RidgeCV
model_cv = RidgeCV(alphas=[0.001, 0.01, 0.1, 1, 10])
model_cv = model_cv.fit(X_train_transformed, y_train)

# Identify the optimzied alpha value
model_cv.alpha_

# Create and train a linear regression model, create predictions with the model, and evaluate its MSE
from sklearn.linear_model import LinearRegression
lr_model = LinearRegression()
lr_model.fit(X_train_transformed, y_train)
y_predicted_lr = lr_model.predict(X_test_transformed)
mean_squared_error(y_test, y_predicted_lr)

from sklearn.linear_model import Lasso

# Create and train a lasso regression model
lasso_model = Lasso(alpha=1)
lasso_model.fit(X_train_transformed, y_train)

# Get the model coeffcients
lasso_model.coef_

# Create predictions with the model
y_predicted_lasso = lasso_model.predict(X_test_transformed)

# Evaluate the MSE
mean_squared_error(y_test, y_predicted_lasso)



from sklearn.datasets import make_regression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error
import pandas as pd

df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_2/datasets/real-estate-evaluation.csv')
df.head()

# Separate the data into features and target 
X = df.drop('Y house price of unit area', axis=1)
y = df['Y house price of unit area']

# Check the features shape 
X.shape

# Split the data into training and testing sets


# Scale the training data


# Create and train the model


# Scale the testing data and create predictions


# Assess the MSE


# Use RidgeCV to optimize for alpha
from sklearn.linear_model import RidgeCV


# Identify the optimzied alpha value


# Create and train a linear regression model, create predictions with the model, and evaluate its MSE
from sklearn.linear_model import LinearRegression


from sklearn.linear_model import Lasso

# Create and train a lasso regression model


# Get the model coeffcients


# Create predictions with the model


# Evaluate the MSE




import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Import the data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_3/datasets/rent-data-label-encoded.csv")
df.head()

# Drop rows with missing values
df = df.dropna()

# Make an X variable with all columns except price
X_full = df.drop(columns = ['price'])
X_full.columns

select_features = ["square_feet", "Gated", "bathrooms", "bedrooms", "has_photo", "Pool", "AC"]

# Create another variable X_sel with only the columns
# in the "select_features" list

X_sel = df[select_features]
X_sel.head()

# Set the target variable y
y = df["price"].values.reshape(-1, 1)

# Now split the data into training and testing sets
X_full_train, X_full_test, X_sel_train, X_sel_test, y_train, y_test = train_test_split(X_full, X_sel, y, random_state=42)

# Create the models
lr1 = LinearRegression()
lr2 = LinearRegression()

# Fit the first model to the full training data. 
lr1.fit(X_full_train, y_train)

# Fit the second model to the select training data.
lr2.fit(X_sel_train, y_train)

# Calculate the mean_squared_error and the r-squared value
# for the testing data

# Use our models to make predictions
predicted1 = lr1.predict(X_full_test)
predicted2 = lr2.predict(X_sel_test)

# Score the predictions with mse and r2
mse1 = mean_squared_error(y_test, predicted1)
r21 = r2_score(y_test, predicted1)
mse2 = mean_squared_error(y_test, predicted2)
r22 = r2_score(y_test, predicted2)

print(f"All Features:")
print(f"mean squared error (MSE): {mse1}")
print(f"R-squared (R2): {r21}")
print("---------------------")
print(f"Select Features:")
print(f"mean squared error (MSE): {mse2}")
print(f"R-squared (R2): {r22}")

# Provided code to create the adjusted r-squared function
def r2_adj(x, y, model):
    r2 = model.score(x,y)
    n_cols = x.shape[1]
    return 1 - (1 - r2) * (len(y) - 1) / (len(y) - n_cols - 1)

# Calculate the adjusted r-squared value of the model
adj_score1 = r2_adj(X_full_test, y_test, lr1)
adj_score2 = r2_adj(X_sel_test, y_test, lr2)
print(f"All Features Adjusted R2: {adj_score1}")
print(f"Select Features Adjusted R2: {adj_score2}")

# Examine linear regression on the better training data using cross validation
from sklearn.model_selection import cross_val_score
cv_scores = cross_val_score(LinearRegression(), X_full_train, y_train, scoring = "r2")
print(f"All scores: {cv_scores}")
print(f"Mean score: {cv_scores.mean()}")
print(f"Standard Deviation: {cv_scores.std()}")



import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Import the data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_3/datasets/rent-data-label-encoded.csv")
df.head()

# Drop rows with missing values
df = df.dropna()

# Make an X variable with all columns except price
X_full = df.drop(columns = ['price'])
X_full.columns

select_features = ["square_feet", "Gated", "bathrooms", "bedrooms", "has_photo", "Pool", "AC"]

# Create another variable X_sel with only the columns
# in the "select_features" list



# Set the target variable y


# Now split the data into training and testing sets


# Create the models


# Fit the first model to the full training data. 


# Fit the second model to the select training data.


# Calculate the mean_squared_error and the r-squared value
# for the testing data

# Use our models to make predictions


# Score the predictions with mse and r2


print(f"All Features:")
print(f"mean squared error (MSE): {mse1}")
print(f"R-squared (R2): {r21}")
print("---------------------")
print(f"Select Features:")
print(f"mean squared error (MSE): {mse2}")
print(f"R-squared (R2): {r22}")

# Provided code to create the adjusted r-squared function
def r2_adj(x, y, model):
    r2 = model.score(x,y)
    n_cols = x.shape[1]
    return 1 - (1 - r2) * (len(y) - 1) / (len(y) - n_cols - 1)

# Calculate the adjusted r-squared value of the model


# Examine linear regression on the better training data using cross validation
from sklearn.model_selection import cross_val_score




import pandas as pd

# Import the data
rent_tx_df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_3/datasets/rent_data_tx.csv")
rent_ca_df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_3/datasets/rent_data_ca.csv")

# Look at the first few rows of the california data
rent_ca_df.head()

# Import the pipeline_utilities module
import pipeline_utilities as p_utils


# Train a model for Texas using the rent_model_generator function
tx_model = p_utils.rent_model_generator(rent_tx_df)

# Train a model for California using the rent_model_generator function
ca_model = p_utils.rent_model_generator(rent_ca_df)



import pandas as pd

# Import the data
rent_tx_df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_3/datasets/rent_data_tx.csv")
rent_ca_df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_3/datasets/rent_data_ca.csv")

# Look at the first few rows of the california data
rent_ca_df.head()

# Import the pipeline_utilities module


# Train a model for Texas using the rent_model_generator function


# Train a model for California using the rent_model_generator function




import pandas as pd
import pipeline_utilities as p_util

# Create a list of DataFrames by using a list comprehension with read_csv
days = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Saturday']
dfs = [pd.read_csv(f'https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_3/datasets/garments-worker-productivity-{day}.csv') for day in days]
dfs[-1].head()

# Create a dictionary with the models
model_dict = {}
for df in dfs:
    print(df.iloc[0,3])
    model_dict[f"{df.iloc[0,3]}_model"] = p_util.prod_model_generator(df)
    print("----------")

model_dict




import pandas as pd

# Import the data
startup_path = "https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_1/datasets/start-up-success.csv"
df = pd.read_csv(startup_path)
df.head()

# Plot the data on a scatter plot
df.plot.scatter(
    x='Industry Health', 
    y='Financial Performance', 
    c='Firm Category', 
    marker='o', 
    s=25, 
    edgecolor='k',
    colormap="winter"
)

# Preview the DataFrame
df.head(3)

# Check the number of unhealthy vs. healthy firms ('Firm Category')
# using value_counts
df['Firm Category'].value_counts()

# Import Module
from sklearn.model_selection import train_test_split

# Split training and testing sets
# Create the features DataFrame, X
X = df.copy()
X = X.drop(columns='Firm Category')

# Create the target DataFrame, y
y = df['Firm Category']

# Use train_test_split to separate the data
X_train, X_test, y_train, y_test = train_test_split(X, y)

# Review the X_train DataFrame
X_train

# Import `LogisticRegression` from sklearn
from sklearn.linear_model import LogisticRegression


# Create a `LogisticRegression` function and assign it 
# to a variable named `logistic_regression_model`.
logistic_regression_model = LogisticRegression()


# Fit the model
logistic_regression_model.fit(X_train, y_train)

# Score the model
print(f"Training Data Score: {logistic_regression_model.score(X_train, y_train)}")
print(f"Testing Data Score: {logistic_regression_model.score(X_test, y_test)}")

# Generate predictions from the model we just fit
predictions = logistic_regression_model.predict(X_train)

# Convert those predictions (and actual values) to a DataFrame
results_df = pd.DataFrame({"Prediction": predictions, "Actual": y_train})
results_df

# Apply the fitted model to the `test` dataset
testing_predictions = logistic_regression_model.predict(X_test)

# Save both the test predictions and actual test values to a DataFrame
results_df = pd.DataFrame({
    "Testing Data Predictions": testing_predictions, 
    "Testing Data Actual Targets": y_test})

# Display the results DataFrame
results_df

# Import the accuracy_score function
from sklearn.metrics import accuracy_score

# Calculate the model's accuracy on the test dataset
accuracy_score(y_test, testing_predictions)




import pandas as pd

# Import the data
startup_path = "https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_1/datasets/start-up-success.csv"
df = pd.read_csv(startup_path)
df.head()

# Plot the data on a scatter plot


# Preview the DataFrame


# Check the number of unhealthy vs. healthy firms ('Firm Category')
# using value_counts


# Import Module
from sklearn.model_selection import train_test_split

# Split training and testing sets
# Create the features DataFrame, X


# Create the target DataFrame, y


# Use train_test_split to separate the data


# Review the X_train DataFrame


# Import `LogisticRegression` from sklearn
from sklearn.linear_model import LogisticRegression


# Create a `LogisticRegression` function and assign it 
# to a variable named `logistic_regression_model`.


# Fit the model


# Score the model


# Generate predictions from the model we just fit


# Convert those predictions (and actual values) to a DataFrame


# Apply the fitted model to the `test` dataset


# Save both the test predictions and actual test values to a DataFrame


# Display the results DataFrame


# Import the accuracy_score function
from sklearn.metrics import accuracy_score

# Calculate the model's accuracy on the test dataset
accuracy_score(y_test, testing_predictions)




# Import the required modules
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression

# Read in the app-data.csv file into a Pandas DataFrame.
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_1/datasets/app-data.csv"
app_data = pd.read_csv(file_path)

# Review the DataFrame
app_data.head()

# The column 'Result' is the thing you want to predict. 
# Class 0 indicates a benign app and class 1 indicates a malware app
# Using value_counts, how many malware apps are in this dataset?
app_data["Result"].value_counts()

# The target column `y` should be the binary `Result` column.
y = app_data["Result"]

# The `X` should be all of the features. 
X = app_data.copy()
X = X.drop(columns="Result")

# Split the dataset using the train_test_split function
X_train, X_test, y_train, y_test = train_test_split(X, y)

# Declare a logistic regression model.
# Apply a random_state of 7 and max_iter of 120 to the model
logistic_regression_model = LogisticRegression(random_state=7, max_iter=120)

# Fit and save the logistic regression model using the training data
lr_model = logistic_regression_model.fit(X_train, y_train)

# Validate the model
print(f"Training Data Score: {lr_model.score(X_train, y_train)}")
print(f"Testing Data Score: {lr_model.score(X_test, y_test)}")

# Make and save testing predictions with the saved logistic regression model using the test data
testing_predections = lr_model.predict(X_test)

# Review the predictions
testing_predections

# Display the accuracy score for the test dataset.
accuracy_score(y_test, testing_predections)



# Import the required modules
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression

# Read in the app-data.csv file into a Pandas DataFrame.
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_1/datasets/app-data.csv"
app_data = pd.read_csv(file_path)

# Review the DataFrame
app_data.head()

# The column 'Result' is the thing you want to predict. 
# Class 0 indicates a benign app and class 1 indicates a malware app
# Using value_counts, how many malware apps are in this dataset?


# The target column `y` should be the binary `Result` column.


# The `X` should be all of the features. 


# Split the dataset using the train_test_split function


# Declare a logistic regression model.
# Apply a random_state of 7 and max_iter of 120 to the model


# Fit and save the logistic regression model using the training data


# Validate the model


# Make and save testing predictions with the saved logistic regression model using the test data


# Review the predictions


# Display the accuracy score for the test dataset.




import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Load in data
df_crowdfunding = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_1/datasets/crowdfunding-data.csv')
df_crowdfunding

# Define features set
# Drop the target to create the X data
X = df_crowdfunding.copy()
X.drop("outcome", axis=1, inplace=True)
X.head()

# Define target vector
y = df_crowdfunding["outcome"].values.reshape(-1, 1)
y[:5]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)

# Scaling the X data by using StandardScaler()
scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_train_scaled

# Transforming the test dataset based on the fit from the training dataset
X_test_scaled = scaler.transform(X_test)
X_test_scaled

# Check the max and min of the scaled training and testing sets
print("Scaled data min/max (StandardScaler):")
print("Training data min:",X_train_scaled.min())
print("Training data max:",X_train_scaled.max())
print("Testing data min:",X_test_scaled.min())
print("Testing data max:",X_test_scaled.max())

# Alternatively, scaling the data by using MinMaxScaler()
scaler = MinMaxScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_train_scaled

# Transforming the test dataset based on the fit from the training dataset
X_test_scaled = scaler.transform(X_test)
X_test_scaled

# Check the max and min of the scaled training and testing sets
print("Scaled data min/max (MinMaxScaler):")
print("Training data min:",X_train_scaled.min())
print("Training data max:",X_train_scaled.max())
print("Testing data min:",X_test_scaled.min())
print("Testing data max:",X_test_scaled.max())



import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Load in data
df_crowdfunding = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_1/datasets/crowdfunding-data.csv')
df_crowdfunding

# Define features set
# Drop the target to create the X data
X = df_crowdfunding.copy()
X.drop("outcome", axis=1, inplace=True)
X.head()

# Define target vector
y = df_crowdfunding["outcome"].values.reshape(-1, 1)
y[:5]

# Split the data into training and testing sets


# Scaling the X data by using StandardScaler()


# Transforming the test dataset based on the fit from the training dataset


# Check the max and min of the scaled training and testing sets
print("Scaled data min/max (StandardScaler):")
print("Training data min:",X_train_scaled.min())
print("Training data max:",X_train_scaled.max())
print("Testing data min:",X_test_scaled.min())
print("Testing data max:",X_test_scaled.max())

# Alternatively, scaling the data by using MinMaxScaler()


# Transforming the test dataset based on the fit from the training dataset


# Check the max and min of the scaled training and testing sets
print("Scaled data min/max (MinMaxScaler):")
print("Training data min:",X_train_scaled.min())
print("Training data max:",X_train_scaled.max())
print("Testing data min:",X_test_scaled.min())
print("Testing data max:",X_test_scaled.max())



import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.linear_model import LogisticRegression

# Load in data
df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_1/datasets/customer-churn.csv')
df

# Drop the label to create the X data
X = df.drop('Churn', axis=1)
X

# Create the y set from the "Churn" column
y = df["Churn"]
y

# Split the data into training and testing sets using random_state=1
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

# Scale the X data by using StandardScaler()
scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_train_scaled

# Transform the test dataset based on the fit from the training dataset
X_test_scaled = scaler.transform(X_test)
X_test_scaled

# Create a `LogisticRegression` function and assign it 
# to a variable named `logistic_regression_model`.
logistic_regression_model = LogisticRegression()

# Fit the model
logistic_regression_model.fit(X_train_scaled, y_train)

# Score the model
print(f"Training Data Score: {logistic_regression_model.score(X_train_scaled, y_train)}")
print(f"Testing Data Score: {logistic_regression_model.score(X_test_scaled, y_test)}")

# Alternatively, scale the data by using MinMaxScaler()
scaler = MinMaxScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_train_scaled

# Transform the test dataset based on the fit from the training dataset
X_test_scaled = scaler.transform(X_test)
X_test_scaled

# Create a `LogisticRegression` function and assign it 
# to a variable named `logistic_regression_model`.
logistic_regression_model = LogisticRegression()

# Fit the model
logistic_regression_model.fit(X_train_scaled, y_train)

# Score the model
print(f"Training Data Score: {logistic_regression_model.score(X_train_scaled, y_train)}")
print(f"Testing Data Score: {logistic_regression_model.score(X_test_scaled, y_test)}")



import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.linear_model import LogisticRegression

# Load in data
df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_1/datasets/customer-churn.csv')
df

# Drop the label to create the X data


# Create the y set from the "Churn" column


# Split the data into training and testing sets using random_state=1


# Scale the X data by using StandardScaler()


# Transform the test dataset based on the fit from the training dataset


# Create a `LogisticRegression` function and assign it 
# to a variable named `logistic_regression_model`.


# Fit the model


# Score the model


# Alternatively, scale the data by using MinMaxScaler()


# Transform the test dataset based on the fit from the training dataset


# Create a `LogisticRegression` function and assign it 
# to a variable named `logistic_regression_model`.


# Fit the model


# Score the model




# Import required dependencies
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC 

# Import data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_1/datasets/occupancy.csv"
df = pd.read_csv(file_path)
df.head()

# Get the target variable (the "Occupancy" column)
y = df["Occupancy"]

# Get the features (everything except the "Occupancy" column)
X = df.copy()
X = X.drop(columns="Occupancy")
X.head()

# Split data into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Create the support vector machine classifier model with a 'linear' kernel
model = SVC(kernel='linear')

# Fit the model to the training data
model.fit(X_train, y_train)

# Validate the model by checking the model accuracy with model.score
print('Train Accuracy: %.3f' % model.score(X_train, y_train))
print('Test Accuracy: %.3f' % model.score(X_test, y_test))

# Make and save testing predictions with the saved SVM model using the testing data
testing_predictions = model.predict(X_test)

# Review the predictions
testing_predictions

# Display the accuracy score for the testing dataset
accuracy_score(y_test, testing_predictions)



# Import required dependencies
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC 

# Import data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_1/datasets/occupancy.csv"
df = pd.read_csv(file_path)
df.head()

# Get the target variable (the "Occupancy" column)


# Get the features (everything except the "Occupancy" column)


# Split data into training and testing sets


# Create the support vector machine classifier model with a 'linear' kernel


# Fit the model to the training data


# Validate the model by checking the model accuracy with model.score


# Make and save testing predictions with the saved SVM model using the testing data


# Review the predictions


# Display the accuracy score for the testing dataset




# Import required dependencies
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC 

# Import data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_1/datasets/app-data.csv"
df = pd.read_csv(file_path)
df.head()

# Get the target variable (the "Result" column)
y = df["Result"]

# Get the features (everything except the "Result" column)
X = df.copy()
X = X.drop(columns="Result")
X.head()

# Split data into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Create the support vector machine classifier model with a 'linear' kernel
model = SVC(kernel='linear')

# Fit the model to the training data
model.fit(X_train, y_train)

# Validate the model by checking the model accuracy with model.score
print('Train Accuracy: %.3f' % model.score(X_train, y_train))
print('Test Accuracy: %.3f' % model.score(X_test, y_test))

# Make and save testing predictions with the saved SVM model using the testing data
testing_predictions = model.predict(X_test)

# Review the predictions
testing_predictions

# Display the accuracy score for the testing dataset
accuracy_score(y_test, testing_predictions)



# Import required dependencies
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC 

# Import data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_1/datasets/app-data.csv"
df = pd.read_csv(file_path)
df.head()

# Get the target variable (the "Result" column)
y = df["Result"]

# Get the features (everything except the "Result" column)
X = df.copy()
X = X.drop(columns="Result")
X.head()

# Split data into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Create the support vector machine classifier model with a 'linear' kernel


# Fit the model to the training data


# Validate the model by checking the model accuracy with model.score


# Make and save testing predictions with the saved SVM model using the testing data


# Review the predictions


# Display the accuracy score for the testing dataset




import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load data
glass_dataset = "https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_2/datasets/glass.csv"
df = pd.read_csv(glass_dataset)
df.head()

# Define features set
X = df.drop("glass", axis=1)
X.head()

# Define target vector
y = df["glass"].values

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

# Create a StandardScater model and fit it to the training data
X_scaler = StandardScaler()
X_scaler.fit(X_train)

# Transform the training and testing data by using the X_scaler model
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

# Loop through different k values to find which has the highest accuracy.
# Note: We use only odd numbers because we don't want any ties.
train_scores = []
test_scores = []
for k in range(1, 20, 2):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train_scaled, y_train)
    train_score = knn.score(X_train_scaled, y_train)
    test_score = knn.score(X_test_scaled, y_test)
    train_scores.append(train_score)
    test_scores.append(test_score)
    print(f"k: {k}, Train/Test Score: {train_score:.3f}/{test_score:.3f}")
    
# Plot the results
plt.plot(range(1, 20, 2), train_scores, marker='o', label="training scores")
plt.plot(range(1, 20, 2), test_scores, marker="x", label="testing scores")
plt.xlabel("k neighbors")
plt.ylabel("accuracy score")
plt.legend()
plt.show()

# Train the KNN model with the best k value
# Note that k: 9 seems to be the best choice for this dataset
knn = KNeighborsClassifier(n_neighbors=9)
knn.fit(X_train_scaled, y_train)
print('k=9 Test Acc: %.3f' % knn.score(X_test_scaled, y_test))

import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load data
glass_dataset = "https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_2/datasets/glass.csv"
df = pd.read_csv(glass_dataset)
df.head()

# Define features set
X = df.drop("glass", axis=1)
X.head()

# Define target vector
y = df["glass"].values

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

# Create a StandardScater model and fit it to the training data
X_scaler = StandardScaler()
X_scaler.fit(X_train)

# Transform the training and testing data by using the X_scaler model
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

# Loop through different k values to find which has the highest accuracy.
# Note: We use only odd numbers because we don't want any ties.
train_scores = []
test_scores = []

    
# Plot the results


# Train the KNN model with the best k value
# Note that k: 9 seems to be the best choice for this dataset


import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler

# Load data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_2/datasets/app-data.csv"
app_data = pd.read_csv(file_path)
app_data.head()

# Define features set
X = app_data.copy()
X.drop("Result", axis=1, inplace=True)
X.head()

# Define target vector
y = app_data["Result"]
y.head()

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3)

# Create a StandardScaler() model and fit it to the training data
X_scaler = StandardScaler().fit(X_train)

# Transform the training and testing data by using the X_scaler model
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

# Loop through different k values to find which has the highest accuracy.
# Note: We use only odd numbers because we don't want any ties.
train_scores = []
test_scores = []
for k in range(1, 20, 2):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train_scaled, y_train)
    train_score = knn.score(X_train_scaled, y_train)
    test_score = knn.score(X_test_scaled, y_test)
    train_scores.append(train_score)
    test_scores.append(test_score)
    print(f"k: {k}, Train/Test Score: {train_score:.3f}/{test_score:.3f}")
    
# Plot the results
plt.plot(range(1, 20, 2), train_scores, marker='o', label="training scores")
plt.plot(range(1, 20, 2), test_scores, marker="x", label="testing scores")
plt.xlabel("k neighbors")
plt.ylabel("accuracy score")
plt.legend()
plt.show()

# Choose the best k, and refit the KNN classifier by using that k value.
# Note that k: 9 provides the best accuracy where the classifier starts to stablize
knn = KNeighborsClassifier(n_neighbors=9)
knn.fit(X_train_scaled, y_train)

# Print the score for the test data.
print('k=9 Test Acc: %.3f' % knn.score(X_test_scaled, y_test))



import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler

# Load data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_2/datasets/app-data.csv"
app_data = pd.read_csv(file_path)
app_data.head()

# Define features set
X = app_data.copy()
X.drop("Result", axis=1, inplace=True)
X.head()

# Define target vector
y = app_data["Result"]
y.head()

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3)

# Create a StandardScaler() model and fit it to the training data
X_scaler = StandardScaler().fit(X_train)

# Transform the training and testing data by using the X_scaler model
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

# Loop through different k values to find which has the highest accuracy.
# Note: We use only odd numbers because we don't want any ties.


    
# Plot the results
plt.plot(range(1, 20, 2), train_scores, marker='o', label="training scores")
plt.plot(range(1, 20, 2), test_scores, marker="x", label="testing scores")
plt.xlabel("k neighbors")
plt.ylabel("accuracy score")
plt.legend()
plt.show()

# Choose the best k, and refit the KNN classifier by using that k value.
# Note that k: 9 provides the best accuracy where the classifier starts to stablize


# Print the score for the test data.




# Initial imports
import pandas as pd
from sklearn import tree
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Needed for decision tree visualization
import pydotplus
from IPython.display import Image

# Load data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_2/datasets/crowdfunding-data.csv"
df_crowdfunding = pd.read_csv(file_path)
df_crowdfunding.head()

# Define features set
X = df_crowdfunding.copy()
X.drop("outcome", axis=1, inplace=True)
X.head()

# Define target vector
y = df_crowdfunding["outcome"].values.reshape(-1, 1)
y[:5]

# Splitting into Train and Test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)

# Create the StandardScaler instance
scaler = StandardScaler()

# Fit the StandardScaler with the training data
X_scaler = scaler.fit(X_train)

# Scale the training data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

# Create the decision tree classifier instance
model = tree.DecisionTreeClassifier()

# Fit the model
model = model.fit(X_train_scaled, y_train)

# Making predictions using the testing data
predictions = model.predict(X_test_scaled)

# Calculate the accuracy score
acc_score = accuracy_score(y_test, predictions)

print(f"Accuracy Score : {acc_score}")

# Create DOT data
dot_data = tree.export_graphviz(
    model, out_file=None, feature_names=X.columns, class_names=["0", "1"], filled=True
)

# Draw graph
graph = pydotplus.graph_from_dot_data(dot_data)

# Show graph
Image(graph.create_png())



# When saving the image, graph.write_<file_type>() must take a string object

# Save the tree as PDF
file_path = "crowdfunding_tree.pdf"
graph.write_pdf(file_path)

# Save the tree as PNG
file_path = "crowdfunding_tree.png"
graph.write_png(file_path)




# Initial imports
import pandas as pd
from sklearn import tree
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Needed for decision tree visualization
import pydotplus
from IPython.display import Image

# Load data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_2/datasets/crowdfunding-data.csv"
df_crowdfunding = pd.read_csv(file_path)
df_crowdfunding.head()

# Define features set


# Define target vector


# Splitting into Train and Test sets


# Create the StandardScaler instance


# Fit the StandardScaler with the training data


# Scale the training data


# Create the decision tree classifier instance


# Fit the model


# Making predictions using the testing data


# Calculate the accuracy score


# Create DOT data


# Draw graph


# Show graph


# When saving the image, graph.write_<file_type>() must take a string object

# Save the tree as PDF


# Save the tree as PNG




# Initial imports
import pandas as pd
from sklearn import tree
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Needed for decision tree visualization
import pydotplus
from IPython.display import Image

# Load data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_2/datasets/app-data.csv"
app_data = pd.read_csv(file_path)
app_data.head()

# Define features set
X = app_data.copy()
X.drop("Result", axis=1, inplace=True)
X.head()

# Define target vector
y = app_data["Result"].values.reshape(-1, 1)
y[:5]

# Splitting into Train and Test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)

# Create the StandardScaler instance
scaler = StandardScaler()

# Fit the StandardScaler with the training data
X_scaler = scaler.fit(X_train)

# Scale the training data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

# Create the decision tree classifier instance
model = tree.DecisionTreeClassifier()

# Fit the model
model = model.fit(X_train_scaled, y_train)

# Making predictions using the testing data
predictions = model.predict(X_test_scaled)

# Calculate the accuracy score
acc_score = accuracy_score(y_test, predictions)

print(f"Accuracy Score : {acc_score}")

# Create DOT data
dot_data = tree.export_graphviz(
    model, out_file=None, feature_names=X.columns, class_names=["0", "1"], filled=True, max_depth=5
)

# Draw graph
graph = pydotplus.graph_from_dot_data(dot_data)

# Show graph
Image(graph.create_png())



# When saving the image, graph.write_<file_type>() must take a string object

# Save the tree as PDF
file_path = "malware_tree.pdf"
graph.write_pdf(file_path)

# Save the tree as PNG
file_path = "malware_tree.png"
graph.write_png(file_path)




# Initial imports
import pandas as pd
from sklearn import tree
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Needed for decision tree visualization
import pydotplus
from IPython.display import Image

# Load data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_2/datasets/app-data.csv"
app_data = pd.read_csv(file_path)
app_data.head()

# Define features set
X = app_data.copy()
X.drop("Result", axis=1, inplace=True)
X.head()

# Define target vector
y = app_data["Result"].values.reshape(-1, 1)
y[:5]

# Splitting into Train and Test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)

# Create the StandardScaler instance
scaler = StandardScaler()

# Fit the StandardScaler with the training data
X_scaler = scaler.fit(X_train)

# Scale the training data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

# Create the decision tree classifier instance


# Fit the model


# Making predictions using the testing data


# Calculate the accuracy score


# Create DOT data


# Draw graph


# Show graph


# When saving the image, graph.write_<file_type>() must take a string object

# Save the tree as PDF


# Save the tree as PNG




%matplotlib inline
from matplotlib import pyplot as plt
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Import data
df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_2/datasets/covtype.csv')
df

# Split features and target
X = df.drop('cover', axis=1)
y = df['cover']

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

# Scale the data
scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train the Random Forest model
clf = RandomForestClassifier(random_state=1, n_estimators=500).fit(X_train_scaled, y_train)

# Evaluate the model
print(f'Training Score: {clf.score(X_train_scaled, y_train)}')
print(f'Testing Score: {clf.score(X_test_scaled, y_test)}')

# Get the feature importance array
feature_importances = clf.feature_importances_

# List the top 10 most important features
importances_sorted = sorted(zip(feature_importances, X.columns), reverse=True)
importances_sorted[:10]

# Plot the feature importances
features = sorted(zip(X.columns, feature_importances), key = lambda x: x[1])
cols = [f[0] for f in features]
width = [f[1] for f in features]

fig, ax = plt.subplots()

fig.set_size_inches(8,6)
plt.margins(y=0.001)

ax.barh(y=cols, width=width)

plt.show()



%matplotlib inline
from matplotlib import pyplot as plt
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Import data
df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_2/datasets/covtype.csv')
df

# Split features and target
X = df.drop('cover', axis=1)
y = df['cover']

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

# Scale the data
scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train the Random Forest model


# Evaluate the model


# Get the feature importance array


# List the top 10 most important features


# Plot the feature importances




# Initial imports
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_2/datasets/app-data.csv"
df_apps = pd.read_csv(file_path)
df_apps.head()

# Define features set
X = df_apps.copy()
X.drop("Result", axis=1, inplace=True)
X.head()

# Define target set
y = df_apps["Result"].ravel()
y[:5]

# Split into Train and Test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)

# Create the random forest classifier instance
rf_model = RandomForestClassifier(n_estimators=128, random_state=78)

# Fit the model
rf_model = rf_model.fit(X_train, y_train)

# Make predictions using the testing data
predictions = rf_model.predict(X_test)

# Calculate the accuracy score
acc_score = accuracy_score(y_test, predictions)

# Display results
print(f"Accuracy Score : {acc_score}")

# Get the feature importance array
importances = rf_model.feature_importances_
# List the top 10 most important features
importances_sorted = sorted(zip(rf_model.feature_importances_, X.columns), reverse=True)
importances_sorted[:10]



# Initial imports
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_2/datasets/app-data.csv"
df_apps = pd.read_csv(file_path)
df_apps.head()

# Define features set
X = df_apps.copy()
X.drop("Result", axis=1, inplace=True)
X.head()

# Define target vector
y = df_apps["Result"].ravel()
y[:5]

# Split into Train and Test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)

# Create the random forest classifier instance


# Fit the model


# Make predictions using the testing data


# Calculate the accuracy score


# Display results


# Get the feature importance array

# List the top 10 most important features




import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Read the forest cover dataset
df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_2/datasets/covtype.csv')

# Split the features and target
X = df.drop('cover', axis=1)
y = df['cover']
target_names = ["Spruce/Fir", "Lodgepole Pine"]

# Prepare the data
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)
scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Import an Extremely Random Trees classifier
from sklearn.ensemble import ExtraTreesClassifier

# Train the ExtraTreesClassifier model
clf = ExtraTreesClassifier(random_state=1).fit(X_train_scaled, y_train)

# Evaluate the model
print(f'Training Score: {clf.score(X_train_scaled, y_train)}')
print(f'Testing Score: {clf.score(X_test_scaled, y_test)}')

# Import Gradient Boosting classifier
from sklearn.ensemble import GradientBoostingClassifier

# Train the Gradient Boosting classifier
clf = GradientBoostingClassifier(random_state=1).fit(X_train_scaled, y_train)

# Evaluate the model
print(f'Training Score: {clf.score(X_train_scaled, y_train)}')
print(f'Testing Score: {clf.score(X_test_scaled, y_test)}')

# Import an Adaptive Boosting classifier
from sklearn.ensemble import AdaBoostClassifier

# Train the AdaBoostClassifier
clf = AdaBoostClassifier(random_state=1).fit(X_train_scaled, y_train)

# Evaluate the model
print(f'Training Score: {clf.score(X_train_scaled, y_train)}')
print(f'Testing Score: {clf.score(X_test_scaled, y_test)}')

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Read the forest cover dataset
df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_2/datasets/covtype.csv')

# Split the features and target
X = df.drop('cover', axis=1)
y = df['cover']
target_names = ["Spruce/Fir", "Lodgepole Pine"]

# Prepare the data
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)
scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Import an Extremely Random Trees classifier
from sklearn.ensemble import ExtraTreesClassifier

# Train the ExtraTreesClassifier model


# Evaluate the model


# Import Gradient Boosting classifier
from sklearn.ensemble import GradientBoostingClassifier

# Train the Gradient Boosting classifier


# Evaluate the model


# Import an Adaptive Boosting classifier
from sklearn.ensemble import AdaBoostClassifier

# Train the AdaBoostClassifier


# Evaluate the model


# Import required dependencies
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Import data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_3/datasets/tic-tac-toe.csv"
df = pd.read_csv(file_path)
df.head()

# Check the data types
df.dtypes

# Get the target variable (the "Class" column)
# Since the target column is an object, we need to convert the data to numerical classes
# Use the LabelEncoder

# Create an instance of the label encoder
le = LabelEncoder()

y = le.fit_transform(df["Class"])

y

# Get the features (everything except the "Class" column)
X = df.copy()
X = X.drop(columns="Class")
X.head()

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Remember that all of the columns in the DataFrame are objects
# Use a OneHotEncoder to convert the training and testing data to numerical values
ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False, dtype='int')
X_train_encoded = pd.DataFrame(data=ohe.fit_transform(X_train), columns=ohe.get_feature_names_out())
X_test_encoded = pd.DataFrame(data=ohe.transform(X_test), columns=ohe.get_feature_names_out())
X_train_encoded

# Create the logistic regression classifier model with a random_state of 1
lr_model = LogisticRegression(random_state=1)

# Fit the model to the training data
lr_model.fit(X_train_encoded, y_train)

# Validate the model by checking the model accuracy with model.score
print('Train Accuracy: %.3f' % lr_model.score(X_train_encoded, y_train))
print('Test Accuracy: %.3f' % lr_model.score(X_test_encoded, y_test))

# Create the support vector machine classifier model with a 'linear' kernel
svm_model = SVC(kernel='linear')

# Fit the model to the training data
svm_model.fit(X_train_encoded, y_train)

# Validate the model by checking the model accuracy with model.score
print('Train Accuracy: %.3f' % svm_model.score(X_train_encoded, y_train))
print('Test Accuracy: %.3f' % svm_model.score(X_test_encoded, y_test))

# Create the KNN model with 5 neighbors
knn_model = KNeighborsClassifier(n_neighbors=5)

# Fit the model to the training data
knn_model.fit(X_train_encoded, y_train)

# Validate the model by checking the model accuracy with model.score
print('Train Accuracy: %.3f' % knn_model.score(X_train_encoded, y_train))
print('Test Accuracy: %.3f' % knn_model.score(X_test_encoded, y_test))

# Create the decision tree classifier model
dt_model = DecisionTreeClassifier()

# Fit the model to the training data
dt_model.fit(X_train_encoded, y_train)

# Validate the model by checking the model accuracy with model.score
print('Train Accuracy: %.3f' % dt_model.score(X_train_encoded, y_train))
print('Test Accuracy: %.3f' % dt_model.score(X_test_encoded, y_test))

# Create the random forest classifier model
# with n_estimators=128 and random_state=1
rf_model = RandomForestClassifier(n_estimators=128, random_state=1)

# Fit the model to the training data
rf_model.fit(X_train_encoded, y_train)

# Validate the model by checking the model accuracy with model.score
print('Train Accuracy: %.3f' % rf_model.score(X_train_encoded, y_train))
print('Test Accuracy: %.3f' % rf_model.score(X_test_encoded, y_test))



# Import required dependencies
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Import data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_3/datasets/tic-tac-toe.csv"
df = pd.read_csv(file_path)
df.head()

# Check the data types


# Get the target variable (the "Class" column)
# Since the target column is an object, we need to convert the data to numerical classes
# Use the LabelEncoder

# Create an instance of the label encoder



# Get the features (everything except the "Class" column)


# Split data into training and testing sets


# Remember that all of the columns in the DataFrame are objects
# Use a OneHotEncoder to convert the training and testing data to numerical values


# Create the logistic regression classifier model with a random_state of 1


# Fit the model to the training data


# Validate the model by checking the model accuracy with model.score


# Create the support vector machine classifier model with a 'linear' kernel


# Fit the model to the training data


# Validate the model by checking the model accuracy with model.score


# Create the KNN model with 5 neighbors


# Fit the model to the training data


# Validate the model by checking the model accuracy with model.score


# Create the decision tree classifier model


# Fit the model to the training data


# Validate the model by checking the model accuracy with model.score


# Create the random forest classifier model
# with n_estimators=128 and random_state=1
rf_model = RandomForestClassifier(n_estimators=128, random_state=1)

# Fit the model to the training data
rf_model.fit(X_train_encoded, y_train)

# Validate the model by checking the model accuracy with model.score




from sklearn import tree
from sklearn.datasets import load_iris

import pydotplus
from IPython.display import Image

# Load the Iris dataset
iris = load_iris()

# Create and score a decision tree classifier
clf = tree.DecisionTreeClassifier()
clf = clf.fit(iris.data, iris.target)
clf.score(iris.data, iris.target)

# Create a decision tree graph
dot_data = tree.export_graphviz(
    clf, out_file=None, 
    feature_names=iris.feature_names,  
    class_names=iris.target_names,  
    filled=True, rounded=True,  
    special_characters=True)  

graph = pydotplus.graph_from_dot_data(dot_data)
graph.write_png('iris.png')

# Show graph
Image(graph.create_png())



# Import required dependencies
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Import data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_3/datasets/car.csv"
df = pd.read_csv(file_path)
df.head()

# Check the value_counts of the target column
df["class"].value_counts()

# Check the data types
df.dtypes

# Get the target variable (the "class" column)
y = df["class"]
y

# Get the features (everything except the "class" column)
X = df.copy()
X = X.drop(columns="class")
X.head()

# Split data into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Since the target column is an object, we need to convert the data to numerical classes
# Encode the y data
# Create an instance of the label encoder
le = LabelEncoder()

# Fit and transform the y training and testing data using the label encoder
y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.transform(y_test)
y_train_encoded

# Remember that all of the columns in the DataFrame are objects
# Use a OneHotEncoder to convert the training data to numerical values
ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False, dtype='int')
X_train_encoded = pd.DataFrame(data=ohe.fit_transform(X_train), columns=ohe.get_feature_names_out())
X_train_encoded

# Encode the test data
X_test_encoded = pd.DataFrame(data=ohe.transform(X_test), columns=ohe.get_feature_names_out())
X_test_encoded

# Create the logistic regression classifier model with a random_state of 1
lr_model = LogisticRegression(random_state=1)

# Fit the model to the training data
lr_model.fit(X_train_encoded, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print('Train Accuracy: %.3f' % lr_model.score(X_train_encoded, y_train_encoded))
print('Test Accuracy: %.3f' % lr_model.score(X_test_encoded, y_test_encoded))

# Create the support vector machine classifier model with a 'poly' kernel
svm_model = SVC(kernel='poly')

# Fit the model to the training data
svm_model.fit(X_train_encoded, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print('Train Accuracy: %.3f' % svm_model.score(X_train_encoded, y_train_encoded))
print('Test Accuracy: %.3f' % svm_model.score(X_test_encoded, y_test_encoded))

# Create the KNN model with 9 neighbors
knn_model = KNeighborsClassifier(n_neighbors=9)

# Fit the model to the training data
knn_model.fit(X_train_encoded, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print('Train Accuracy: %.3f' % knn_model.score(X_train_encoded, y_train_encoded))
print('Test Accuracy: %.3f' % knn_model.score(X_test_encoded, y_test_encoded))

# Create the decision tree classifier model
dt_model = DecisionTreeClassifier()

# Fit the model to the training data
dt_model.fit(X_train_encoded, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print('Train Accuracy: %.3f' % dt_model.score(X_train_encoded, y_train_encoded))
print('Test Accuracy: %.3f' % dt_model.score(X_test_encoded, y_test_encoded))

# Create the random forest classifier model
# with n_estimators=128 and random_state=1
rf_model = RandomForestClassifier(n_estimators=128, random_state=1)

# Fit the model to the training data
rf_model.fit(X_train_encoded, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print('Train Accuracy: %.3f' % rf_model.score(X_train_encoded, y_train_encoded))
print('Test Accuracy: %.3f' % rf_model.score(X_test_encoded, y_test_encoded))



# Import required dependencies
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Import data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_3/datasets/car.csv"
df = pd.read_csv(file_path)
df.head()

# Check the value_counts of the target column
df["class"].value_counts()

# Check the data types
df.dtypes

# Get the target variable (the "class" column)
y = df["class"]
y

# Get the features (everything except the "class" column)
X = df.copy()
X = X.drop(columns="class")
X.head()

# Split data into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Since the target column is an object, we need to convert the data to numerical classes
# Encode the y data
# Create an instance of the label encoder


# Fit and transform the y training and testing data using the label encoder


# Remember that all of the columns in the DataFrame are objects
# Use a OneHotEncoder to convert the training data to numerical values


# Encode the test data


# Create the logistic regression classifier model with a random_state of 1


# Fit the model to the training data


# Validate the model by checking the model accuracy with model.score


# Create the support vector machine classifier model with a 'poly' kernel


# Fit the model to the training data


# Validate the model by checking the model accuracy with model.score


# Create the KNN model with 9 neighbors


# Fit the model to the training data


# Validate the model by checking the model accuracy with model.score


# Create the decision tree classifier model


# Fit the model to the training data


# Validate the model by checking the model accuracy with model.score


# Create the random forest classifier model
# with n_estimators=128 and random_state=1


# Fit the model to the training data


# Validate the model by checking the model accuracy with model.score




import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Load in data
df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_3/datasets/customer-churn.csv')
df

# Drop the label to create the X data
X = df.drop('Churn', axis=1)
X

# Create the y set from the "Churn" column
y = df["Churn"]
y

# Split the data into training and testing sets using random_state=1
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

# Scale the X data by using StandardScaler()
scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_train_scaled

# Transform the test dataset based on the fit from the training dataset
X_test_scaled = scaler.transform(X_test)
X_test_scaled

# Create the logistic regression classifier model with a random_state of 1
lr_model = LogisticRegression()

# Fit the model to the training data
lr_model.fit(X_train_scaled, y_train)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {lr_model.score(X_train_scaled, y_train)}")
print(f"Testing Data Score: {lr_model.score(X_test_scaled, y_test)}")

# Create the support vector machine classifier model with a 'rbf' kernel
svm_model = SVC(kernel='rbf')

# Fit the model to the training data
svm_model.fit(X_train_scaled, y_train)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {svm_model.score(X_train_scaled, y_train)}")
print(f"Testing Data Score: {svm_model.score(X_test_scaled, y_test)}")

# Create the KNN model with 9 neighbors
knn_model = KNeighborsClassifier(n_neighbors=9)

# Fit the model to the training data
knn_model.fit(X_train_scaled, y_train)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {knn_model.score(X_train_scaled, y_train)}")
print(f"Testing Data Score: {knn_model.score(X_test_scaled, y_test)}")

# Create the decision tree classifier model
dt_model = DecisionTreeClassifier()

# Fit the model to the training data
dt_model.fit(X_train_scaled, y_train)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {dt_model.score(X_train_scaled, y_train)}")
print(f"Testing Data Score: {dt_model.score(X_test_scaled, y_test)}")

# Create the random forest classifier model
# with n_estimators=128 and random_state=1
rf_model = RandomForestClassifier(n_estimators=128, random_state=1)

# Fit the model to the training data
rf_model.fit(X_train_scaled, y_train)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {rf_model.score(X_train_scaled, y_train)}")
print(f"Testing Data Score: {rf_model.score(X_test_scaled, y_test)}")



import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import AdaBoostClassifier

# Load in data
df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_3/datasets/letter-recognition.csv')
df

# Drop the label to create the X data
X = df.drop('lettr', axis=1)
X

# Create the y set from the "lettr" column
y = df["lettr"]
y

# Split the data into training and testing sets using random_state=1
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

# Encode the y data with the label encoder
# Create an instance of the label encoder
le = LabelEncoder()

# Fit and transform the y training and testing data using the label encoder
y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.transform(y_test)
y_train_encoded

# Scale the X data by using StandardScaler()
scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_train_scaled

# Transform the test dataset based on the fit from the training dataset
X_test_scaled = scaler.transform(X_test)
X_test_scaled

# Create the logistic regression classifier model with a random_state of 1
lr_model = LogisticRegression(random_state=1, max_iter=500)

# Fit the model to the training data
lr_model.fit(X_train_scaled, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {lr_model.score(X_train_scaled, y_train_encoded)}")
print(f"Testing Data Score: {lr_model.score(X_test_scaled, y_test_encoded)}")

# Create the support vector machine classifier model with a 'rbf' kernel
svm_model = SVC(kernel='rbf')

# Fit the model to the training data
svm_model.fit(X_train_scaled, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {svm_model.score(X_train_scaled, y_train_encoded)}")
print(f"Testing Data Score: {svm_model.score(X_test_scaled, y_test_encoded)}")

import matplotlib.pyplot as plt
# Loop through different k values to find which has the highest accuracy.
# Note: We use only odd numbers because we don't want any ties.
train_scores = []
test_scores = []
for k in range(1, 20, 2):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train_scaled, y_train_encoded)
    train_score = knn.score(X_train_scaled, y_train_encoded)
    test_score = knn.score(X_test_scaled, y_test_encoded)
    train_scores.append(train_score)
    test_scores.append(test_score)
    print(f"k: {k}, Train/Test Score: {train_score:.3f}/{test_score:.3f}")
    
# Plot the results
plt.plot(range(1, 20, 2), train_scores, marker='o', label="training scores")
plt.plot(range(1, 20, 2), test_scores, marker="x", label="testing scores")
plt.xlabel("k neighbors")
plt.ylabel("accuracy score")
plt.legend()
plt.show()

# Create the KNN model with 3 neighbors
knn_model = KNeighborsClassifier(n_neighbors=3)

# Fit the model to the training data
knn_model.fit(X_train_scaled, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {knn_model.score(X_train_scaled, y_train_encoded)}")
print(f"Testing Data Score: {knn_model.score(X_test_scaled, y_test_encoded)}")

# Create the decision tree classifier model
dt_model = DecisionTreeClassifier()

# Fit the model to the training data
dt_model.fit(X_train_scaled, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {dt_model.score(X_train_scaled, y_train_encoded)}")
print(f"Testing Data Score: {dt_model.score(X_test_scaled, y_test_encoded)}")

# Create the random forest classifier model
# with n_estimators=128 and random_state=1
rf_model = RandomForestClassifier(n_estimators=128, random_state=1)

# Fit the model to the training data
rf_model.fit(X_train_scaled, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {rf_model.score(X_train_scaled, y_train_encoded)}")
print(f"Testing Data Score: {rf_model.score(X_test_scaled, y_test_encoded)}")

# Train the Gradient Boosting classifier
clf = GradientBoostingClassifier(random_state=1).fit(X_train_scaled, y_train_encoded)

# Evaluate the model
print(f'Training Score: {clf.score(X_train_scaled, y_train_encoded)}')
print(f'Testing Score: {clf.score(X_test_scaled, y_test_encoded)}')

# Train the AdaBoostClassifier
clf = AdaBoostClassifier(random_state=1).fit(X_train_scaled, y_train_encoded)

# Evaluate the model
print(f'Training Score: {clf.score(X_train_scaled, y_train_encoded)}')
print(f'Testing Score: {clf.score(X_test_scaled, y_test_encoded)}')



# Import required dependencies
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Import data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_3/datasets/phishing.csv"
df = pd.read_csv(file_path)
df.head()

# Check the value_counts of the target column
df["Result"].value_counts()

# Check the data types
df.dtypes

# Get the target variable (the "Result" column)
y = df["Result"]
y

# Get the features (everything except the "Result" column)
X = df.copy()
X = X.drop(columns="Result")
X.head()

# Split data into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Remember that all of the columns in the DataFrame are objects
# Use a OneHotEncoder to convert the training data to numerical values
ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False, dtype='int')
X_train_encoded = pd.DataFrame(data=ohe.fit_transform(X_train), columns=ohe.get_feature_names_out())
X_train_encoded

# Encode the test data
X_test_encoded = pd.DataFrame(data=ohe.transform(X_test), columns=ohe.get_feature_names_out())
X_test_encoded

# Create the logistic regression classifier model with a random_state of 1
lr_model = LogisticRegression(random_state=1)

# Fit the model to the training data
lr_model.fit(X_train_encoded, y_train)

# Validate the model by checking the model accuracy with model.score
print('Train Accuracy: %.3f' % lr_model.score(X_train_encoded, y_train))
print('Test Accuracy: %.3f' % lr_model.score(X_test_encoded, y_test))

# Create the support vector machine classifier model with a 'linear' kernel
svm_model = SVC(kernel='linear')

# Fit the model to the training data
svm_model.fit(X_train_encoded, y_train)

# Validate the model by checking the model accuracy with model.score
print('Train Accuracy: %.3f' % svm_model.score(X_train_encoded, y_train))
print('Test Accuracy: %.3f' % svm_model.score(X_test_encoded, y_test))

# Create the KNN model with 9 neighbors
knn_model = KNeighborsClassifier(n_neighbors=9)

# Fit the model to the training data
knn_model.fit(X_train_encoded, y_train)

# Validate the model by checking the model accuracy with model.score
print('Train Accuracy: %.3f' % knn_model.score(X_train_encoded, y_train))
print('Test Accuracy: %.3f' % knn_model.score(X_test_encoded, y_test))

# Create the decision tree classifier model
dt_model = DecisionTreeClassifier()

# Fit the model to the training data
dt_model.fit(X_train_encoded, y_train)

# Validate the model by checking the model accuracy with model.score
print('Train Accuracy: %.3f' % dt_model.score(X_train_encoded, y_train))
print('Test Accuracy: %.3f' % dt_model.score(X_test_encoded, y_test))

# Create the random forest classifier model
# with n_estimators=128 and random_state=1
rf_model = RandomForestClassifier(n_estimators=128, random_state=1)

# Fit the model to the training data
rf_model.fit(X_train_encoded, y_train)

# Validate the model by checking the model accuracy with model.score
print('Train Accuracy: %.3f' % rf_model.score(X_train_encoded, y_train))
print('Test Accuracy: %.3f' % rf_model.score(X_test_encoded, y_test))



import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import AdaBoostClassifier

# Load in data
df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_3/datasets/sports-articles.csv')
df

# Drop the label to create the X data
X = df.drop('Label', axis=1)
X

# Create the y set from the "Label" column
y = df["Label"]
y

# Split the data into training and testing sets using random_state=1
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

# Encode the y data with the label encoder
# Create an instance of the label encoder
le = LabelEncoder()

# Fit and transform the y training and testing data using the label encoder
y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.transform(y_test)
y_train_encoded

# Scale the X data by using StandardScaler()
scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_train_scaled

# Transform the test dataset based on the fit from the training dataset
X_test_scaled = scaler.transform(X_test)
X_test_scaled

# Create the logistic regression classifier model with a random_state of 1
lr_model = LogisticRegression(random_state=1)

# Fit the model to the training data
lr_model.fit(X_train_scaled, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {lr_model.score(X_train_scaled, y_train_encoded)}")
print(f"Testing Data Score: {lr_model.score(X_test_scaled, y_test_encoded)}")

# Create the support vector machine classifier model with a 'rbf' kernel
svm_model = SVC(kernel='rbf')

# Fit the model to the training data
svm_model.fit(X_train_scaled, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {svm_model.score(X_train_scaled, y_train_encoded)}")
print(f"Testing Data Score: {svm_model.score(X_test_scaled, y_test_encoded)}")

import matplotlib.pyplot as plt
# Loop through different k values to find which has the highest accuracy.
# Note: We use only odd numbers because we don't want any ties.
train_scores = []
test_scores = []
for k in range(1, 20, 2):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train_scaled, y_train_encoded)
    train_score = knn.score(X_train_scaled, y_train_encoded)
    test_score = knn.score(X_test_scaled, y_test_encoded)
    train_scores.append(train_score)
    test_scores.append(test_score)
    print(f"k: {k}, Train/Test Score: {train_score:.3f}/{test_score:.3f}")
    
# Plot the results
plt.plot(range(1, 20, 2), train_scores, marker='o', label="training scores")
plt.plot(range(1, 20, 2), test_scores, marker="x", label="testing scores")
plt.xlabel("k neighbors")
plt.ylabel("accuracy score")
plt.legend()
plt.show()

# Create the KNN model with 11 neighbors
knn_model = KNeighborsClassifier(n_neighbors=11)

# Fit the model to the training data
knn_model.fit(X_train_scaled, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {knn_model.score(X_train_scaled, y_train_encoded)}")
print(f"Testing Data Score: {knn_model.score(X_test_scaled, y_test_encoded)}")

# Create the decision tree classifier model
dt_model = DecisionTreeClassifier()

# Fit the model to the training data
dt_model.fit(X_train_scaled, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {dt_model.score(X_train_scaled, y_train_encoded)}")
print(f"Testing Data Score: {dt_model.score(X_test_scaled, y_test_encoded)}")

# Create the random forest classifier model
# with n_estimators=128 and random_state=1
rf_model = RandomForestClassifier(n_estimators=128, random_state=1)

# Fit the model to the training data
rf_model.fit(X_train_scaled, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {rf_model.score(X_train_scaled, y_train_encoded)}")
print(f"Testing Data Score: {rf_model.score(X_test_scaled, y_test_encoded)}")

# Train the Gradient Boosting classifier
clf = GradientBoostingClassifier(random_state=1).fit(X_train_scaled, y_train_encoded)

# Evaluate the model
print(f'Training Score: {clf.score(X_train_scaled, y_train_encoded)}')
print(f'Testing Score: {clf.score(X_test_scaled, y_test_encoded)}')

# Train the AdaBoostClassifier
clf = AdaBoostClassifier(random_state=1).fit(X_train_scaled, y_train_encoded)

# Evaluate the model
print(f'Training Score: {clf.score(X_train_scaled, y_train_encoded)}')
print(f'Testing Score: {clf.score(X_test_scaled, y_test_encoded)}')



import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Load in data
df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_3/datasets/vertebral-column.csv')
df

# Drop the label to create the X data
X = df.drop('class', axis=1)
X

# Create the y set from the "Churn" column
y = df["class"]
y

# Split the data into training and testing sets using random_state=1
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

# Encode the y data with the label encoder
# Create an instance of the label encoder
le = LabelEncoder()

# Fit and transform the y training and testing data using the label encoder
y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.transform(y_test)
y_train_encoded

# Scale the X data by using StandardScaler()
scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_train_scaled

# Transform the test dataset based on the fit from the training dataset
X_test_scaled = scaler.transform(X_test)
X_test_scaled

# Create the logistic regression classifier model with a random_state of 1
lr_model = LogisticRegression()

# Fit the model to the training data
lr_model.fit(X_train_scaled, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {lr_model.score(X_train_scaled, y_train_encoded)}")
print(f"Testing Data Score: {lr_model.score(X_test_scaled, y_test_encoded)}")

# Create the support vector machine classifier model with a 'rbf' kernel
svm_model = SVC(kernel='linear')

# Fit the model to the training data
svm_model.fit(X_train_scaled, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {svm_model.score(X_train_scaled, y_train_encoded)}")
print(f"Testing Data Score: {svm_model.score(X_test_scaled, y_test_encoded)}")

import matplotlib.pyplot as plt
# Loop through different k values to find which has the highest accuracy.
# Note: We use only odd numbers because we don't want any ties.
train_scores = []
test_scores = []
for k in range(1, 20, 2):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train_scaled, y_train_encoded)
    train_score = knn.score(X_train_scaled, y_train_encoded)
    test_score = knn.score(X_test_scaled, y_test_encoded)
    train_scores.append(train_score)
    test_scores.append(test_score)
    print(f"k: {k}, Train/Test Score: {train_score:.3f}/{test_score:.3f}")
    
# Plot the results
plt.plot(range(1, 20, 2), train_scores, marker='o', label="training scores")
plt.plot(range(1, 20, 2), test_scores, marker="x", label="testing scores")
plt.xlabel("k neighbors")
plt.ylabel("accuracy score")
plt.legend()
plt.show()

# Create the KNN model with 9 neighbors
knn_model = KNeighborsClassifier(n_neighbors=9)

# Fit the model to the training data
knn_model.fit(X_train_scaled, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {knn_model.score(X_train_scaled, y_train_encoded)}")
print(f"Testing Data Score: {knn_model.score(X_test_scaled, y_test_encoded)}")

# Create the decision tree classifier model
dt_model = DecisionTreeClassifier()

# Fit the model to the training data
dt_model.fit(X_train_scaled, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {dt_model.score(X_train_scaled, y_train_encoded)}")
print(f"Testing Data Score: {dt_model.score(X_test_scaled, y_test_encoded)}")

# Create the random forest classifier model
# with n_estimators=128 and random_state=1
rf_model = RandomForestClassifier(n_estimators=128, random_state=1)

# Fit the model to the training data
rf_model.fit(X_train_scaled, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {rf_model.score(X_train_scaled, y_train_encoded)}")
print(f"Testing Data Score: {rf_model.score(X_test_scaled, y_test_encoded)}")



%matplotlib inline
from matplotlib import pyplot as plt
from sklearn.datasets import make_regression, make_swiss_roll
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

def test_model(model, data):
    X_train_scaled, X_test_scaled, y_train, y_test = data
    reg = model.fit(X_train_scaled, y_train)
    print(f'Model: {type(reg).__name__}')
    print(f'Train score: {reg.score(X_train_scaled, y_train)}')
    print(f'Test Score: {reg.score(X_test_scaled, y_test)}\n')
    plt.show()    

# Create data
X, y = make_regression(random_state=1)
X = pd.DataFrame(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)
scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
data = [X_train_scaled, X_test_scaled, y_train, y_test]

from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor
from sklearn.svm import SVR

test_model(LinearRegression(), data)

test_model(KNeighborsRegressor(), data)
test_model(RandomForestRegressor(), data)
test_model(ExtraTreesRegressor(), data)
test_model(AdaBoostRegressor(), data)
test_model(SVR(C=1.0, epsilon=0.2), data)

# Create data
X, y = make_swiss_roll(random_state=1, n_samples=500, noise=1)
X = pd.DataFrame(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)
scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
data = [X_train_scaled, X_test_scaled, y_train, y_test]

X_train_scaled

# Plot the result
ax = plt.figure().add_subplot(projection='3d')
ax.view_init(7, -80)
ax.scatter(X[0], X[1], X[2],
           color=plt.cm.jet(y/y.max()),
           s=20, edgecolor='k')
plt.savefig("swiss_roll.png")
plt.show()

test_model(LinearRegression(), data)

test_model(KNeighborsRegressor(), data)
test_model(RandomForestRegressor(), data)
test_model(ExtraTreesRegressor(), data)
test_model(AdaBoostRegressor(), data)
test_model(SVR(C=1.0, epsilon=0.2), data)



# Import the data
import pandas as pd

# Create a Dataframe for the data
df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m14/datasets/bank_marketing.csv')
df.head(10)



# Import the data
import pandas as pd

# Create a Dataframe for the data




# Import the data
import pandas as pd

df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m14/datasets/bank_marketing.csv')
df.head()

# Drop rows with null values
df_clean = df.dropna().copy()

# Convert y to numeric
df_clean['y'] = pd.get_dummies(df_clean['y'], drop_first = True)

# Drop all non-numeric columns
df_clean = df_clean.select_dtypes(include='number')

# Verify changes with the info method
df_clean.info()

# Setup X and y variables
X = df_clean.drop(columns='y')
y = df_clean['y']

# Create and train a Random Forest model
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier()
model.fit(X, y)

# Check the model's score
model.score(X, y)



# Import the data
import pandas as pd

df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m14/datasets/bank_marketing.csv')
df.head()

# Drop rows with null values

# Convert y to numeric

# Drop all non-numeric columns

# Verify changes with the info method


# Setup X and y variables


# Create and train a logistic regression model
from sklearn.linear_model import LogisticRegression



# Check the model's score


import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, balanced_accuracy_score, roc_auc_score

# Import the data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m14/lesson_1/datasets/app-data-imbalanced.csv")
df.head()

# Show the total number of positive and negative results
df['Result'].value_counts()

# Create an X and y variable
X = df.drop(columns=['Result'])
y = df['Result']

# Create a Logistic Regression Model
classifier = LogisticRegression()

# Fit (train) or model using the training data
classifier.fit(X, y)

# Calculate the accuracy of the model
classifier.score(X, y)

# Make predictions on the test data
predictions = classifier.predict(X)

# Create a confusion matrix
print(confusion_matrix(y, predictions, labels = [1,0]))

# Create a classification report
print(classification_report(y, predictions, labels = [1, 0]))

# Calculate the balanced accuracy score
print(balanced_accuracy_score(y, predictions))

# Predict values with probabilities
pred_probas = classifier.predict_proba(X)

# Print the probabilities
pred_probas

# Each prediction includes a prediction for both the 0 class and the 1 class
# We only need the predictions for the 1 class; use a list comprehension to 
# gather the second value from each list

pred_probas_firsts = [prob[1] for prob in pred_probas]

# Print the first 5 probabilities
pred_probas_firsts[0:5]

# Calculate the roc_auc_score
print(roc_auc_score(y, pred_probas_firsts))



import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, balanced_accuracy_score, roc_auc_score

# Import the data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m14/lesson_1/datasets/app-data-imbalanced.csv")
df.head()

# Show the total number of positive and negative results
df['Result'].value_counts()

# Create an X and y variable
X = df.drop(columns=['Result'])
y = df['Result']

# Create a Logistic Regression Model
classifier = LogisticRegression()

# Fit (train) or model using the training data
classifier.fit(X, y)

# Calculate the accuracy of the model
classifier.score(X, y)

# Make predictions on the test data
predictions = classifier.predict(X)

# Create a confusion matrix


# Create a classification report


# Calculate the balanced accuracy score


# Predict values with probabilities


# Print the probabilities


# Each prediction includes a prediction for both the 0 class and the 1 class
# We only need the predictions for the 1 class; use a list comprehension to 
# gather the second value from each list


# Print the first 5 probabilities


# Calculate the roc_auc_score




import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, balanced_accuracy_score, roc_auc_score

# Import the data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m14/lesson_1/datasets/crowdfunding-data-imbalanced.csv")
df.head()

# Show the total number of positive and negative outcomes
df['outcome'].value_counts()

# Create an X and y variable
X = df.drop(columns=['outcome'])
y = df['outcome']

# Create a Logistic Regression Model
classifier = LogisticRegression()

# Fit the model to the training data
classifier.fit(X, y)

# Calculate the accuracy of the model
classifier.score(X, y)

# Make predictions on the test data
predictions = classifier.predict(X)

# Create a confusion matrix
print(confusion_matrix(y, predictions, labels = [1,0]))

# Create a classification report
print(classification_report(y, predictions, labels = [1, 0]))

# Calculate the balanced accuracy score
print(balanced_accuracy_score(y, predictions))

# Predict values with probabilities
pred_probas = classifier.predict_proba(X)

# Print the probabilities
pred_probas

# Each prediction includes a prediction for both the 0 class and the 1 class
# We only need the predictions for the 1 class; use a list comprehension to 
# gather the second value from each list

pred_probas_firsts = [prob[1] for prob in pred_probas]

# Print the first 5 probabilities
pred_probas_firsts[0:5]

# Calculate the roc_auc_score
print(roc_auc_score(y, pred_probas_firsts))

import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, balanced_accuracy_score, roc_auc_score

# Import the data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m14/lesson_1/datasets/crowdfunding-data-imbalanced.csv")
df.head()

# Show the total number of positive and negative outcomes


# Create an X and y variable


# Create a Logistic Regression Model


# Fit the model to the training data


# Calculate the accuracy of the model



# Make predictions on the test data


# Create a confusion matrix


# Create a classification report


# Calculate the balanced accuracy score


# Predict values with probabilities


# Print the probabilities


# Each prediction includes a prediction for both the 0 class and the 1 class
# We only need the predictions for the 1 class; use a list comprehension to 
# gather the second value from each list


# Print the first 5 probabilities



# Calculate the roc_auc_score


import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# Import the data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m14/lesson_1/datasets/app-data.csv")
df.info()

# Create an X and y variable
X = df.drop(columns=['Result'])
y = df['Result']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13)

# Create a Random Forest model
classifier = RandomForestClassifier(random_state=13)

# Fit (train) or model using the training data
classifier.fit(X_train, y_train)

# Calculate the accuracy of the model on the testing data
classifier.score(X_test, y_test)

# Calculate the accuracy of the model on the training data
classifier.score(X_train, y_train)

# Create a loop to vary the max_depth parameter
# Make sure to record the train and test scores 
# for each pass.

# Depths should span from 1 up to 40 in steps of 2
depths = range(1, 40, 2)

# The scores dataframe will hold depths and scores
# to make plotting easy
scores = {'train': [], 'test': [], 'depth': []}

# Loop through each depth (this will take time to run)
for depth in depths:
    clf = RandomForestClassifier(max_depth=depth)
    clf.fit(X_train, y_train)

    train_score = clf.score(X_train, y_train)
    test_score = clf.score(X_test, y_test)

    scores['depth'].append(depth)
    scores['train'].append(train_score)
    scores['test'].append(test_score)

# Create a dataframe from the scores dictionary and
# set the index to depth
scores_df = pd.DataFrame(scores).set_index('depth')

scores_df.head()

# Plot the scores dataframe with the plot method
scores_df.plot()



import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# Import the data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m14/lesson_1/datasets/app-data.csv")
df.info()

# Create an X and y variable
X = df.drop(columns=['Result'])
y = df['Result']

# Split the data into training and testing sets


# Create a Random Forest model


# Fit (train) or model using the training data


# Calculate the accuracy of the model on the testing data


# Calculate the accuracy of the model on the training data


# Create a loop to vary the max_depth parameter
# Make sure to record the train and test scores 
# for each pass.

# Depths should span from 1 up to 40 in steps of 2


# The scores dataframe will hold depths and scores
# to make plotting easy


# Loop through each depth (this will take time to run)


# Create a dataframe from the scores dictionary and
# set the index to depth





# Plot the scores dataframe with the plot method




import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# Import the data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_2/datasets/crowdfunding-data.csv")
df.info()

# Create an X and y variable
X = df.drop(columns=['outcome'])
y = df['outcome']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y)

# Create a Random Forest model
classifier = RandomForestClassifier()

# Fit (train) or model using the training data
classifier.fit(X_train, y_train)

# Calculate the accuracy of the model on the testing data
classifier.score(X_test, y_test)

# Calculate the accuracy of the model on the training data
classifier.score(X_train, y_train)

# Create a loop to vary the max_depth parameter
# Make sure to record the train and test scores 
# for each pass.

# Depths should span from 1 up to 15 in steps of 1
depths = range(1, 15)

# The scores dataframe will hold depths and scores
# to make plotting easy
scores = {'train': [], 'test': [], 'depth': []}

# Loop through each depth
for depth in depths:
    clf = RandomForestClassifier(max_depth=depth)
    clf.fit(X_train, y_train)

    train_score = clf.score(X_train, y_train)
    test_score = clf.score(X_test, y_test)

    scores['depth'].append(depth)
    scores['train'].append(train_score)
    scores['test'].append(test_score)

# Create a dataframe from the scores dictionary and
# set the index to depth
scores_df = pd.DataFrame(scores).set_index('depth')

# Plot the scores dataframe with the plot method
scores_df.plot()



import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# Import the data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_2/datasets/crowdfunding-data.csv")
df.info()

# Create an X and y variable
X = df.drop(columns=['outcome'])
y = df['outcome']

# Split the data into training and testing sets


# Create a Random Forest model


# Fit (train) or model using the training data


# Calculate the accuracy of the model on the testing data


# Calculate the accuracy of the model on the training data


# Create a loop to vary the max_depth parameter
# Make sure to record the train and test scores 
# for each pass.

# Depths should span from 1 up to 15 in steps of 1


# The scores dataframe will hold depths and scores
# to make plotting easy


# Loop through each depth


# Create a dataframe from the scores dictionary and
# set the index to depth


# Plot the scores dataframe with the plot method




# Import the data
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import balanced_accuracy_score

df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m14/datasets/bank_marketing.csv')
df.head()

# Drop rows with null values
df_clean = df.dropna().copy()

# Convert y to numeric
df_clean['y'] = pd.get_dummies(df_clean['y'], drop_first = True)

# Drop all non-numeric columns
df_clean = df_clean.select_dtypes(include='number')

# Verify changes with the info method
df_clean.info()

# Setup X and y variables
X = df_clean.drop(columns='y')
y = df_clean['y']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13)

# Create and train a random forest model
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier()
model.fit(X_train, y_train)

# Check the model's balanced accuracy on the test set

y_pred = model.predict(X_test)
print(balanced_accuracy_score(y_test, y_pred))

# Check the model's balanced accuracy on the training set

y_train_pred = model.predict(X_train)
print(balanced_accuracy_score(y_train, y_train_pred))

# Try the following values for max_depth

max_depths = range(1, 10)
models = {'train_score': [], 'test_score': [], 'max_depth': []}

# Loop through each value in max_depths
for depth in max_depths:
    clf = RandomForestClassifier(max_depth = depth)
    clf.fit(X_train, y_train)

    train_pred = clf.predict(X_train)
    test_pred = clf.predict(X_test)

    train_score = balanced_accuracy_score(y_train, train_pred)
    test_score = balanced_accuracy_score(y_test, test_pred)

    models['train_score'].append(train_score)
    models['test_score'].append(test_score)
    models['max_depth'].append(depth)

# Create a dataframe from the models dictionary with max_depth as the index
models_df = pd.DataFrame(models).set_index('max_depth')


# Plot the results
models_df.plot()

clf = RandomForestClassifier(max_depth=5)
clf.fit(X_train, y_train) 

train_pred = clf.predict(X_train)
test_pred = clf.predict(X_test)

print(balanced_accuracy_score(y_train, train_pred))
print(balanced_accuracy_score(y_test, test_pred))




# Import the data
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import balanced_accuracy_score

df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m14/datasets/bank_marketing.csv')
df.head()

# Drop rows with null values
df_clean = df.dropna().copy()

# Convert y to numeric
df_clean['y'] = pd.get_dummies(df_clean['y'], drop_first = True)

# Drop all non-numeric columns
df_clean = df_clean.select_dtypes(include='number')

# Verify changes with the info method
df_clean.info()

# Setup X and y variables
X = df_clean.drop(columns='y')
y = df_clean['y']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13)

# Create and train a logistic regression model
from sklearn.linear_model import LogisticRegression



# Check the model's balanced accuracy on the test set


# Check the model's balanced accuracy on the training set



# Try the following values for max_depth

max_depths = range(1, 10)
models = {'train_score': [], 'test_score': [], 'max_depth': []}

# Loop through each value in max_depths




# Create a dataframe from the models dictionary with max_depth as the index



# Plot the results




# Import the data
import pandas as pd

# Create a Dataframe for the data
df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m14/datasets/bank_marketing.csv')
df.head(10)



# Import the data
import pandas as pd

# Create a Dataframe for the data




# Import the data
import pandas as pd

df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m14/datasets/bank_marketing.csv')
df.head()

# Drop rows with null values

# Convert y to numeric

# Drop all non-numeric columns

# Verify changes with the info method


# Setup X and y variables


# Create and train a logistic regression model
from sklearn.linear_model import LogisticRegression



# Check the model's score


import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, balanced_accuracy_score, roc_auc_score

# Import the data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m14/lesson_1/datasets/app-data-imbalanced.csv")
df.head()

# Show the total number of positive and negative results
df['Result'].value_counts()

# Create an X and y variable
X = df.drop(columns=['Result'])
y = df['Result']

# Create a Logistic Regression Model
classifier = LogisticRegression()

# Fit (train) or model using the training data
classifier.fit(X, y)

# Calculate the accuracy of the model
classifier.score(X, y)

# Make predictions on the test data
predictions = classifier.predict(X)

# Create a confusion matrix
print(confusion_matrix(y, predictions, labels = [1,0]))

# Create a classification report
print(classification_report(y, predictions, labels = [1, 0]))

# Calculate the balanced accuracy score
print(balanced_accuracy_score(y, predictions))

# Predict values with probabilities
pred_probas = classifier.predict_proba(X)

# Print the probabilities
pred_probas

# Each prediction includes a prediction for both the 0 class and the 1 class
# We only need the predictions for the 1 class; use a list comprehension to 
# gather the second value from each list

pred_probas_firsts = [prob[1] for prob in pred_probas]

# Print the first 5 probabilities
pred_probas_firsts[0:5]

# Calculate the roc_auc_score
print(roc_auc_score(y, pred_probas_firsts))



import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, balanced_accuracy_score, roc_auc_score

# Import the data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m14/lesson_1/datasets/app-data-imbalanced.csv")
df.head()

# Show the total number of positive and negative results
df['Result'].value_counts()

# Create an X and y variable
X = df.drop(columns=['Result'])
y = df['Result']

# Create a Logistic Regression Model
classifier = LogisticRegression()

# Fit (train) or model using the training data
classifier.fit(X, y)

# Calculate the accuracy of the model
classifier.score(X, y)

# Make predictions on the test data
predictions = classifier.predict(X)

# Create a confusion matrix


# Create a classification report


# Calculate the balanced accuracy score


# Predict values with probabilities


# Print the probabilities


# Each prediction includes a prediction for both the 0 class and the 1 class
# We only need the predictions for the 1 class; use a list comprehension to 
# gather the second value from each list


# Print the first 5 probabilities


# Calculate the roc_auc_score




import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, balanced_accuracy_score, roc_auc_score

# Import the data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m14/lesson_1/datasets/crowdfunding-data-imbalanced.csv")
df.head()

# Show the total number of positive and negative outcomes


# Create an X and y variable


# Create a Logistic Regression Model


# Fit the model to the training data


# Calculate the accuracy of the model



# Make predictions on the test data


# Create a confusion matrix


# Create a classification report


# Calculate the balanced accuracy score


# Predict values with probabilities


# Print the probabilities


# Each prediction includes a prediction for both the 0 class and the 1 class
# We only need the predictions for the 1 class; use a list comprehension to 
# gather the second value from each list


# Print the first 5 probabilities



# Calculate the roc_auc_score


import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# Import the data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m14/lesson_1/datasets/app-data.csv")
df.info()

# Create an X and y variable
X = df.drop(columns=['Result'])
y = df['Result']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13)

# Create a Random Forest model
classifier = RandomForestClassifier(random_state=13)

# Fit (train) or model using the training data
classifier.fit(X_train, y_train)

# Calculate the accuracy of the model on the testing data
classifier.score(X_test, y_test)

# Calculate the accuracy of the model on the training data
classifier.score(X_train, y_train)

# Create a loop to vary the max_depth parameter
# Make sure to record the train and test scores 
# for each pass.

# Depths should span from 1 up to 40 in steps of 2
depths = range(1, 40, 2)

# The scores dataframe will hold depths and scores
# to make plotting easy
scores = {'train': [], 'test': [], 'depth': []}

# Loop through each depth (this will take time to run)
for depth in depths:
    clf = RandomForestClassifier(max_depth=depth)
    clf.fit(X_train, y_train)

    train_score = clf.score(X_train, y_train)
    test_score = clf.score(X_test, y_test)

    scores['depth'].append(depth)
    scores['train'].append(train_score)
    scores['test'].append(test_score)

# Create a dataframe from the scores dictionary and
# set the index to depth
scores_df = pd.DataFrame(scores).set_index('depth')

scores_df.head()

# Plot the scores dataframe with the plot method
scores_df.plot()



import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# Import the data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m14/lesson_1/datasets/app-data.csv")
df.info()

# Create an X and y variable
X = df.drop(columns=['Result'])
y = df['Result']

# Split the data into training and testing sets


# Create a Random Forest model


# Fit (train) or model using the training data


# Calculate the accuracy of the model on the testing data


# Calculate the accuracy of the model on the training data


# Create a loop to vary the max_depth parameter
# Make sure to record the train and test scores 
# for each pass.

# Depths should span from 1 up to 40 in steps of 2


# The scores dataframe will hold depths and scores
# to make plotting easy


# Loop through each depth (this will take time to run)


# Create a dataframe from the scores dictionary and
# set the index to depth





# Plot the scores dataframe with the plot method




import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# Import the data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_2/datasets/crowdfunding-data.csv")
df.info()

# Create an X and y variable
X = df.drop(columns=['outcome'])
y = df['outcome']

# Split the data into training and testing sets


# Create a Random Forest model


# Fit (train) or model using the training data


# Calculate the accuracy of the model on the testing data


# Calculate the accuracy of the model on the training data


# Create a loop to vary the max_depth parameter
# Make sure to record the train and test scores 
# for each pass.

# Depths should span from 1 up to 15 in steps of 1


# The scores dataframe will hold depths and scores
# to make plotting easy


# Loop through each depth


# Create a dataframe from the scores dictionary and
# set the index to depth


# Plot the scores dataframe with the plot method




# Import the data
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import balanced_accuracy_score

df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m14/datasets/bank_marketing.csv')
df.head()

# Drop rows with null values
df_clean = df.dropna().copy()

# Convert y to numeric
df_clean['y'] = pd.get_dummies(df_clean['y'], drop_first = True)

# Drop all non-numeric columns
df_clean = df_clean.select_dtypes(include='number')

# Verify changes with the info method
df_clean.info()

# Setup X and y variables
X = df_clean.drop(columns='y')
y = df_clean['y']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13)

# Create and train a logistic regression model
from sklearn.linear_model import LogisticRegression



# Check the model's balanced accuracy on the test set


# Check the model's balanced accuracy on the training set



# Try the following values for max_depth

max_depths = range(1, 10)
models = {'train_score': [], 'test_score': [], 'max_depth': []}

# Loop through each value in max_depths




# Create a dataframe from the models dictionary with max_depth as the index



# Plot the results




# Import the data
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import balanced_accuracy_score

df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m14/datasets/bank_marketing.csv')
df.head()

# Drop rows with null values
df_clean = df.dropna().copy()

# Convert y to numeric
df_clean['y'] = pd.get_dummies(df_clean['y'], drop_first = True)

# Drop all non-numeric columns
df_clean = df_clean.select_dtypes(include='number')

# Verify changes with the info method
df_clean.info()

# Setup X and y variables
X = df_clean.drop(columns='y')
y = df_clean['y']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13)

# Create and train a logistic regression model
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(max_depth=5)
model.fit(X_train, y_train)

# Check the model's balanced accuracy on the test set

y_pred = model.predict(X_test)
print(balanced_accuracy_score(y_test, y_pred))

# Check the model's balanced accuracy on the training set

y_train_pred = model.predict(X_train)
print(balanced_accuracy_score(y_train, y_train_pred))



# Import the data
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import balanced_accuracy_score

df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m14/datasets/bank_marketing.csv')
df.head()

# Drop rows with null values
df_clean = df.dropna().copy()

# Convert y to numeric
df_clean['y'] = pd.get_dummies(df_clean['y'], drop_first = True)

# Drop all non-numeric columns
df_clean = df_clean.select_dtypes(include='number')

# Verify changes with the info method
df_clean.info()

# Setup X and y variables
X = df_clean.drop(columns='y')
y = df_clean['y']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13)

# Create and train a logistic regression model
from sklearn.ensemble import RandomForestClassifier



# Check the model's balanced accuracy on the test set


# Check the model's balanced accuracy on the training set




import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Import the data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m14/lesson_2/datasets/app-data-leakage.csv")
df.head()

# Create an X and y variable
X = df.drop(columns=['Result'])
y = df['Result']

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 13)

# Create a Logistic Regression Model
classifier = RandomForestClassifier(random_state=13)

# Fit (train) or model using the training data
classifier.fit(X_train, y_train)

# Calculate the accuracy of the model with training data
classifier.score(X_train, y_train)

# Calculate the accuracy of the model with testing data
classifier.score(X_test, y_test)

# Check correlation of columns
df.corr()['Result'].sort_values()

# Plot app_number and Result in a scatter plot
df.plot(kind='scatter', x='app_number', y='Result')



import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Import the data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m14/lesson_2/datasets/app-data-leakage.csv")
df.head()

# Create an X and y variable
X = df.drop(columns=['Result'])
y = df['Result']

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 13)

# Create a Logistic Regression Model

# Fit (train) or model using the training data

# Calculate the accuracy of the model with training data


# Calculate the accuracy of the model with testing data


# Check correlation of columns


# Plot app_number and Result in a scatter plot




import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Import the data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m14/lesson_2/datasets/crowdfunding-data-leakage.csv")

# Create an X and y variable
X = df.drop(columns=['outcome'])
y = df['outcome']

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 13)

# Create a Random Forest Model
classifier = RandomForestClassifier(random_state=13)

# Fit (train) or model using the training data
classifier.fit(X_train, y_train)

# Calculate the accuracy of the model with training data
classifier.score(X_train, y_train)

# Calculate the accuracy of the model with testing data
classifier.score(X_test, y_test)

# Identify any columns that could be leaking data
df.head()

# Check correlation of columns to the outcome column
df.corr()['outcome'].sort_values()

# Plot rewards_given and outcome in a scatter plot
df.plot(kind='scatter', x='rewards_given', y='outcome')

# Import the data
df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m14/lesson_2/datasets/start-up-success-leakage.csv')

# Create an X and y variable
X = df.drop(columns=['Firm Category'])
y = df['Firm Category']

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 13)

# Create a Random Forest Model
classifier = RandomForestClassifier(random_state=13)

# Fit (train) or model using the training data
classifier.fit(X_train, y_train)

# Calculate the accuracy of the model with training data
classifier.score(X_train, y_train)

# Calculate the accuracy of the model with testing data
classifier.score(X_test, y_test)

# Identify any columns that could be leaking data
df.head()

# Check correlation of columns to the Firm Category column
df.corr()['Firm Category'].sort_values()

# Plot Firm ID and Firm Category in a scatter plot
df.plot(kind='scatter', x='Firm ID', y='Firm Category')



import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Import the data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m14/lesson_2/datasets/crowdfunding-data-leakage.csv")

# Create an X and y variable
X = df.drop(columns=['outcome'])
y = df['outcome']

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 13)

# Create a Random Forest Model
classifier = RandomForestClassifier(random_state=13)

# Fit (train) or model using the training data
classifier.fit(X_train, y_train)

# Calculate the accuracy of the model with training data
classifier.score(X_train, y_train)

# Calculate the accuracy of the model with testing data
classifier.score(X_test, y_test)

# Identify any columns that could be leaking data
df.head()

# Check correlation of columns to the outcome column


# Plot rewards_given and outcome in a scatter plot


# Import the data
df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m14/lesson_2/datasets/start-up-success-leakage.csv')

# Create an X and y variable
X = df.drop(columns=['Firm Category'])
y = df['Firm Category']

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 13)

# Create a Random Forest Model
classifier = RandomForestClassifier(random_state=13)

# Fit (train) or model using the training data
classifier.fit(X_train, y_train)

# Calculate the accuracy of the model with training data
classifier.score(X_train, y_train)

# Calculate the accuracy of the model with testing data
classifier.score(X_test, y_test)

# Identify any columns that could be leaking data
df.head()

# Check correlation of columns to the Firm Category column


# Plot Firm ID and Firm Category in a scatter plot




import pandas as pd
from sklearn.model_selection import train_test_split

# Import the data
import numpy as np
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m14/lesson_2/datasets/crowdfunding-missing-data.csv")
df.head()

# Split into training and testing sets
X = df.drop(columns = 'outcome')
y = df['outcome'].values.reshape(-1, 1)

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13)

# Find the percentage of rows missing values in each column
X_train.isna().sum()/len(df)

# Describe the other columns in the rows with missing values
X_train.loc[X_train['backers_count'].isna()].describe()

# Describe the whole dataset
X_train.describe()

# Perform other exploratory analysis here
# For this specific example, try making histograms
# of days_active for the whole dataset and then
# again only when backers_count is missing.

X_train['days_active'].plot(kind='hist', alpha=0.2)
X_train.loc[df['backers_count'].isna(), 'days_active'].plot(kind='hist')
print(X_train.loc[df['backers_count'].isna(), 'days_active'].unique())

# Since backers_count seems to be missing in the first week
# of a campaign, removing the data would be detrimental.
# A good choice might be to fill the data using the backers
# counts from campaigns in week 2.

mean_of_week_2_backers_counts = X_train.loc[(X_train['days_active'] >= 6) & (X_train['days_active'] <= 13), 'backers_count'].mean()
mean_of_week_2_backers_counts


# Create a function to fill missing values with half of the mean of week 2

def X_preprocess(X_data):
    X_data['backers_count'] = X_data['backers_count'].fillna(int(round(mean_of_week_2_backers_counts/2)))
    return X_data

# Preprocess the training and testing data

X_train_clean = X_preprocess(X_train)
X_test_clean = X_preprocess(X_test)

# Check for missing values
print(X_train_clean.isna().sum()/len(X_train_clean))
print(X_test_clean.isna().sum()/len(X_test_clean))



import pandas as pd
from sklearn.model_selection import train_test_split

# Import the data
import numpy as np
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m14/lesson_2/datasets/crowdfunding-missing-data.csv")
df.head()

# Split into training and testing sets
X = df.drop(columns = 'outcome')
y = df['outcome'].values.reshape(-1, 1)

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13)

# Find the percentage of rows missing values in each column


# Describe the other columns in the rows with missing values


# Describe the whole dataset


# Perform other exploratory analysis here
# For this specific example, try making histograms
# of days_active for the whole dataset and then
# again only when backers_count is missing.



# Since backers_count seems to be missing in the first week
# of a campaign, removing the data would be detrimental.
# A good choice might be to fill the data using the backers
# counts from campaigns in week 2.




# Create a function to fill missing values with half of the mean of week 2



# Preprocess the training and testing data




# Check for missing values





import pandas as pd
import numpy as np
import random
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder
from sklearn.model_selection import train_test_split

# Import the data
df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m14/lesson_2/datasets/text-data.csv')
df.head()

# Create X and y and split into training and testing sets
X = df.drop(columns='arrived')
y = df['arrived']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13)

# Decide how to encode the backpack_color column
X_train['backpack_color'].value_counts()

# Create an encoder for the backpack_color column
backpack_color_ohe = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)

# Train the encoder
backpack_color_ohe.fit(X_train['backpack_color'].values.reshape(-1,1))


# Decide how to encode the grade column
df['grade'].value_counts()



# Create an encoder for the backpack_color column
grade_ord_enc = OrdinalEncoder(categories = [['F', 'D', 'C', 'B', 'A']], encoded_missing_value=-1, handle_unknown='use_encoded_value', unknown_value=-1)

# Train the encoder
grade_ord_enc.fit(X_train['grade'].values.reshape(-1,1))

# Decide how to encode the favorite_creature column
df['favorite_creature'].value_counts()

# Create an encoder for the backpack_color column
creature_ohe = OneHotEncoder(handle_unknown='infrequent_if_exist', sparse_output=False, min_frequency=0.2)

# Train the encoder
creature_ohe.fit(X_train['favorite_creature'].values.reshape(-1,1))

# Create a function using the pretrained encoders to use on
# any new data (including the testing data)

def X_preprocess(X_data):
    # Transform each column into numpy arrays
    backpack_color_encoded = backpack_color_ohe.transform(X_data['backpack_color'].values.reshape(-1,1))
    grade_encoded = grade_ord_enc.transform(X_data['grade'].values.reshape(-1,1))
    favorite_creature_encoded = creature_ohe.transform(X_data['favorite_creature'].values.reshape(-1,1))

    # Reorganize the numpy arrays into a DataFrame
    backpack_color_df = pd.DataFrame(backpack_color_encoded, columns = backpack_color_ohe.get_feature_names_out())
    creature_df = pd.DataFrame(favorite_creature_encoded, columns= creature_ohe.get_feature_names_out())
    out_df = pd.concat([backpack_color_df, creature_df], axis = 1)
    out_df['grade'] = grade_encoded

    # Return the DataFrame
    return out_df


# Preprocess the training data
X_preprocess(X_train)

# Preprocess the testing data
X_preprocess(X_test)

import pandas as pd
import numpy as np
import random
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder
from sklearn.model_selection import train_test_split

# Import the data
df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m14/lesson_2/datasets/text-data.csv')
df.head()

# Create X and y and split into training and testing sets
X = df.drop(columns='arrived')
y = df['arrived']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13)

# Decide how to encode the backpack_color column


# Create an encoder for the backpack_color column


# Train the encoder



# Decide how to encode the grade column




# Create an encoder for the backpack_color column


# Train the encoder



# Decide how to encode the favorite_creature column


# Create an encoder for the backpack_color column


# Train the encoder



# Create a function using the pretrained encoders to use on
# any new data (including the testing data)

def X_preprocess(X_data):
    # Transform each column into numpy arrays
    
    
    # Reorganize the numpy arrays into a DataFrame
   


    # Return the DataFrame
    return 


# Preprocess the training data
X_preprocess(X_train)

# Preprocess the testing data
X_preprocess(X_test)

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Import the data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m14/datasets/crowdfunding-data.csv")
df.head()

# Create a column "pledged_per_backer"
df['pledged_per_backer'] = df['pledged'] / df['backers_count']

df.head()

# Fill the missing values with zeros
df['pledged_per_backer'] = df['pledged_per_backer'].fillna(0)
df.head()

# Create a backers_per_day column
df['backers_per_day'] = df['backers_count'] / df['days_active']

df.head()

# Create a days_to_goal column
def days_to_goal(row):
    amount_remaining = row['goal'] - row['pledged']
    pledged_per_day = row['pledged_per_backer'] * row['backers_per_day']
    # Note that we can't divide by zero:
    # return a large number if pledged_per_day is zero
    if pledged_per_day == 0:
        return 10000
    return (amount_remaining)/(pledged_per_day)

df['days_to_goal'] = df.apply(days_to_goal, axis=1)
df.head()

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Import the data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m14/datasets/crowdfunding-data.csv")
df.head()

# Create a column "pledged_per_backer"



# Fill the missing values with zeros



# Create a backers_per_day column



# Create a days_to_goal column

# Create a function to apply


    # Note that we can't divide by zero:
    # return a large number if pledged_per_day is zero
   
    return 

# Apply the function


# Import the data
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import balanced_accuracy_score
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder

df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m14/datasets/bank_marketing.csv')
df.head()

# Setup X and y variables
X = df.drop(columns='y')
y = df['y'].values.reshape(-1,1)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13)
X_train.describe()

# Find the percentage of null values in each column
X_train.isna().sum()/len(X_train)

# Explore each column with missing values to determine the best fill strategy
# First the job column
X_train['job'].value_counts()

# The job column is varied and the number of missing values is small
# It might suffice to fill the missing values with "unknown"
# We'll make a function to handle this.
def fill_job(X_data):
    X_data['job'] = X_data['job'].fillna('unknown')
    return X_data

# Education is next
X_train['education'].value_counts()

X_train.loc[X_train['education'].isna()].describe()

X_train.loc[X_train['education'].isna(), 'job'].value_counts()

# The vast majority of rows missing an education value
# have a job which wouldn't require a higher education
# Lets fillna for education with 'primary', but 'unknown'
# might be a good choice as well

def fill_education(X_data):
    X_data['education'] = X_data['education'].fillna('primary')
    return X_data

# Now for the contact column
X_train['contact'].value_counts()

X_train.loc[X_train['contact'].isna()].describe()

X_train.loc[X_train['contact'].isna(), 'education'].value_counts()

X_train.loc[X_train['contact'].isna(), 'job'].value_counts()

# This one is harder to find; we'll just fillna
# using 'unknown' for this one

def fill_contact(X_data):
    X_data['contact'] = X_data['contact'].fillna('unknown')
    return X_data


# Next is pdays
# This column says how many days it has been since the last 
# marketing contact for this client

X_train['pdays'].plot(kind='hist')

X_train.loc[X_train['pdays'].isna()].describe()

# Hmm... previous has some interesting output, lets explore that
X_train.loc[X_train['pdays'].isna(), 'previous'].value_counts()

# According to the information about the dataset,
# a zero in the 'previous' column means that this client
# has not been contacted before! Lets put a -1 in place
# of the NaNs to indicate this importance to the model.

def fill_pdays(X_data):
    X_data['pdays'] = X_data['pdays'].fillna(-1)
    return X_data

# Lastly is poutcome

X_train['poutcome'].value_counts()

# The number of missing values in this column 
# closely matched that of pdays
# Lets check the 'previous' column

X_train.loc[X_train['poutcome'].isna(), 'previous'].value_counts()

# Since the vast majority of missing data didn't have a previous
# campaign, we can fill the data with 'nonexistent'. 

def fill_poutcome(X_data):
    X_data['poutcome'] = X_data['poutcome'].fillna('nonexistent')
    return X_data

# Lets combine all our missing data functions into a single function
def fill_missing(X_data):
    X_data = fill_job(X_data)
    X_data = fill_education(X_data)
    X_data = fill_contact(X_data)
    X_data = fill_pdays(X_data)
    X_data = fill_poutcome(X_data)
    return X_data



# Lets apply this fill missing function to our data before 
# moving on to encoding
X_train_filled = fill_missing(X_train)
X_test_filled = fill_missing(X_test)

X_train_filled.head()

# First is job
X_train_filled['job'].value_counts()

# Lots of unique values, not ordinal data
# Lets convert to no more than 5 categories

encode_job = OneHotEncoder(max_categories=5, handle_unknown='infrequent_if_exist', sparse_output=False)

# Train the encoder
encode_job.fit(X_train_filled['job'].values.reshape(-1, 1))

# Next is marital
X_train_filled['marital'].value_counts()

# Only three values; lets use two OneHotEncoded columns
# remembering to choose options for unknown values
encode_marital = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)

# Train the encoder
encode_marital.fit(X_train_filled['marital'].values.reshape(-1, 1))

# Next is education
X_train_filled['education'].value_counts()

# This is ordinal! Lets use the ordinal encoder
# We'll set any unknown values to -1
encode_education = OrdinalEncoder(categories=[['primary', 'secondary', 'tertiary']], handle_unknown='use_encoded_value', unknown_value=-1)

# Train the encoder
encode_education.fit(X_train_filled['education'].values.reshape(-1, 1))

# Next is default
X_train_filled['default'].value_counts()

# Lets make this an Ordinal column
encode_default = OrdinalEncoder(categories=[['no', 'yes']], handle_unknown='use_encoded_value', unknown_value=-1)

# Train the encoder
encode_default.fit(X_train_filled['default'].values.reshape(-1, 1))

# Next is housing
X_train_filled['housing'].value_counts()

# Lets make this an Ordinal column
encode_housing= OrdinalEncoder(categories=[['no', 'yes']], handle_unknown='use_encoded_value', unknown_value=-1)

# Train the encoder
encode_housing.fit(X_train_filled['housing'].values.reshape(-1, 1))

# Next is loan
X_train_filled['loan'].value_counts()

# Lets make this an Ordinal column
encode_loan = OrdinalEncoder(categories=[['no', 'yes']], handle_unknown='use_encoded_value', unknown_value=-1)

# Train the encoder
encode_loan.fit(X_train_filled['loan'].values.reshape(-1, 1))

# Next is contact
X_train_filled['contact'].value_counts()

# Lets use two OneHotEncoded columns
encode_contact = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)

# Train the encoder
encode_contact.fit(X_train_filled['contact'].values.reshape(-1, 1))

# Next is month
X_train_filled['month'].value_counts()

# This month seems ordinal by may not behave that way...
# Lets use ordinal for now, but consider experimenting with this!
encode_month = OrdinalEncoder(categories=[['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']], handle_unknown='use_encoded_value', unknown_value=-1)

# Train the encoder
encode_month.fit(X_train_filled['month'].values.reshape(-1, 1))

# Next is the poutcome column
X_train_filled['poutcome'].value_counts()

# Lets use OneHotEncoding for this
encode_poutcome = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)

# Train the encoder
encode_poutcome.fit(X_train_filled['poutcome'].values.reshape(-1, 1))


# Combine the encoders into a function
# Make sure to return a dataframe
def encode_categorical(X_data):
    # Separate numeric columns
    X_data_numeric = X_data.select_dtypes(include='number').reset_index()

    # Multicolumn encoders first
    job_encoded_df = pd.DataFrame(encode_job.transform(X_data['job'].values.reshape(-1, 1)), columns=encode_job.get_feature_names_out())
    marital_encoded_df = pd.DataFrame(encode_marital.transform(X_data['marital'].values.reshape(-1, 1)), columns=encode_marital.get_feature_names_out())
    contact_encoded_df = pd.DataFrame(encode_contact.transform(X_data['contact'].values.reshape(-1, 1)), columns=encode_contact.get_feature_names_out())
    poutcome_encoded_df = pd.DataFrame(encode_poutcome.transform(X_data['poutcome'].values.reshape(-1, 1)), columns=encode_poutcome.get_feature_names_out())

    # Concat all dfs together
    dfs = [X_data_numeric, job_encoded_df, marital_encoded_df, contact_encoded_df, poutcome_encoded_df]
    X_data_encoded = pd.concat(dfs, axis=1)

    # Add single column encoders
    X_data_encoded['education'] = encode_education.transform(X_data['education'].values.reshape(-1, 1))
    X_data_encoded['default'] = encode_default.transform(X_data['default'].values.reshape(-1, 1))
    X_data_encoded['housing'] = encode_housing.transform(X_data['housing'].values.reshape(-1, 1))
    X_data_encoded['loan'] = encode_loan.transform(X_data['loan'].values.reshape(-1, 1))
    X_data_encoded['month'] = encode_month.transform(X_data['month'].values.reshape(-1, 1))
    
    return X_data_encoded

# Apply the encoding function to both training and testing
X_train_encoded = encode_categorical(X_train_filled)
X_test_encoded = encode_categorical(X_test_filled)

# Check the final X_train data
X_train_encoded.head()

# Wait! Don't forget the y data!
y_train

# Create a OneHotEncoder
encode_y = OneHotEncoder(drop='first', sparse_output=False)

# Train the encoder
encode_y.fit(y_train)

# Apply it to both y_train and y_test
# Use np.ravel to reshape for logistic regression
y_train_encoded = np.ravel(encode_y.transform(y_train))
y_test_encoded = np.ravel(encode_y.transform(y_test))
y_train_encoded

# Create and train an SVC model
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=500)
model.fit(X_train_encoded, y_train_encoded)

# Check the model's balanced accuracy on the test set

y_test_pred = model.predict(X_test_encoded)
print(balanced_accuracy_score(y_test_encoded, y_test_pred))

# Check the model's balanced accuracy on the training set

y_train_pred = model.predict(X_train_encoded)
print(balanced_accuracy_score(y_train_encoded, y_train_pred))

# We overfit! Lets try varying the max depth

models = {'train_score': [], 'test_score': [], 'max_depth': []}

for depth in range(1,10):
    models['max_depth'].append(depth)
    model = RandomForestClassifier(n_estimators=500, max_depth=depth)
    model.fit(X_train_encoded, y_train_encoded)
    y_test_pred = model.predict(X_test_encoded)
    y_train_pred = model.predict(X_train_encoded)

    models['train_score'].append(balanced_accuracy_score(y_train_encoded, y_train_pred))
    models['test_score'].append(balanced_accuracy_score(y_test_encoded, y_test_pred))

models_df = pd.DataFrame(models)

models_df.plot(x='max_depth')

# it looks like the lines start to diverge a lot after 7
# Create and train a RandomForest model
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(max_depth = 7, n_estimators=100)
model.fit(X_train_encoded, y_train_encoded)

y_train_pred = model.predict(X_train_encoded)
print(balanced_accuracy_score(y_train_encoded, y_train_pred))

y_test_pred = model.predict(X_test_encoded)
print(balanced_accuracy_score(y_test_encoded, y_test_pred))



# Import the data
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import balanced_accuracy_score
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder

df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m14/datasets/bank_marketing.csv')
df.head()

# Setup X and y variables
X = df.drop(columns='y')
y = df['y'].values.reshape(-1,1)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13)
X_train.describe()

# Do what you can to improve the model!

# Explore each column with missing values to determine the best fill strategy
# First the job column
X_train['job'].value_counts()

# The job column is varied and the number of missing values is small
# It might suffice to fill the missing values with "unknown"
# We'll make a function to handle this.
def fill_job(X_data):
    X_data['job'] = X_data['job'].fillna('unknown')
    return X_data

# Education is next
X_train['education'].value_counts()

X_train.loc[X_train['education'].isna()].describe()

X_train.loc[X_train['education'].isna(), 'job'].value_counts()

# The vast majority of rows missing an education value
# have a job which wouldn't require a higher education
# Lets fillna for education with 'primary', but 'unknown'
# might be a good choice as well

def fill_education(X_data):
    X_data['education'] = X_data['education'].fillna('primary')
    return X_data

# Now for the contact column
X_train['contact'].value_counts()

X_train.loc[X_train['contact'].isna()].describe()

X_train.loc[X_train['contact'].isna(), 'education'].value_counts()

X_train.loc[X_train['contact'].isna(), 'job'].value_counts()

# This one is harder to find; we'll just fillna
# using 'unknown' for this one

def fill_contact(X_data):
    X_data['contact'] = X_data['contact'].fillna('unknown')
    return X_data


# Next is pdays
# This column says how many days it has been since the last 
# marketing contact for this client

X_train['pdays'].plot(kind='hist')

X_train.loc[X_train['pdays'].isna()].describe()

# Hmm... previous has some interesting output, lets explore that
X_train.loc[X_train['pdays'].isna(), 'previous'].value_counts()

# According to the information about the dataset,
# a zero in the 'previous' column means that this client
# has not been contacted before! Lets put a -1 in place
# of the NaNs to indicate this importance to the model.

def fill_pdays(X_data):
    X_data['pdays'] = X_data['pdays'].fillna(-1)
    return X_data

# Lastly is poutcome

X_train['poutcome'].value_counts()

# The number of missing values in this column 
# closely matched that of pdays
# Lets check the 'previous' column

X_train.loc[X_train['poutcome'].isna(), 'previous'].value_counts()

# Since the vast majority of missing data didn't have a previous
# campaign, we can fill the data with 'nonexistent'. 

def fill_poutcome(X_data):
    X_data['poutcome'] = X_data['poutcome'].fillna('nonexistent')
    return X_data

# Lets combine all our missing data functions into a single function
def fill_missing(X_data):
    X_data = fill_job(X_data)
    X_data = fill_education(X_data)
    X_data = fill_contact(X_data)
    X_data = fill_pdays(X_data)
    X_data = fill_poutcome(X_data)
    return X_data



# Lets apply this fill missing function to our data before 
# moving on to encoding
X_train_filled = fill_missing(X_train)
X_test_filled = fill_missing(X_test)

X_train_filled.head()

# First is job
X_train_filled['job'].value_counts()

# Lots of unique values, not ordinal data
# Lets convert to no more than 5 categories

encode_job = OneHotEncoder(max_categories=5, handle_unknown='infrequent_if_exist', sparse_output=False)

# Train the encoder
encode_job.fit(X_train_filled['job'].values.reshape(-1, 1))

# Next is marital
X_train_filled['marital'].value_counts()

# Only three values; lets use two OneHotEncoded columns
# remembering to choose options for unknown values
encode_marital = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)

# Train the encoder
encode_marital.fit(X_train_filled['marital'].values.reshape(-1, 1))

# Next is education
X_train_filled['education'].value_counts()

# This is ordinal! Lets use the ordinal encoder
# We'll set any unknown values to -1
encode_education = OrdinalEncoder(categories=[['primary', 'secondary', 'tertiary']], handle_unknown='use_encoded_value', unknown_value=-1)

# Train the encoder
encode_education.fit(X_train_filled['education'].values.reshape(-1, 1))

# Next is default
X_train_filled['default'].value_counts()

# Lets make this an Ordinal column
encode_default = OrdinalEncoder(categories=[['no', 'yes']], handle_unknown='use_encoded_value', unknown_value=-1)

# Train the encoder
encode_default.fit(X_train_filled['default'].values.reshape(-1, 1))

# Next is housing
X_train_filled['housing'].value_counts()

# Lets make this an Ordinal column
encode_housing= OrdinalEncoder(categories=[['no', 'yes']], handle_unknown='use_encoded_value', unknown_value=-1)

# Train the encoder
encode_housing.fit(X_train_filled['housing'].values.reshape(-1, 1))

# Next is loan
X_train_filled['loan'].value_counts()

# Lets make this an Ordinal column
encode_loan = OrdinalEncoder(categories=[['no', 'yes']], handle_unknown='use_encoded_value', unknown_value=-1)

# Train the encoder
encode_loan.fit(X_train_filled['loan'].values.reshape(-1, 1))

# Next is contact
X_train_filled['contact'].value_counts()

# Lets use two OneHotEncoded columns
encode_contact = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)

# Train the encoder
encode_contact.fit(X_train_filled['contact'].values.reshape(-1, 1))

# Next is month
X_train_filled['month'].value_counts()

# This month seems ordinal by may not behave that way...
# Lets use ordinal for now, but consider experimenting with this!
encode_month = OrdinalEncoder(categories=[['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']], handle_unknown='use_encoded_value', unknown_value=-1)

# Train the encoder
encode_month.fit(X_train_filled['month'].values.reshape(-1, 1))

# Next is the poutcome column
X_train_filled['poutcome'].value_counts()

# Lets use OneHotEncoding for this
encode_poutcome = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)

# Train the encoder
encode_poutcome.fit(X_train_filled['poutcome'].values.reshape(-1, 1))


# Combine the encoders into a function
# Make sure to return a dataframe
def encode_categorical(X_data):
    # Separate numeric columns
    X_data_numeric = X_data.select_dtypes(include='number').reset_index()

    # Multicolumn encoders first
    job_encoded_df = pd.DataFrame(encode_job.transform(X_data['job'].values.reshape(-1, 1)), columns=encode_job.get_feature_names_out())
    marital_encoded_df = pd.DataFrame(encode_marital.transform(X_data['marital'].values.reshape(-1, 1)), columns=encode_marital.get_feature_names_out())
    contact_encoded_df = pd.DataFrame(encode_contact.transform(X_data['contact'].values.reshape(-1, 1)), columns=encode_contact.get_feature_names_out())
    poutcome_encoded_df = pd.DataFrame(encode_poutcome.transform(X_data['poutcome'].values.reshape(-1, 1)), columns=encode_poutcome.get_feature_names_out())

    # Concat all dfs together
    dfs = [X_data_numeric, job_encoded_df, marital_encoded_df, contact_encoded_df, poutcome_encoded_df]
    X_data_encoded = pd.concat(dfs, axis=1)

    # Add single column encoders
    X_data_encoded['education'] = encode_education.transform(X_data['education'].values.reshape(-1, 1))
    X_data_encoded['default'] = encode_default.transform(X_data['default'].values.reshape(-1, 1))
    X_data_encoded['housing'] = encode_housing.transform(X_data['housing'].values.reshape(-1, 1))
    X_data_encoded['loan'] = encode_loan.transform(X_data['loan'].values.reshape(-1, 1))
    X_data_encoded['month'] = encode_month.transform(X_data['month'].values.reshape(-1, 1))
    
    return X_data_encoded

# Apply the encoding function to both training and testing
X_train_encoded = encode_categorical(X_train_filled)
X_test_encoded = encode_categorical(X_test_filled)

# Check the final X_train data
X_train_encoded.head()

# Wait! Don't forget the y data!
y_train

# Create a OneHotEncoder
encode_y = OneHotEncoder(drop='first', sparse_output=False)

# Train the encoder
encode_y.fit(y_train)

# Apply it to both y_train and y_test
# Use np.ravel to reshape for logistic regression
y_train_encoded = np.ravel(encode_y.transform(y_train))
y_test_encoded = np.ravel(encode_y.transform(y_test))
y_train_encoded

# Create and train an SVC model
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=500)
model.fit(X_train_encoded, y_train_encoded)

# Check the model's balanced accuracy on the test set

y_test_pred = model.predict(X_test_encoded)
print(balanced_accuracy_score(y_test_encoded, y_test_pred))

# Check the model's balanced accuracy on the training set

y_train_pred = model.predict(X_train_encoded)
print(balanced_accuracy_score(y_train_encoded, y_train_pred))

# We overfit! Lets try varying the max depth

models = {'train_score': [], 'test_score': [], 'max_depth': []}

for depth in range(1,10):
    models['max_depth'].append(depth)
    model = RandomForestClassifier(n_estimators=500, max_depth=depth)
    model.fit(X_train_encoded, y_train_encoded)
    y_test_pred = model.predict(X_test_encoded)
    y_train_pred = model.predict(X_train_encoded)

    models['train_score'].append(balanced_accuracy_score(y_train_encoded, y_train_pred))
    models['test_score'].append(balanced_accuracy_score(y_test_encoded, y_test_pred))

models_df = pd.DataFrame(models)

models_df.plot(x='max_depth')

# it looks like the lines start to diverge a lot after 7
# Create and train a RandomForest model
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(max_depth = 7, n_estimators=100)
model.fit(X_train_encoded, y_train_encoded)

y_train_pred = model.predict(X_train_encoded)
print(balanced_accuracy_score(y_train_encoded, y_train_pred))

y_test_pred = model.predict(X_test_encoded)
print(balanced_accuracy_score(y_test_encoded, y_test_pred))



# Import the data
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import balanced_accuracy_score
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder

df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m14/datasets/bank_marketing.csv')
df.head()

# Setup X and y variables
X = df.drop(columns='y')
y = df['y'].values.reshape(-1,1)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13)
X_train.describe()

# Find the percentage of null values in each column
X_train.isna().sum()/len(X_train)

# Explore each column with missing values to determine the best fill strategy
# First the job column
X_train['job'].value_counts()

# The job column is varied and the number of missing values is small
# It might suffice to fill the missing values with "unknown"
# We'll make a function to handle this.
def fill_job(X_data):
    X_data['job'] = X_data['job'].fillna('unknown')
    return X_data

# Education is next
X_train['education'].value_counts()

X_train.loc[X_train['education'].isna()].describe()

X_train.loc[X_train['education'].isna(), 'job'].value_counts()

# The vast majority of rows missing an education value
# have a job which wouldn't require a higher education
# Lets fillna for education with 'primary', but 'unknown'
# might be a good choice as well

def fill_education(X_data):
    X_data['education'] = X_data['education'].fillna('primary')
    return X_data

# Now for the contact column
X_train['contact'].value_counts()

X_train.loc[X_train['contact'].isna()].describe()

X_train.loc[X_train['contact'].isna(), 'education'].value_counts()

X_train.loc[X_train['contact'].isna(), 'job'].value_counts()

# This one is harder to find; we'll just fillna
# using 'unknown' for this one

def fill_contact(X_data):
    X_data['contact'] = X_data['contact'].fillna('unknown')
    return X_data


# Next is pdays
# This column says how many days it has been since the last 
# marketing contact for this client

X_train['pdays'].plot(kind='hist')

X_train.loc[X_train['pdays'].isna()].describe()

# Hmm... previous has some interesting output, lets explore that
X_train.loc[X_train['pdays'].isna(), 'previous'].value_counts()

# According to the information about the dataset,
# a zero in the 'previous' column means that this client
# has not been contacted before! Lets put a -1 in place
# of the NaNs to indicate this importance to the model.

def fill_pdays(X_data):
    X_data['pdays'] = X_data['pdays'].fillna(-1)
    return X_data

# Lastly is poutcome

X_train['poutcome'].value_counts()

# The number of missing values in this column 
# closely matched that of pdays
# Lets check the 'previous' column

X_train.loc[X_train['poutcome'].isna(), 'previous'].value_counts()

# Since the vast majority of missing data didn't have a previous
# campaign, we can fill the data with 'nonexistent'. 

def fill_poutcome(X_data):
    X_data['poutcome'] = X_data['poutcome'].fillna('nonexistent')
    return X_data

# Lets combine all our missing data functions into a single function
def fill_missing(X_data):
    X_data = fill_job(X_data)
    X_data = fill_education(X_data)
    X_data = fill_contact(X_data)
    X_data = fill_pdays(X_data)
    X_data = fill_poutcome(X_data)
    return X_data



# Lets apply this fill missing function to our data before 
# moving on to encoding
X_train_filled = fill_missing(X_train)
X_test_filled = fill_missing(X_test)

X_train_filled.head()

# First is job
X_train_filled['job'].value_counts()

# Lots of unique values, not ordinal data
# Lets convert to no more than 5 categories

encode_job = OneHotEncoder(max_categories=5, handle_unknown='infrequent_if_exist', sparse_output=False)

# Train the encoder
encode_job.fit(X_train_filled['job'].values.reshape(-1, 1))

# Next is marital
X_train_filled['marital'].value_counts()

# Only three values; lets use two OneHotEncoded columns
# remembering to choose options for unknown values
encode_marital = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)

# Train the encoder
encode_marital.fit(X_train_filled['marital'].values.reshape(-1, 1))

# Next is education
X_train_filled['education'].value_counts()

# This is ordinal! Lets use the ordinal encoder
# We'll set any unknown values to -1
encode_education = OrdinalEncoder(categories=[['primary', 'secondary', 'tertiary']], handle_unknown='use_encoded_value', unknown_value=-1)

# Train the encoder
encode_education.fit(X_train_filled['education'].values.reshape(-1, 1))

# Next is default
X_train_filled['default'].value_counts()

# Lets make this an Ordinal column
encode_default = OrdinalEncoder(categories=[['no', 'yes']], handle_unknown='use_encoded_value', unknown_value=-1)

# Train the encoder
encode_default.fit(X_train_filled['default'].values.reshape(-1, 1))

# Next is housing
X_train_filled['housing'].value_counts()

# Lets make this an Ordinal column
encode_housing= OrdinalEncoder(categories=[['no', 'yes']], handle_unknown='use_encoded_value', unknown_value=-1)

# Train the encoder
encode_housing.fit(X_train_filled['housing'].values.reshape(-1, 1))

# Next is loan
X_train_filled['loan'].value_counts()

# Lets make this an Ordinal column
encode_loan = OrdinalEncoder(categories=[['no', 'yes']], handle_unknown='use_encoded_value', unknown_value=-1)

# Train the encoder
encode_loan.fit(X_train_filled['loan'].values.reshape(-1, 1))

# Next is contact
X_train_filled['contact'].value_counts()

# Lets use two OneHotEncoded columns
encode_contact = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)

# Train the encoder
encode_contact.fit(X_train_filled['contact'].values.reshape(-1, 1))

# Next is month
X_train_filled['month'].value_counts()

# This month seems ordinal by may not behave that way...
# Lets use ordinal for now, but consider experimenting with this!
encode_month = OrdinalEncoder(categories=[['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']], handle_unknown='use_encoded_value', unknown_value=-1)

# Train the encoder
encode_month.fit(X_train_filled['month'].values.reshape(-1, 1))

# Next is the poutcome column
X_train_filled['poutcome'].value_counts()

# Lets use OneHotEncoding for this
encode_poutcome = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)

# Train the encoder
encode_poutcome.fit(X_train_filled['poutcome'].values.reshape(-1, 1))


# Combine the encoders into a function
# Make sure to return a dataframe
def encode_categorical(X_data):
    # Separate numeric columns
    X_data_numeric = X_data.select_dtypes(include='number').reset_index()

    # Multicolumn encoders first
    job_encoded_df = pd.DataFrame(encode_job.transform(X_data['job'].values.reshape(-1, 1)), columns=encode_job.get_feature_names_out())
    marital_encoded_df = pd.DataFrame(encode_marital.transform(X_data['marital'].values.reshape(-1, 1)), columns=encode_marital.get_feature_names_out())
    contact_encoded_df = pd.DataFrame(encode_contact.transform(X_data['contact'].values.reshape(-1, 1)), columns=encode_contact.get_feature_names_out())
    poutcome_encoded_df = pd.DataFrame(encode_poutcome.transform(X_data['poutcome'].values.reshape(-1, 1)), columns=encode_poutcome.get_feature_names_out())

    # Concat all dfs together
    dfs = [X_data_numeric, job_encoded_df, marital_encoded_df, contact_encoded_df, poutcome_encoded_df]
    X_data_encoded = pd.concat(dfs, axis=1)

    # Add single column encoders
    X_data_encoded['education'] = encode_education.transform(X_data['education'].values.reshape(-1, 1))
    X_data_encoded['default'] = encode_default.transform(X_data['default'].values.reshape(-1, 1))
    X_data_encoded['housing'] = encode_housing.transform(X_data['housing'].values.reshape(-1, 1))
    X_data_encoded['loan'] = encode_loan.transform(X_data['loan'].values.reshape(-1, 1))
    X_data_encoded['month'] = encode_month.transform(X_data['month'].values.reshape(-1, 1))
    
    return X_data_encoded

# Apply the encoding function to both training and testing
X_train_encoded = encode_categorical(X_train_filled)
X_test_encoded = encode_categorical(X_test_filled)

# Check the final X_train data
X_train_encoded.head()

# Wait! Don't forget the y data!
y_train

# Create a OneHotEncoder
encode_y = OneHotEncoder(drop='first', sparse_output=False)

# Train the encoder
encode_y.fit(y_train)

# Apply it to both y_train and y_test
# Use np.ravel to reshape for logistic regression
y_train_encoded = np.ravel(encode_y.transform(y_train))
y_test_encoded = np.ravel(encode_y.transform(y_test))
y_train_encoded

# Create and train an SVC model
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=500)
model.fit(X_train_encoded, y_train_encoded)

# Check the model's balanced accuracy on the test set

y_test_pred = model.predict(X_test_encoded)
print(balanced_accuracy_score(y_test_encoded, y_test_pred))

# Check the model's balanced accuracy on the training set

y_train_pred = model.predict(X_train_encoded)
print(balanced_accuracy_score(y_train_encoded, y_train_pred))

# We overfit! Lets try varying the max depth

models = {'train_score': [], 'test_score': [], 'max_depth': []}

for depth in range(1,10):
    models['max_depth'].append(depth)
    model = RandomForestClassifier(n_estimators=500, max_depth=depth)
    model.fit(X_train_encoded, y_train_encoded)
    y_test_pred = model.predict(X_test_encoded)
    y_train_pred = model.predict(X_train_encoded)

    models['train_score'].append(balanced_accuracy_score(y_train_encoded, y_train_pred))
    models['test_score'].append(balanced_accuracy_score(y_test_encoded, y_test_pred))

models_df = pd.DataFrame(models)

models_df.plot(x='max_depth')

# it looks like the lines start to diverge a lot after 7
# Create and train a RandomForest model
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(max_depth = 7, n_estimators=100)
model.fit(X_train_encoded, y_train_encoded)

y_train_pred = model.predict(X_train_encoded)
print(balanced_accuracy_score(y_train_encoded, y_train_pred))

y_test_pred = model.predict(X_test_encoded)
print(balanced_accuracy_score(y_test_encoded, y_test_pred))



# Import the data
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import balanced_accuracy_score
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder

df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m14/datasets/bank_marketing.csv')
df.head()

# Setup X and y variables
X = df.drop(columns='y')
y = df['y'].values.reshape(-1,1)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13)
X_train.describe()

# Find the percentage of null values in each column



# Explore each column with missing values to determine the best fill strategy
# First the job column



# The job column is varied and the number of missing values is small
# It might suffice to fill the missing values with "unknown"
# We'll make a function to handle this.



# Education is next









# The vast majority of rows missing an education value
# have a job which wouldn't require a higher education
# Lets fillna for education with 'primary', but 'unknown'
# might be a good choice as well




# Now for the contact column












# This one is harder to find; we'll just fillna
# using 'unknown' for this one




# Next is pdays
# This column says how many days it has been since the last 
# marketing contact for this client






# Hmm... previous has some interesting output, lets explore that



# According to the information about the dataset,
# a zero in the 'previous' column means that this client
# has not been contacted before! Lets put a -1 in place
# of the NaNs to indicate this importance to the model.




# Lastly is poutcome




# The number of missing values in this column 
# closely matched that of pdays
# Lets check the 'previous' column



# Since the vast majority of missing data didn't have a previous
# campaign, we can fill the data with 'nonexistent'. 




# Lets combine all our missing data functions into a single function





# Lets apply this fill missing function to our data before 
# moving on to encoding






# First is job



# Lots of unique values, not ordinal data
# Lets convert to no more than 5 categories



# Train the encoder



# Next is marital



# Only three values; lets use two OneHotEncoded columns
# remembering to choose options for unknown values


# Train the encoder



# Next is education



# This is ordinal! Lets use the ordinal encoder
# We'll set any unknown values to -1


# Train the encoder



# Next is default



# Lets make this an Ordinal column


# Train the encoder



# Next is housing



# Lets make this an Ordinal column


# Train the encoder



# Next is loan



# Lets make this an Ordinal column


# Train the encoder



# Next is contact



# Lets use two OneHotEncoded columns


# Train the encoder



# Next is month



# This month seems ordinal by may not behave that way...
# Lets use ordinal for now, but consider experimenting with this!


# Train the encoder



# Next is the poutcome column



# Lets use OneHotEncoding for this


# Train the encoder



# Combine the encoders into a function
# Make sure to return a dataframe
def encode_categorical(X_data):
    # Separate numeric columns
    X_data_numeric = X_data.select_dtypes(include='number').reset_index()

    # Multicolumn encoders first
    job_encoded_df = pd.DataFrame(encode_job.transform(X_data['job'].values.reshape(-1, 1)), columns=encode_job.get_feature_names_out())
    marital_encoded_df = pd.DataFrame(encode_marital.transform(X_data['marital'].values.reshape(-1, 1)), columns=encode_marital.get_feature_names_out())
    contact_encoded_df = pd.DataFrame(encode_contact.transform(X_data['contact'].values.reshape(-1, 1)), columns=encode_contact.get_feature_names_out())
    poutcome_encoded_df = pd.DataFrame(encode_poutcome.transform(X_data['poutcome'].values.reshape(-1, 1)), columns=encode_poutcome.get_feature_names_out())

    # Concat all dfs together
    dfs = [X_data_numeric, job_encoded_df, marital_encoded_df, contact_encoded_df, poutcome_encoded_df]
    X_data_encoded = pd.concat(dfs, axis=1)

    # Add single column encoders
    X_data_encoded['education'] = encode_education.transform(X_data['education'].values.reshape(-1, 1))
    X_data_encoded['default'] = encode_default.transform(X_data['default'].values.reshape(-1, 1))
    X_data_encoded['housing'] = encode_housing.transform(X_data['housing'].values.reshape(-1, 1))
    X_data_encoded['loan'] = encode_loan.transform(X_data['loan'].values.reshape(-1, 1))
    X_data_encoded['month'] = encode_month.transform(X_data['month'].values.reshape(-1, 1))
    
    return X_data_encoded

# Apply the encoding function to both training and testing



# Check the final X_train data



# Wait! Don't forget the y data!



# Create a OneHotEncoder


# Train the encoder


# Apply it to both y_train and y_test
# Use np.ravel to reshape for logistic regression



# Create and train an SVC model
from sklearn.ensemble import RandomForestClassifier



# Check the model's balanced accuracy on the test set



# Check the model's balanced accuracy on the training set



# We overfit! Lets try varying the max depth





# it looks like the lines start to diverge a lot after 7
# Create and train a RandomForest model
from sklearn.ensemble import RandomForestClassifier




import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings("ignore")

df = pd.read_csv('https://static.bc-edx.com/mbc/ai/m5/datasets/numeric_bank.csv')
df.head()

target = df["y"]
target_names = ["negative", "positive"]

data = df.drop("y", axis=1)
data.head()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=42)

# Create three KNN classifiers
from sklearn.neighbors import KNeighborsClassifier
untuned_model = KNeighborsClassifier()
grid_tuned_model = KNeighborsClassifier()
random_tuned_model = KNeighborsClassifier()

## Train a model without tuning
from sklearn.metrics import classification_report
untuned_model.fit(X_train, y_train)
untuned_y_pred = untuned_model.predict(X_test)
print(classification_report(y_test, untuned_y_pred,
                            target_names=target_names))

# Create the grid search estimator along with a parameter object containing the values to adjust.
# Try adjusting n_neighbors with values of 1 through 19. Adjust leaf_size by using 10, 50, 100, and 500.
# Include both uniform and distance options for weights.
from sklearn.model_selection import GridSearchCV
param_grid = {
    'n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15, 17, 19],
    'weights': ['uniform', 'distance'],
    'leaf_size': [10, 50, 100, 500]
}
grid_clf = GridSearchCV(grid_tuned_model, param_grid, verbose=3)

# Fit the model by using the grid search estimator.
# This will take the KNN model and try each combination of parameters.
grid_clf.fit(X_train, y_train)

# List the best parameters for this dataset
print(grid_clf.best_params_)

# Print the classification report for the best model
grid_y_pred = grid_clf.predict(X_test)
print(classification_report(y_test, grid_y_pred,
                            target_names=target_names))

# Create the parameter object for the randomized search estimator.
# Try adjusting n_neighbors with values of 1 through 19. 
# Adjust leaf_size by using a range from 1 to 500.
# Include both uniform and distance options for weights.
param_grid = {
    'n_neighbors': np.arange(1,20,2),
    'weights': ['uniform', 'distance'],
    'leaf_size': np.arange(1, 500)
}
param_grid

# Create the randomized search estimator
from sklearn.model_selection import RandomizedSearchCV
random_clf = RandomizedSearchCV(random_tuned_model, param_grid, random_state=0, verbose=3)

# Fit the model by using the randomized search estimator.
random_clf.fit(X_train, y_train)

# List the best parameters for this dataset
print(random_clf.best_params_)

# Make predictions with the hypertuned model
random_tuned_pred = random_clf.predict(X_test)

# Calculate the classification report
print(classification_report(y_test, random_tuned_pred,
                            target_names=target_names))

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings("ignore")

df = pd.read_csv('https://static.bc-edx.com/mbc/ai/m5/datasets/numeric_bank.csv')
df.head()

target = df["y"]
target_names = ["negative", "positive"]

data = df.drop("y", axis=1)
feature_names = data.columns
data.head()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=42)

# Create three KNN classifiers
from sklearn.neighbors import KNeighborsClassifier
untuned_model = KNeighborsClassifier()
grid_tuned_model = KNeighborsClassifier()
random_tuned_model = KNeighborsClassifier()

from sklearn.metrics import classification_report
## Train a model without tuning and print the classification report


# Create the grid search estimator along with a parameter object containing the values to adjust.
# Try adjusting n_neighbors with values of 1 through 19. Adjust leaf_size by using 10, 50, 100, and 500.
# Include both uniform and distance options for weights.
from sklearn.model_selection import GridSearchCV



# Fit the model by using the grid search estimator.
# This will take the KNN model and try each combination of parameters.



# List the best parameters for this dataset



# Print the classification report for the best model



# Create the parameter object for the randomized search estimator.
# Try adjusting n_neighbors with values of 1 through 19. 
# Adjust leaf_size by using a range from 1 to 500.
# Include both uniform and distance options for weights.



# Create the randomized search estimator
from sklearn.model_selection import RandomizedSearchCV



# Fit the model by using the randomized search estimator.



# List the best parameters for this dataset



# Make predictions with the hypertuned model



# Calculate the classification report



# Import modules
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Generate Data
X, y = make_blobs(n_samples=[5000, 50], random_state=1, cluster_std=7)

# Convert ndarray to pandas datatypes
X = pd.DataFrame(X)
y = pd.Series(y)

# Plot data
plt.scatter(
    x=X[0],
    y=X[1],
    c=y)
plt.show()

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

# Count distinct values
y_train.value_counts()

# Import RandomUnderSampler from imblearn
from imblearn.under_sampling import RandomUnderSampler

# Instantiate the RandomUnderSampler instance
rus = RandomUnderSampler(random_state=1)

# Fit the data to the model
X_resampled, y_resampled = rus.fit_resample(X_train, y_train)

# Count distinct resampled values
y_resampled.value_counts()

# Instantiate an initial RamdonForestClassifier instance
model = RandomForestClassifier()

# Fit the initial model based the training data
model.fit(X_train, y_train)

# Instantiate a second RamdonForestClassifier instance
model_resampled = RandomForestClassifier()

# Fit the second model based the resampled data
model_resampled.fit(X_resampled, y_resampled)

# Make predictions using the initial model
y_pred = model.predict(X_test)

# Make predictions using the model based on the resampled data
y_pred_resampled = model_resampled.predict(X_test)

# Plot the data using the original y_test information
plt.scatter(
    x=X_test[0],
    y=X_test[1],
    c=y_test)
plt.show()

# Plot the data using the predictions based on the original test data
plt.scatter(
    x=X_test[0],
    y=X_test[1],
    c=y_pred)
plt.show()

# Plot the data using the predictions based on the resampled test data
plt.scatter(
    x=X_test[0],
    y=X_test[1],
    c=y_pred_resampled)
plt.show()

# Print classification report
print(classification_report(y_test, y_pred))
print(classification_report(y_test, y_pred_resampled))

# Import RandomOverSampler from imblearn
from imblearn.over_sampling import RandomOverSampler

# Instantiate the RandomOverSampler instance
random_oversampler = RandomOverSampler(random_state=1)

# Fit the data to the model
X_resampled, y_resampled = random_oversampler.fit_resample(X_train, y_train)

# Count distinct values
y_resampled.value_counts()

# Create a RandomForestClassifier instance and fit it to the original data
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Create a RandomForestClassifier instance and fit it to the resampled data
model_resampled = RandomForestClassifier()
model_resampled.fit(X_resampled, y_resampled)

# Make predictions for testing features
y_pred = model.predict(X_test)
y_pred_resampled = model_resampled.predict(X_test)

# Print the classification reports for the two models
print(classification_report(y_test, y_pred))
print(classification_report(y_test, y_pred_resampled))



# Import modules
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Generate Data
X, y = make_blobs(n_samples=[5000, 50], random_state=1, cluster_std=7)

# Convert ndarray to pandas datatypes
X = pd.DataFrame(X)
y = pd.Series(y)

# Plot data
plt.scatter(
    x=X[0],
    y=X[1],
    c=y)
plt.show()

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

# Count distinct values
y_train.value_counts()

# Import RandomUnderSampler from imblearn
from imblearn.under_sampling import RandomUnderSampler

# Instantiate the RandomUnderSampler instance
rus = RandomUnderSampler(random_state=1)

# Fit the data to the model


# Count distinct resampled values


# Instantiate an initial RamdonForestClassifier instance


# Fit the initial model based the training data


# Instantiate a second RamdonForestClassifier instance


# Fit the second model based the resampled data


# Make predictions using the initial model


# Make predictions using the model based on the resampled data


# Plot the data using the original y_test information
plt.scatter(
    x=X_test[0],
    y=X_test[1],
    c=y_test)
plt.show()

# Plot the data using the predictions based on the original test data
plt.scatter(
    x=X_test[0],
    y=X_test[1],
    c=y_pred)
plt.show()

# Plot the data using the predictions based on the resampled test data
plt.scatter(
    x=X_test[0],
    y=X_test[1],
    c=y_pred_resampled)
plt.show()

# Print classification report
print(classification_report(y_test, y_pred))
print(classification_report(y_test, y_pred_resampled))

# Import RandomOverSampler from imblearn
from imblearn.over_sampling import RandomOverSampler

# Instantiate the RandomOverSampler instance
random_oversampler = RandomOverSampler(random_state=1)

# Fit the data to the model


# Count distinct values


# Create a RandomForestClassifier instance and fit it to the original data


# Create a RandomForestClassifier instance and fit it to the resampled data


# Make predictions for testing features


# Print the classification reports for the two models




# Import modules
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler

# Read the CSV file into a Pandas DataFrame
bank_data_df = pd.read_csv('../Resources/bank.csv')

# Review the DataFrame
bank_data_df.head()

# Split the features and target data
y = bank_data_df['y']
X = bank_data_df.drop(columns='y')

# Encode the features dataset's categorical variables using get_dummies
X = pd.get_dummies(X)

# Review the features DataFrame
X.head()

# Split data into training and testing datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

# Review the distinct values from y
y_train.value_counts()

# Instantiate a StandardScaler instance
scaler = StandardScaler()

# Fit the training data to the standard scaler
X_scaler = scaler.fit(X_train)

# Transform the training data using the scaler
X_train_scaled = X_scaler.transform(X_train)

# Transform the testing data using the scaler
X_test_scaled = X_scaler.transform(X_test)

# Import the RandomForestClassifier from sklearn
from sklearn.ensemble import RandomForestClassifier

# Instantiate a RandomForestClassifier instance
model = RandomForestClassifier()

# Fit the traning data to the model
model.fit(X_train_scaled, y_train)

# Predict labels for original scaled testing features
y_pred = model.predict(X_test_scaled)

# Import RandomUnderSampler from imblearn
from imblearn.under_sampling import RandomUnderSampler

# Instantiate a RandomUnderSampler instance
rus = RandomUnderSampler(random_state=1)

# Fit the training data to the random undersampler model
X_undersampled, y_undersampled = rus.fit_resample(X_train_scaled, y_train)

# Count distinct values for the resampled target data
y_undersampled.value_counts()

# Instantiate a new RandomForestClassier model
model_undersampled = RandomForestClassifier()

# Fit the undersampled data the new model
model_undersampled.fit(X_undersampled, y_undersampled)

# Predict labels for oversampled testing features
y_pred_undersampled = model_undersampled.predict(X_test_scaled)

# Print classification reports
print(f"Classification Report - Original Data")
print(classification_report(y_test, y_pred))
print("---------")
print(f"Classification Report - Undersampled Data")
print(classification_report(y_test, y_pred_undersampled))

# Import RandomOverSampler from imblearn
from imblearn.over_sampling import RandomOverSampler

# Instantiate a RandomOversampler instance
ros = RandomOverSampler(random_state=1)

# Fit the training data to the `RandomOverSampler` model
X_oversampled, y_oversampled = ros.fit_resample(X_train_scaled, y_train)

# Count distinct values
y_oversampled.value_counts()

# Instantiate a new RandomForestClassier model
model_oversampled = RandomForestClassifier()

# Fit the oversampled data the new model
model_oversampled.fit(X_oversampled, y_oversampled)

# Predict labels for oversampled testing features
y_pred_oversampled = model_oversampled.predict(X_test_scaled)

# Print classification reports
print(f"Classification Report - Original Data")
print(classification_report(y_test, y_pred))
print("---------")
print(f"Classification Report - Undersampled Data")
print(classification_report(y_test, y_pred_undersampled))
print("---------")
print(f"Classification Report - Oversampled Data")
print(classification_report(y_test, y_pred_oversampled))

# Import ClusterCentroids from imblearn
from imblearn.under_sampling import ClusterCentroids

# Instantiate a ClusterCentroids instance
cc_sampler = ClusterCentroids(random_state=1)

# Fit the training data to the cluster centroids model
X_resampled, y_resampled = cc_sampler.fit_resample(X_train_scaled, y_train)

# Count distinct values for the resampled target data
y_resampled.value_counts()

# Instantiate a new RandomForestClassier model
cc_model = RandomForestClassifier()

# Fit the resampled data the new model
cc_model.fit(X_resampled, y_resampled)

# Predict labels for resampled testing features
cc_y_pred = cc_model.predict(X_test_scaled)

# Print classification reports
print(f"Classification Report - Original Data")
print(classification_report(y_test, y_pred))
print("---------")
print(f"Classification Report - Resampled Data - CentroidClusters")
print(classification_report(y_test, cc_y_pred))

# Import SMOTE from imblearn
from imblearn.over_sampling import SMOTE

# Instantiate the SMOTE instance 
# Set the sampling_strategy parameter equal to auto
smote_sampler = SMOTE(random_state=1, sampling_strategy='auto')

# Fit the training data to the smote_sampler model
X_resampled, y_resampled = smote_sampler.fit_resample(X_train_scaled, y_train)

# Count distinct values for the resampled target data
y_resampled.value_counts()

# Instantiate a new RandomForestClassier model 
smote_model = RandomForestClassifier()

# Fit the resampled data to the new model
smote_model.fit(X_resampled, y_resampled)

# Predict labels for resampled testing features
smote_y_pred = smote_model.predict(X_test_scaled)

# Print classification reports
print(f"Classification Report - Original Data")
print(classification_report(y_test, y_pred))
print("---------")
print(f"Classification Report - Resampled Data - SMOTE")
print(classification_report(y_test, smote_y_pred))

# Import SMOTEEN from imblearn
from imblearn.combine import SMOTEENN

# Instantiate the SMOTEENN instance
smote_enn = SMOTEENN(random_state=1)

# Fit the model to the training data
X_resampled, y_resampled = smote_enn.fit_resample(X_train_scaled, y_train)

# Instantiate a new RandomForestClassier model
smoteenn_model = RandomForestClassifier()

# Fit the resampled data the new model
smoteenn_model.fit(X_resampled, y_resampled)

# Predict labels for resampled testing features
smoteenn_y_pred = smoteenn_model.predict(X_test_scaled)

# Print classification reports
print(f"Classification Report - Original Data")
print(classification_report(y_test, y_pred))
print("---------")
print(f"Classification Report - Resampled Data - SMOTEENN")
print(classification_report(y_test, smoteenn_y_pred))

import pandas as pd
from path import Path
import pandas as pd
from collections import Counter

data = Path('../Resources/cc_default.csv')
df = pd.read_excel(data, skiprows=[0])

x_cols = [i for i in df.columns if i != 'default_next_month']
X = df[x_cols]
y = df['default_next_month']

# train-test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

Counter(y_train)

from imblearn.over_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=1)
X_resampled, y_resampled = ros.fit_resample(X_train, y_train)
Counter(y_resampled)

# Logistic regression using random oversampled data
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(random_state=1)
model.fit(X_resampled, y_resampled)

# Display the confusion matrix
from sklearn.metrics import confusion_matrix
y_pred = model.predict(X_test)
confusion_matrix(y_test, y_pred)

from sklearn.metrics import balanced_accuracy_score
balanced_accuracy_score(y_test, y_pred)

# Print the imbalanced classification report
from imblearn.metrics import classification_report_imbalanced
print(classification_report_imbalanced(y_test, y_pred))

from imblearn.over_sampling import SMOTE
X_resampled, y_resampled = SMOTE(random_state=1, ratio=1.0).fit_resample(X_train, y_train)
from collections import Counter
Counter(y_resampled)

model = LogisticRegression(random_state=1)
model.fit(X_resampled, y_resampled)

# Calculated the balanced accuracy score
y_pred = model.predict(X_test)
balanced_accuracy_score(y_test, y_pred)

# Display the confusion matrix
confusion_matrix(y_test, y_pred)

# Print the imbalanced classification report
print(classification_report_imbalanced(y_test, y_pred))



# Import modules
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler

# Read the CSV file into a Pandas DataFrame
bank_data_df = pd.read_csv('../Resources/bank.csv')

# Review the DataFrame
bank_data_df.head()

# Split the features and target data
y = bank_data_df['y']
X = bank_data_df.drop(columns='y')

# Encode the features dataset's categorical variables using get_dummies
X = pd.get_dummies(X)

# Review the features DataFrame
X.head()

# Split data into training and testing datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

# Review the distinct values from y
y_train.value_counts()

# Instantiate a StandardScaler instance
scaler = StandardScaler()

# Fit the training data to the standard scaler
X_scaler = scaler.fit(X_train)

# Transform the training data using the scaler
X_train_scaled = X_scaler.transform(X_train)

# Transform the testing data using the scaler
X_test_scaled = X_scaler.transform(X_test)

# Import the RandomForestClassifier from sklearn
from sklearn.ensemble import RandomForestClassifier

# Instantiate a RandomForestClassifier instance
model = RandomForestClassifier()

# Fit the traning data to the model
model.fit(X_train_scaled, y_train)

# Predict labels for original scaled testing features
y_pred = model.predict(X_test_scaled)

# Import RandomUnderSampler from imblearn
from imblearn.under_sampling import RandomUnderSampler

# Instantiate a RandomUnderSampler instance


# Fit the training data to the random undersampler model


# Count distinct values for the resampled target data


# Instantiate a new RandomForestClassier model


# Fit the undersampled data the new model


# Predict labels for oversampled testing features


# Print classification reports
print(f"Classification Report - Original Data")
print(classification_report(y_test, y_pred))
print("---------")
print(f"Classification Report - Undersampled Data")
print(classification_report(y_test, y_pred_undersampled))

# Import RandomOverSampler from imblearn
from imblearn.over_sampling import RandomOverSampler

# Instantiate a RandomOversampler instance


# Fit the training data to the `RandomOverSampler` model


# Count distinct values


# Instantiate a new RandomForestClassier model
model_oversampled = RandomForestClassifier()

# Fit the oversampled data the new model


# Predict labels for oversampled testing features


# Print classification reports
print(f"Classification Report - Original Data")
print(classification_report(y_test, y_pred))
print("---------")
print(f"Classification Report - Undersampled Data")
print(classification_report(y_test, y_pred_undersampled))
print("---------")
print(f"Classification Report - Oversampled Data")
print(classification_report(y_test, y_pred_oversampled))

# Import ClusterCentroids from imblearn
from imblearn.under_sampling import ClusterCentroids

# Instantiate a ClusterCentroids instance
cc_sampler = ClusterCentroids(random_state=1)

# Fit the training data to the cluster centroids model


# Count distinct values for the resampled target data


# Instantiate a new RandomForestClassier model

# Fit the resampled data the new model


# Predict labels for resampled testing features


# Print classification reports
print(f"Classification Report - Original Data")
print(classification_report(y_test, y_pred))
print("---------")
print(f"Classification Report - Resampled Data - CentroidClusters")
print(classification_report(y_test, cc_y_pred))

# Import SMOTE from imblearn
from imblearn.over_sampling import SMOTE

# Instantiate the SMOTE instance 
# Set the sampling_strategy parameter equal to auto
smote_sampler = SMOTE(random_state=1, sampling_strategy='auto')

# Fit the training data to the smote_sampler model


# Count distinct values for the resampled target data


# Instantiate a new RandomForestClassier model 


# Fit the resampled data to the new model


# Predict labels for resampled testing features


# Print classification reports
print(f"Classification Report - Original Data")
print(classification_report(y_test, y_pred))
print("---------")
print(f"Classification Report - Resampled Data - SMOTE")
print(classification_report(y_test, smote_y_pred))

# Import SMOTEEN from imblearn
from imblearn.combine import SMOTEENN

# Instantiate the SMOTEENN instance
smote_enn = SMOTEENN(random_state=1)

# Fit the model to the training data


# Instantiate a new RandomForestClassier model


# Fit the resampled data the new model


# Predict labels for resampled testing features


# Print classification reports
print(f"Classification Report - Original Data")
print(classification_report(y_test, y_pred))
print("---------")
print(f"Classification Report - Resampled Data - SMOTEENN")
print(classification_report(y_test, smoteenn_y_pred))

# Import the data
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import balanced_accuracy_score
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder

from ml_utils import train_test_split_marketing,\
    fill_missing,\
    build_encoders,\
    encode_categorical,\
    build_target_encoder,\
    encode_target

df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m14/datasets/bank_marketing.csv')
df.head()

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split_marketing(df)
X_train.describe()

# Fill the missing values using the imported function
X_train_filled = fill_missing(X_train)
X_test_filled = fill_missing(X_test)
X_train_filled.head()

# Create the encoders for categorical variables (use X_train_filled)
encoders = build_encoders(X_train_filled)
encoders

# Encode X_train_filled and X_test_filled
X_train_encoded = encode_categorical(X_train_filled, encoders)
X_test_encoded = encode_categorical(X_test_filled, encoders)

X_train_encoded.head()

# Encode y_train and y_test
y_encoder = build_target_encoder(y_train)
y_train_encoded = encode_target(y_train, y_encoder)
y_test_encoded = encode_target(y_test, y_encoder)

from sklearn.decomposition import PCA

pca_model = PCA(n_components = 10)
pca_model.fit(X_train_encoded)

X_train_pca = pd.DataFrame(pca_model.transform(X_train_encoded))
X_test_pca = pd.DataFrame(pca_model.transform(X_test_encoded))
X_train_pca

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=100)
model.fit(X_train_pca, y_train_encoded)

y_test_pred = model.predict(X_test_pca)
print(balanced_accuracy_score(y_test_encoded, y_test_pred))

models = {'train_score': [], 'test_score': [], 'max_depth': []}

for depth in range(1,10):
    models['max_depth'].append(depth)
    model = RandomForestClassifier(n_estimators=100, max_depth=depth)
    model.fit(X_train_pca, y_train_encoded)
    y_test_pred = model.predict(X_test_pca)
    y_train_pred = model.predict(X_train_pca)

    models['train_score'].append(balanced_accuracy_score(y_train_encoded, y_train_pred))
    models['test_score'].append(balanced_accuracy_score(y_test_encoded, y_test_pred))

models_df = pd.DataFrame(models)

models_df.plot(x='max_depth')

models = {'train_score': [], 'test_score': [], 'n_estimators': []}

for n in [50, 100, 500, 1000]:
    models['n_estimators'].append(n)
    model = RandomForestClassifier(n_estimators=n, max_depth=7)
    model.fit(X_train_pca, y_train_encoded)
    y_test_pred = model.predict(X_test_pca)
    y_train_pred = model.predict(X_train_pca)

    models['train_score'].append(balanced_accuracy_score(y_train_encoded, y_train_pred))
    models['test_score'].append(balanced_accuracy_score(y_test_encoded, y_test_pred))

models_df = pd.DataFrame(models)

models_df.plot(x='n_estimators')

from sklearn.neighbors import KNeighborsClassifier


from sklearn.model_selection import RandomizedSearchCV
param_grid = {
    'n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15, 17, 19],
    'weights': ['uniform', 'distance'],
    'leaf_size': [10, 50, 100, 500]
}
random_knn = RandomizedSearchCV(KNeighborsClassifier(), param_grid, verbose=3)

random_knn.fit(X_train_pca, y_train_encoded)

y_pred = random_knn.predict(X_test_pca)
print(balanced_accuracy_score(y_test_encoded, y_pred))

model = RandomForestClassifier(n_estimators=100, max_depth=7, random_state=13)
model.fit(X_train_encoded, y_train_encoded)
y_pred = model.predict(X_test_encoded)
print(balanced_accuracy_score(y_test_encoded, y_pred))



# Import the data
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import balanced_accuracy_score
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder

df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m14/datasets/bank_marketing.csv')
df.head()

# Setup X and y variables
X = df.drop(columns='y')
y = df['y'].values.reshape(-1,1)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13)
X_train.describe()

# Find the percentage of null values in each column
X_train.isna().sum()/len(X_train)

# Explore each column with missing values to determine the best fill strategy
# First the job column
X_train['job'].value_counts()

# The job column is varied and the number of missing values is small
# It might suffice to fill the missing values with "unknown"
# We'll make a function to handle this.
def fill_job(X_data):
    X_data['job'] = X_data['job'].fillna('unknown')
    return X_data

# Education is next
X_train['education'].value_counts()

X_train.loc[X_train['education'].isna()].describe()

X_train.loc[X_train['education'].isna(), 'job'].value_counts()

# The vast majority of rows missing an education value
# have a job which wouldn't require a higher education
# Lets fillna for education with 'primary', but 'unknown'
# might be a good choice as well

def fill_education(X_data):
    X_data['education'] = X_data['education'].fillna('primary')
    return X_data

# Now for the contact column
X_train['contact'].value_counts()

X_train.loc[X_train['contact'].isna()].describe()

X_train.loc[X_train['contact'].isna(), 'education'].value_counts()

X_train.loc[X_train['contact'].isna(), 'job'].value_counts()

# This one is harder to find; we'll just fillna
# using 'unknown' for this one

def fill_contact(X_data):
    X_data['contact'] = X_data['contact'].fillna('unknown')
    return X_data


# Next is pdays
# This column says how many days it has been since the last 
# marketing contact for this client

X_train['pdays'].plot(kind='hist')

X_train.loc[X_train['pdays'].isna()].describe()

# Hmm... previous has some interesting output, lets explore that
X_train.loc[X_train['pdays'].isna(), 'previous'].value_counts()

# According to the information about the dataset,
# a zero in the 'previous' column means that this client
# has not been contacted before! Lets put a -1 in place
# of the NaNs to indicate this importance to the model.

def fill_pdays(X_data):
    X_data['pdays'] = X_data['pdays'].fillna(-1)
    return X_data

# Lastly is poutcome

X_train['poutcome'].value_counts()

# The number of missing values in this column 
# closely matched that of pdays
# Lets check the 'previous' column

X_train.loc[X_train['poutcome'].isna(), 'previous'].value_counts()

# Since the vast majority of missing data didn't have a previous
# campaign, we can fill the data with 'nonexistent'. 

def fill_poutcome(X_data):
    X_data['poutcome'] = X_data['poutcome'].fillna('nonexistent')
    return X_data

# Lets combine all our missing data functions into a single function
def fill_missing(X_data):
    X_data = fill_job(X_data)
    X_data = fill_education(X_data)
    X_data = fill_contact(X_data)
    X_data = fill_pdays(X_data)
    X_data = fill_poutcome(X_data)
    return X_data



# Lets apply this fill missing function to our data before 
# moving on to encoding
X_train_filled = fill_missing(X_train)
X_test_filled = fill_missing(X_test)

X_train_filled.head()

# First is job
X_train_filled['job'].value_counts()

# Lots of unique values, not ordinal data
# Lets convert to no more than 5 categories

encode_job = OneHotEncoder(max_categories=5, handle_unknown='infrequent_if_exist', sparse_output=False)

# Train the encoder
encode_job.fit(X_train_filled['job'].values.reshape(-1, 1))

# Next is marital
X_train_filled['marital'].value_counts()

# Only three values; lets use two OneHotEncoded columns
# remembering to choose options for unknown values
encode_marital = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)

# Train the encoder
encode_marital.fit(X_train_filled['marital'].values.reshape(-1, 1))

# Next is education
X_train_filled['education'].value_counts()

# This is ordinal! Lets use the ordinal encoder
# We'll set any unknown values to -1
encode_education = OrdinalEncoder(categories=[['primary', 'secondary', 'tertiary']], handle_unknown='use_encoded_value', unknown_value=-1)

# Train the encoder
encode_education.fit(X_train_filled['education'].values.reshape(-1, 1))

# Next is default
X_train_filled['default'].value_counts()

# Lets make this an Ordinal column
encode_default = OrdinalEncoder(categories=[['no', 'yes']], handle_unknown='use_encoded_value', unknown_value=-1)

# Train the encoder
encode_default.fit(X_train_filled['default'].values.reshape(-1, 1))

# Next is housing
X_train_filled['housing'].value_counts()

# Lets make this an Ordinal column
encode_housing= OrdinalEncoder(categories=[['no', 'yes']], handle_unknown='use_encoded_value', unknown_value=-1)

# Train the encoder
encode_housing.fit(X_train_filled['housing'].values.reshape(-1, 1))

# Next is loan
X_train_filled['loan'].value_counts()

# Lets make this an Ordinal column
encode_loan = OrdinalEncoder(categories=[['no', 'yes']], handle_unknown='use_encoded_value', unknown_value=-1)

# Train the encoder
encode_loan.fit(X_train_filled['loan'].values.reshape(-1, 1))

# Next is contact
X_train_filled['contact'].value_counts()

# Lets use two OneHotEncoded columns
encode_contact = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)

# Train the encoder
encode_contact.fit(X_train_filled['contact'].values.reshape(-1, 1))

# Next is month
X_train_filled['month'].value_counts()

# This month seems ordinal by may not behave that way...
# Lets use ordinal for now, but consider experimenting with this!
encode_month = OrdinalEncoder(categories=[['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']], handle_unknown='use_encoded_value', unknown_value=-1)

# Train the encoder
encode_month.fit(X_train_filled['month'].values.reshape(-1, 1))

# Next is the poutcome column
X_train_filled['poutcome'].value_counts()

# Lets use OneHotEncoding for this
encode_poutcome = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)

# Train the encoder
encode_poutcome.fit(X_train_filled['poutcome'].values.reshape(-1, 1))


# Combine the encoders into a function
# Make sure to return a dataframe
def encode_categorical(X_data):
    # Separate numeric columns
    X_data_numeric = X_data.select_dtypes(include='number').reset_index()

    # Multicolumn encoders first
    job_encoded_df = pd.DataFrame(encode_job.transform(X_data['job'].values.reshape(-1, 1)), columns=encode_job.get_feature_names_out())
    marital_encoded_df = pd.DataFrame(encode_marital.transform(X_data['marital'].values.reshape(-1, 1)), columns=encode_marital.get_feature_names_out())
    contact_encoded_df = pd.DataFrame(encode_contact.transform(X_data['contact'].values.reshape(-1, 1)), columns=encode_contact.get_feature_names_out())
    poutcome_encoded_df = pd.DataFrame(encode_poutcome.transform(X_data['poutcome'].values.reshape(-1, 1)), columns=encode_poutcome.get_feature_names_out())

    # Concat all dfs together
    dfs = [X_data_numeric, job_encoded_df, marital_encoded_df, contact_encoded_df, poutcome_encoded_df]
    X_data_encoded = pd.concat(dfs, axis=1)

    # Add single column encoders
    X_data_encoded['education'] = encode_education.transform(X_data['education'].values.reshape(-1, 1))
    X_data_encoded['default'] = encode_default.transform(X_data['default'].values.reshape(-1, 1))
    X_data_encoded['housing'] = encode_housing.transform(X_data['housing'].values.reshape(-1, 1))
    X_data_encoded['loan'] = encode_loan.transform(X_data['loan'].values.reshape(-1, 1))
    X_data_encoded['month'] = encode_month.transform(X_data['month'].values.reshape(-1, 1))
    
    return X_data_encoded

# Apply the encoding function to both training and testing
X_train_encoded = encode_categorical(X_train_filled)
X_test_encoded = encode_categorical(X_test_filled)

# Check the final X_train data
X_train_encoded.head()

# Wait! Don't forget the y data!
y_train

# Create a OneHotEncoder
encode_y = OneHotEncoder(drop='first', sparse_output=False)

# Train the encoder
encode_y.fit(y_train)

# Apply it to both y_train and y_test
# Use np.ravel to reshape for logistic regression
y_train_encoded = np.ravel(encode_y.transform(y_train))
y_test_encoded = np.ravel(encode_y.transform(y_test))
y_train_encoded

# Create and train an SVC model
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=500)
model.fit(X_train_encoded, y_train_encoded)

# Check the model's balanced accuracy on the test set

y_test_pred = model.predict(X_test_encoded)
print(balanced_accuracy_score(y_test_encoded, y_test_pred))

# Check the model's balanced accuracy on the training set

y_train_pred = model.predict(X_train_encoded)
print(balanced_accuracy_score(y_train_encoded, y_train_pred))

# We overfit! Lets try varying the max depth

models = {'train_score': [], 'test_score': [], 'max_depth': []}

for depth in range(1,10):
    models['max_depth'].append(depth)
    model = RandomForestClassifier(n_estimators=500, max_depth=depth)
    model.fit(X_train_encoded, y_train_encoded)
    y_test_pred = model.predict(X_test_encoded)
    y_train_pred = model.predict(X_train_encoded)

    models['train_score'].append(balanced_accuracy_score(y_train_encoded, y_train_pred))
    models['test_score'].append(balanced_accuracy_score(y_test_encoded, y_test_pred))

models_df = pd.DataFrame(models)

models_df.plot(x='max_depth')

# it looks like the lines start to diverge a lot after 7
# Create and train a RandomForest model
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(max_depth = 7, n_estimators=100)
model.fit(X_train_encoded, y_train_encoded)

y_train_pred = model.predict(X_train_encoded)
print(balanced_accuracy_score(y_train_encoded, y_train_pred))

y_test_pred = model.predict(X_test_encoded)
print(balanced_accuracy_score(y_test_encoded, y_test_pred))



# Import the data
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import balanced_accuracy_score
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder

from ml_utils import train_test_split_marketing,\
    fill_missing,\
    build_encoders,\
    encode_categorical,\
    build_target_encoder,\
    encode_target

df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m14/datasets/bank_marketing.csv')
df.head()

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split_marketing(df)
X_train.describe()

# Fill the missing values using the imported function
X_train_filled = fill_missing(X_train)
X_test_filled = fill_missing(X_test)
X_train_filled.head()

# Create the encoders for categorical variables (use X_train_filled)
encoders = build_encoders(X_train_filled)
encoders

# Encode X_train_filled and X_test_filled
X_train_encoded = encode_categorical(X_train_filled, encoders)
X_test_encoded = encode_categorical(X_test_filled, encoders)

X_train_encoded.head()

# Encode y_train and y_test
y_encoder = build_target_encoder(y_train)
y_train_encoded = encode_target(y_train, y_encoder)
y_test_encoded = encode_target(y_test, y_encoder)

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=100, max_depth=7, random_state=13)
model.fit(X_train_encoded, y_train_encoded)
y_pred = model.predict(X_test_encoded)
print(balanced_accuracy_score(y_test_encoded, y_pred))

# Import new data and test with the model

new_df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m14/lesson_3/datasets/bank_marketing_new_data.csv')

X = new_df.drop(columns='y')
y = new_df['y'].values.reshape(-1, 1)

X_filled = fill_missing(X)
X_encoded = encode_categorical(X_filled, encoders)
y_encoded = encode_target(y, y_encoder)

y_pred = model.predict(X_encoded)
print(balanced_accuracy_score(y_encoded, y_pred))



# Import the data
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import balanced_accuracy_score
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder

from ml_utils import train_test_split_marketing,\
    fill_missing,\
    build_encoders,\
    encode_categorical,\
    build_target_encoder,\
    encode_target

df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m14/datasets/bank_marketing.csv')
df.head()

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split_marketing(df)
X_train.describe()

# Fill the missing values using the imported function
X_train_filled = fill_missing(X_train)
X_test_filled = fill_missing(X_test)
X_train_filled.head()

# Create the encoders for categorical variables (use X_train_filled)
encoders = build_encoders(X_train_filled)
encoders

# Encode X_train_filled and X_test_filled
X_train_encoded = encode_categorical(X_train_filled, encoders)
X_test_encoded = encode_categorical(X_test_filled, encoders)

X_train_encoded.head()

# Encode y_train and y_test
y_encoder = build_target_encoder(y_train)
y_train_encoded = encode_target(y_train, y_encoder)
y_test_encoded = encode_target(y_test, y_encoder)

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=100, max_depth=7, random_state=13)
model.fit(X_train_encoded, y_train_encoded)
y_pred = model.predict(X_test_encoded)
print(balanced_accuracy_score(y_test_encoded, y_pred))

# Import new data and test with the model

new_df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m14/lesson_3/datasets/bank_marketing_new_data.csv')

# Split the new data into X and y


# Apply preprocessing to the X and y data


# Make predictions and check the balanced accuracy score




# Import dependencies
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Import the generated dataset
data = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m18/lesson_1/datasets/sample-data.csv")
data

# Plot the data
data.plot.scatter("X1", "X2", c="y", colormap="winter")

# Separate the X and y data
X = data.drop(columns="y")
y = data["y"]

# Use sklearn to split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)

# Create a logistic regression model
log_classifier = LogisticRegression(solver="lbfgs", random_state=1, max_iter=200)

# Train the model
log_classifier.fit(X_train,y_train)

# Generate the predictions from the test data
y_pred = log_classifier.predict(X_test)

# Print the accuracy score
print(f" Logistic regression model accuracy: {accuracy_score(y_test,y_pred):.3f}")



# Import dependencies
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Import the generated dataset
data = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m18/lesson_1/datasets/sample-data.csv")
data

# Plot the data
data.plot.scatter("X1", "X2", c="y", colormap="winter")

# Separate the X and y data
X = data.drop(columns="y")
y = data["y"]

# Use sklearn to split dataset


# Create a logistic regression model


# Train the model


# Generate the predictions from the test data


# Print the accuracy score




# Import our dependencies
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf

# Import the sample data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m18/lesson_1/datasets/sample-data-1000.csv")

# Plot the sample data
df.plot.scatter(x="Feature 1", y="Feature 2", c="Target", colormap="winter")

# Separate the X and y
X = df.drop(columns="Target")
y = df["Target"]

# Use sklearn to split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)

# Create scaler instance
X_scaler = StandardScaler()

# Fit the scaler
X_scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

# Create the Keras Sequential model
nn_model = tf.keras.models.Sequential()

# Set input nodes to the number of features
input_nodes = len(X.columns)

# Add our first Dense layer, including the input layer
nn_model.add(tf.keras.layers.Dense(units=5, activation="relu", input_dim=input_nodes))

# Add the output layer that uses a probability activation function
nn_model.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

# Check the structure of the Sequential model
nn_model.summary()

# Compile the Sequential model together and customize metrics
nn_model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])

# Fit the model to the training data
fit_model = nn_model.fit(X_train_scaled, y_train, epochs=100)

# Create a DataFrame containing training history
history_df = pd.DataFrame(fit_model.history)

# Increase the index by 1 to match the number of epochs
history_df.index += 1

# Plot the loss
history_df.plot(y="loss")

# Plot the accuracy
history_df.plot(y="accuracy")

# Evaluate the model using the test data
model_loss, model_accuracy = nn_model.evaluate(X_test_scaled, y_test, verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")

# Import our dependencies
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf

# Import the sample data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m18/lesson_1/datasets/sample-data-1000.csv")

# Plot the sample data
df.plot.scatter(x="Feature 1", y="Feature 2", c="Target", colormap="winter")

# Separate the X and y
X = df.drop(columns="Target")
y = df["Target"]

# Use sklearn to split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)

# Create scaler instance
X_scaler = StandardScaler()

# Fit the scaler
X_scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

# Create the Keras Sequential model


# Set input nodes to the number of features


# Add our first Dense layer, including the input layer


# Add the output layer that uses a probability activation function


# Check the structure of the Sequential model


# Compile the Sequential model together and customize metrics


# Fit the model to the training data


# Create a DataFrame containing training history


# Increase the index by 1 to match the number of epochs


# Plot the loss


# Plot the accuracy


# Evaluate the model using the test data


# Import our dependencies
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf

# Import the sample data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m18/lesson_1/datasets/sample-data-5000.csv")

# Plot the sample data
df.plot.scatter(x="Feature 1", y="Feature 2", c="Target", colormap="winter")

# Separate the X and y
X = df.drop(columns="Target")
y = df["Target"]

# Use sklearn to split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)

# Create scaler instance
X_scaler = StandardScaler()

# Fit the scaler
X_scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

# Create the Keras Sequential model
nn_model = tf.keras.models.Sequential()

# Set input nodes to the number of features
input_nodes = len(X.columns)

# Add our first Dense layer, including the input layer
nn_model.add(tf.keras.layers.Dense(units=5, activation="relu", input_dim=input_nodes))

# Add the output layer that uses a probability activation function
nn_model.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

# Check the structure of the Sequential model
nn_model.summary()

# Compile the Sequential model together and customize metrics
nn_model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])

# Fit the model to the training data
fit_model = nn_model.fit(X_train_scaled, y_train, epochs=50)

# Evaluate the model using the test data
model_loss, model_accuracy = nn_model.evaluate(X_test_scaled, y_test, verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")

# Create a DataFrame containing training history
history_df = pd.DataFrame(fit_model.history, index=range(1,len(fit_model.history["loss"])+1))

# Plot the accuracy
history_df.plot(y="accuracy")

# Import our dependencies
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf

# Import the sample data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m18/lesson_1/datasets/sample-data-5000.csv")

# Plot the sample data
df.plot.scatter(x="Feature 1", y="Feature 2", c="Target", colormap="winter")

# Separate the X and y
X = df.drop(columns="Target")
y = df["Target"]

# Use sklearn to split dataset


# Create scaler instance


# Fit the scaler


# Scale the data


# Create the Keras Sequential model


# Set input nodes to the number of features


# Add our first Dense layer, including the input layer


# Add the output layer that uses a probability activation function


# Check the structure of the Sequential model


# Compile the Sequential model together and customize metrics


# Fit the model to the training data


# Evaluate the model using the test data


# Create a DataFrame containing training history


# Plot the accuracy


# Import our dependencies
import pandas as pd
from sklearn.model_selection import train_test_split
import tensorflow as tf

# Import the speech recognition data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m18/lesson_1/datasets/voice.csv")
df.head()

# Drop the label to create the X data
X = df.drop('Class', axis=1)
# Create the y set from the "Class" column
y = df["Class"]

# Use sklearn to split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)

# Create the Keras Sequential model
nn_model = tf.keras.models.Sequential()

# Set input nodes to the number of features
input_nodes = len(X.columns)

# Add our first Dense layer, including the input layer
nn_model.add(tf.keras.layers.Dense(units=5, activation="relu", input_dim=input_nodes))

# Add the output layer that uses a probability activation function
nn_model.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

# Check the structure of the Sequential model
nn_model.summary()

# Compile the Sequential model together and customize metrics
nn_model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])

# Fit the model to the training data
fit_model = nn_model.fit(X_train, y_train, epochs=100)

# Create a DataFrame containing training history
history_df = pd.DataFrame(fit_model.history)

# Increase the index by 1 to match the number of epochs
history_df.index += 1

# Plot the loss
history_df.plot(y="loss")

# Plot the accuracy
history_df.plot(y="accuracy")

# Evaluate the model using the test data
model_loss, model_accuracy = nn_model.evaluate(X_test, y_test, verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")

# Make predictions
predictions = nn_model.predict(X_test,verbose=2)
predictions

# Round predictions
predictions_rounded = [round(prediction[0],0) for prediction in predictions]
predictions_rounded

# Check the classification report
from sklearn.metrics import classification_report
print(classification_report(y_test, predictions_rounded))


# Compare with random forest model
from sklearn.ensemble import RandomForestClassifier

# Create the random forest classifier model
# with n_estimators=128 and random_state=1
rf_model = RandomForestClassifier(n_estimators=128, random_state=1)

# Fit the model to the training data
rf_model.fit(X_train, y_train)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {rf_model.score(X_train, y_train)}")
print(f"Testing Data Score: {rf_model.score(X_test, y_test)}")

# Make predictions and produce the classification report for the random forest model
predictions = rf_model.predict(X_test)
print(classification_report(y_test, predictions))

# Import our dependencies
import pandas as pd
from sklearn.model_selection import train_test_split
import tensorflow as tf

# Import the speech recognition data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m18/lesson_1/datasets/voice.csv")
df.head()

# Check the classes in the dataset
df["Class"].unique()

# Fix the classes to create binary output, necessary for our neural network model
df.loc[df["Class"]==2,"Class"] = 0

df["Class"].unique()

# Drop the label to create the X data
X = df.drop('Class', axis=1)
# Create the y set from the "Class" column
y = df["Class"]

# Use sklearn to split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)

# Create the Keras Sequential model
nn_model = tf.keras.models.Sequential()

# Set input nodes to the number of features
input_nodes = len(X.columns)

# Add our first Dense layer, including the input layer
nn_model.add(tf.keras.layers.Dense(units=5, activation="relu", input_dim=input_nodes))

# Add the output layer that uses a probability activation function
nn_model.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

# Check the structure of the Sequential model
nn_model.summary()

# Compile the Sequential model together and customize metrics
nn_model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])

# Fit the model to the training data
fit_model = nn_model.fit(X_train, y_train, epochs=100)

# Create a DataFrame containing training history
history_df = pd.DataFrame(fit_model.history)

# Increase the index by 1 to match the number of epochs
history_df.index += 1

# Plot the loss
history_df.plot(y="loss")

# Plot the accuracy
history_df.plot(y="accuracy")

# Evaluate the model using the test data
model_loss, model_accuracy = nn_model.evaluate(X_test, y_test, verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")

# Make predictions
predictions = nn_model.predict(X_test,verbose=2)
predictions

# Round predictions
predictions_rounded = [round(prediction[0],0) for prediction in predictions]
predictions_rounded

# Check the classification report
from sklearn.metrics import classification_report
print(classification_report(y_test, predictions_rounded))


# Compare with random forest model
from sklearn.ensemble import RandomForestClassifier

# Create the random forest classifier model
# with n_estimators=128 and random_state=1
rf_model = RandomForestClassifier(n_estimators=128, random_state=1)

# Fit the model to the training data
rf_model.fit(X_train, y_train)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {rf_model.score(X_train, y_train)}")
print(f"Testing Data Score: {rf_model.score(X_test, y_test)}")

# Make predictions and produce the classification report for the random forest model
predictions = rf_model.predict(X_test)
print(classification_report(y_test, predictions))

# Import our dependencies
import pandas as pd
from sklearn.model_selection import train_test_split
import tensorflow as tf

# Import the speech recognition data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m18/lesson_1/datasets/voice.csv")
df.head()

# Drop the label to create the X data
X = df.drop('Class', axis=1)
# Create the y set from the "Class" column
y = df["Class"]

# Use sklearn to split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)

# Create the Keras Sequential model


# Set input nodes to the number of features


# Add our first Dense layer, including the input layer


# Add the output layer that uses a probability activation function


# Check the structure of the Sequential model


# Compile the Sequential model together and customize metrics


# Fit the model to the training data


# Create a DataFrame containing training history


# Increase the index by 1 to match the number of epochs


# Plot the loss


# Plot the accuracy


# Evaluate the model using the test data


# Make predictions


# Round predictions


# Check the classification report
from sklearn.metrics import classification_report


# Compare with random forest model
from sklearn.ensemble import RandomForestClassifier

# Create the random forest classifier model
# with n_estimators=128 and random_state=1


# Fit the model to the training data


# Validate the model by checking the model accuracy with model.score


# Make predictions and produce the classification report for the random forest model


# Import our dependencies
import pandas as pd
import sklearn as skl
import tensorflow as tf
from sklearn.model_selection import train_test_split

# Read the dataset
df_moons = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m18/lesson_2/datasets/moons-data-1.csv")

# Plot the nonlinear dummy data
df_moons.plot.scatter(x="Feature 1",y="Feature 2", c="Target",colormap="winter")

# Separate data into X and y
X = df_moons.drop("Target", axis=1)
y = df_moons["Target"]

# Use sklearn to split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)

# Create scaler instance
X_scaler = skl.preprocessing.StandardScaler()

# Fit the scaler
X_scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

# Create the Keras Sequential model
nn_model = tf.keras.models.Sequential()

# Set input nodes to the number of features
input_nodes = len(X.columns)

# Add our first Dense layer, including the input layer
nn_model.add(tf.keras.layers.Dense(units=1, activation="relu", input_dim=input_nodes))

# Add the output layer that uses a probability activation function
nn_model.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

# Check the structure of the Sequential model
nn_model.summary()

# Compile the Sequential model together and customize metrics
nn_model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])

# Fit the model to the training data
fit_model = nn_model.fit(X_train_scaled, y_train, epochs=100)

# Evaluate the model using the test data
model_loss, model_accuracy = nn_model.evaluate(X_test_scaled,y_test,verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")

# Create a new neural network model with 6 neurons
nn_model2 = tf.keras.models.Sequential()

nn_model2.add(tf.keras.layers.Dense(units=6, activation="relu", input_dim=input_nodes))

nn_model2.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

nn_model2.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])

fit_model = nn_model2.fit(X_train_scaled, y_train, epochs=100)


# Evaluate the model using the test data
model_loss, model_accuracy = nn_model2.evaluate(X_test_scaled,y_test,verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")

# Import our dependencies
import pandas as pd
import sklearn as skl
import tensorflow as tf
from sklearn.model_selection import train_test_split

# Read the dataset
df_moons = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m18/lesson_2/datasets/moons-data-1.csv")

# Plot the nonlinear dummy data
df_moons.plot.scatter(x="Feature 1",y="Feature 2", c="Target",colormap="winter")

# Separate data into X and y
X = df_moons.drop("Target", axis=1)
y = df_moons["Target"]

# Use sklearn to split dataset


# Create scaler instance


# Fit the scaler


# Scale the data


# Create the Keras Sequential model


# Set input nodes to the number of features


# Add our first Dense layer, including the input layer


# Add the output layer that uses a probability activation function


# Check the structure of the Sequential model


# Compile the Sequential model together and customize metrics


# Fit the model to the training data


# Evaluate the model using the test data


# Create a new neural network model with 6 neurons


# Evaluate the model using the test data


# Import our dependencies
import pandas as pd
import matplotlib as plt
import sklearn as skl
import tensorflow as tf
from sklearn.model_selection import train_test_split

# Read the nonlinear data
df_moons = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m18/lesson_2/datasets/moons-data-2.csv")

# Separate the X and y
X = df_moons.drop("Target", axis=1)
y = df_moons["Target"]

# Use sklearn to split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)

# Create scaler instance
X_scaler = skl.preprocessing.StandardScaler()

# Fit the scaler
X_scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)


# Create a Keras Sequential model and add more than one Dense hidden layer
nn_model = tf.keras.models.Sequential()

# Set the input nodes to the number of features
input_nodes = len(X.columns)

nn_model.add(tf.keras.layers.Dense(units=6, activation="relu", input_dim=input_nodes))

nn_model.add(tf.keras.layers.Dense(units=6, activation="relu"))

nn_model.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

# Check the structure of the Sequential model
nn_model.summary()

# Compile the model and train over more than 100 epochs
nn_model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])

fit_model = nn_model.fit(X_train_scaled, y_train, epochs=200)

# Evaluate the performance of model using the loss and predictive accuracy of the model on the test dataset.
model_loss, model_accuracy = nn_model.evaluate(X_test_scaled,y_test,verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")



# Import our dependencies
import pandas as pd
import matplotlib as plt
import sklearn as skl
import tensorflow as tf
from sklearn.model_selection import train_test_split

# Read the nonlinear data
df_moons = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m18/lesson_2/datasets/moons-data-2.csv")

# Separate the X and y
X = df_moons.drop("Target", axis=1)
y = df_moons["Target"]

# Use sklearn to split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)

# Create scaler instance
X_scaler = skl.preprocessing.StandardScaler()

# Fit the scaler
X_scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)


# Create a Keras Sequential model and add more than one Dense hidden layer


# Set the input nodes to the number of features


# Check the structure of the Sequential model


# Compile the model and train over more than 100 epochs


# Evaluate the performance of model using the loss and predictive accuracy of the model on the test dataset.




# Dependencies
import numpy as np
import matplotlib.pyplot as plt

# Example outlier plot of reaction times
times = [96,98,100,105,85,88,95,100,101,102,97,98,5]
fig1, ax1 = plt.subplots()
ax1.set_title('Reaction Times in Tennis')
ax1.set_ylabel('Reaction Time (ms)')
ax1.boxplot(times)
plt.show()

# Determine which data points are outside of the 1.5*IQR range
quartiles = np.quantile(times,[.25,.75])
iqr = quartiles[1]-quartiles[0]
lower_bound = quartiles[0]-(1.5*iqr)
upper_bound = quartiles[1]+(1.5*iqr)

potential_outliers = [time for time in times if time < lower_bound or time > upper_bound]
potential_outliers

# Dependencies
import numpy as np
import matplotlib.pyplot as plt

# Example outlier plot of reaction times
times = [96,98,100,105,85,88,95,100,101,102,97,98,5]
fig1, ax1 = plt.subplots()
ax1.set_title('Reaction Times in Tennis')
ax1.set_ylabel('Reaction Time (ms)')
ax1.boxplot(times)
plt.show()

# Determine which data points are outside of the 1.5*IQR range


!pip install keras-tuner

# Import our dependencies
import pandas as pd
import sklearn as skl
import tensorflow as tf


# Read the dataset
df_moons = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m18/lesson_2/datasets/moons-data-1.csv")

# Separate data into X and y
X = df_moons.drop("Target", axis=1)
y = df_moons["Target"]

# Use sklearn to split dataset
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)

# Create scaler instance
X_scaler = skl.preprocessing.StandardScaler()

# Fit the scaler
X_scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

# Create a method that creates a new Sequential model with hyperparameter options
def create_model(hp):
    nn_model = tf.keras.models.Sequential()

    # Allow kerastuner to decide which activation function to use in hidden layers
    activation = hp.Choice('activation',['relu','tanh','sigmoid'])

    # Allow kerastuner to decide number of neurons in first layer
    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',
        min_value=1,
        max_value=10,
        step=2), activation=activation, input_dim=len(X.columns)))

    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers
    for i in range(hp.Int('num_layers', 1, 6)):
        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),
            min_value=1,
            max_value=10,
            step=2),
            activation=activation))

    nn_model.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

    # Compile the model
    nn_model.compile(loss="binary_crossentropy", optimizer='adam', metrics=["accuracy"])

    return nn_model

# Import the kerastuner library
import keras_tuner as kt

tuner = kt.Hyperband(
    create_model,
    objective="val_accuracy",
    max_epochs=20,
    hyperband_iterations=2)

# Run the kerastuner search for best hyperparameters
tuner.search(X_train_scaled,y_train,epochs=20,validation_data=(X_test_scaled,y_test))

# Get best model hyperparameters
best_hyper = tuner.get_best_hyperparameters(1)[0]
best_hyper.values

# Evaluate best model against full test data
best_model = tuner.get_best_models(1)[0]
model_loss, model_accuracy = best_model.evaluate(X_test_scaled,y_test,verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")



!pip install keras-tuner

# Import our dependencies
import pandas as pd
import sklearn as skl
import tensorflow as tf


# Read the dataset
df_moons = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m18/lesson_2/datasets/moons-data-1.csv")

# Separate data into X and y
X = df_moons.drop("Target", axis=1)
y = df_moons["Target"]

# Use sklearn to split dataset
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)

# Create scaler instance
X_scaler = skl.preprocessing.StandardScaler()

# Fit the scaler
X_scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

# Create a method that creates a new Sequential model with hyperparameter options
def create_model(hp):


    # Allow kerastuner to decide which activation function to use in hidden layers


    # Allow kerastuner to decide number of neurons in first layer


    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers


    # Compile the model


    return nn_model

# Import the kerastuner library


# Run the kerastuner search for best hyperparameters


# Get best model hyperparameters


# Evaluate best model against full test data




! pip install keras-tuner

# Import our dependencies
import pandas as pd
import sklearn as skl
import tensorflow as tf

# Import the nonlinear dummy data
df_circles = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m18/lesson_2/datasets/circles-data.csv")

# Split data into X and y
X = df_circles.drop("Target", axis=1)
y = df_circles["Target"]

# Use sklearn to split dataset
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)

# Create scaler instance
X_scaler = skl.preprocessing.StandardScaler()

# Fit the scaler
X_scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

# Plot the nonlinear dummy data
df_circles.plot.scatter(x="Feature 1",y="Feature 2", c="Target", colormap="winter")

# Create a method that creates a new Sequential model with hyperparameter options
def create_model(hp):
    nn_model = tf.keras.models.Sequential()

    # Allow kerastuner to decide which activation function to use in hidden layers
    activation = hp.Choice('activation',['relu','tanh'])

    # Allow kerastuner to decide number of neurons in first layer
    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',
        min_value=1,
        max_value=30,
        step=5), activation=activation, input_dim=len(X.columns)))

    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers
    for i in range(hp.Int('num_layers', 1, 5)):
        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),
            min_value=1,
            max_value=30,
            step=5),
            activation=activation))

    nn_model.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

    # Compile the model
    nn_model.compile(loss="binary_crossentropy", optimizer='adam', metrics=["accuracy"])

    return nn_model

# Import the kerastuner library
import keras_tuner as kt

tuner = kt.Hyperband(
    create_model,
    objective="val_accuracy",
    max_epochs=20,
    hyperband_iterations=2)

# Run the kerastuner search for best hyperparameters
tuner.search(X_train_scaled,y_train,epochs=20,validation_data=(X_test_scaled,y_test))

# Get top 3 model hyperparameters and print the values
top_hyper = tuner.get_best_hyperparameters(3)
for param in top_hyper:
    print(param.values)

# Evaluate the top 3 models against the test dataset
top_model = tuner.get_best_models(3)
for model in top_model:
    model_loss, model_accuracy = model.evaluate(X_test_scaled,y_test,verbose=2)
    print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")



# Import our dependencies
import pandas as pd
import sklearn as skl
import tensorflow as tf

# Import the nonlinear dummy data
df_circles = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m18/lesson_2/datasets/circles-data.csv")

# Split data into X and y
X = df_circles.drop("Target", axis=1)
y = df_circles["Target"]

# Use sklearn to split dataset
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)

# Create scaler instance
X_scaler = skl.preprocessing.StandardScaler()

# Fit the scaler
X_scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

# Plot the nonlinear dummy data


# Create a method that creates a new Sequential model with hyperparameter options


    # Allow kerastuner to decide which activation function to use in hidden layers


    # Allow kerastuner to decide number of neurons in first layer


    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers


    # Compile the model

    

# Import the kerastuner library


# Run the kerastuner search for best hyperparameters


# Get top 3 model hyperparameters and print the values


# Evaluate the top 3 models against the test dataset




# Import our dependencies
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler,OneHotEncoder
from pathlib import Path
import pandas as pd
import tensorflow as tf

# Import our input dataset
attrition_df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m18/lesson_2/datasets/HR-Employee-Attrition.csv')
attrition_df

# Generate our categorical variable lists
attrition_cat = attrition_df.dtypes[attrition_df.dtypes == "object"].index.tolist()

# Check the number of unique values in each column
attrition_df[attrition_cat].nunique()

# Create a OneHotEncoder instance
enc = OneHotEncoder(sparse_output=False)

# Fit and transform the OneHotEncoder using the categorical variable list
encode_df = pd.DataFrame(enc.fit_transform(attrition_df[attrition_cat]))

# Add the encoded variable names to the dataframe
encode_df.columns = enc.get_feature_names_out(attrition_cat)
encode_df.head()

# Merge one-hot encoded features and drop the originals
attrition_df = attrition_df.merge(encode_df,left_index=True, right_index=True)
attrition_df = attrition_df.drop(attrition_cat, axis=1)
attrition_df.head()

# Split our preprocessed data into our features and target arrays
y = attrition_df["Attrition_Yes"].values
X = attrition_df.drop(["Attrition_Yes","Attrition_No"],axis=1).values

# Split the preprocessed data into a training and testing dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)

# Create a StandardScaler instances
scaler = StandardScaler()

# Fit the StandardScaler
X_scaler = scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

# Define the model - deep neural net
number_input_features = len(X_train[0])
hidden_nodes_layer1 =  8
hidden_nodes_layer2 = 5

nn = tf.keras.models.Sequential()

# First hidden layer
nn.add(
    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation="relu")
)

# Second hidden layer
nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation="relu"))

# Output layer
nn.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

# Check the structure of the model
nn.summary()

# Compile the model
nn.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])

# Train the model
fit_model = nn.fit(X_train_scaled,y_train,epochs=100)

# Evaluate the model using the test data
model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)

# Display evaluation results
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")

# Set the model's file path
file_path = Path("saved_models/attrition.keras")

# Export your model to a keras file
nn.save(file_path)

# Import the required libraries
import tensorflow as tf

# Set the model's file path
file_path = Path("saved_models/attrition.keras")

# Load the model to a new object
nn_imported = tf.keras.models.load_model(file_path)

# Evaluate the model using the test data
model_loss, model_accuracy = nn_imported.evaluate(X_test_scaled, y_test, verbose=2)

# Display evaluation results
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")

# Import our dependencies
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler,OneHotEncoder
from pathlib import Path
import pandas as pd
import tensorflow as tf

# Import our input dataset
attrition_df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m18/lesson_2/datasets/HR-Employee-Attrition.csv')
attrition_df

# Generate our categorical variable lists


# Check the number of unique values in each column


# Create a OneHotEncoder instance


# Fit and transform the OneHotEncoder using the categorical variable list


# Add the encoded variable names to the dataframe


# Merge one-hot encoded features and drop the originals


# Split our preprocessed data into our features and target arrays


# Split the preprocessed data into a training and testing dataset


# Create a StandardScaler instances


# Fit the StandardScaler


# Scale the data


# Define the model - deep neural net


# First hidden layer


# Second hidden layer


# Output layer


# Check the structure of the model


# Compile the model


# Train the model


# Evaluate the model using the test data


# Display evaluation results


# Set the model's file path


# Export your model to a keras file


# Import the required libraries
import tensorflow as tf

# Set the model's file path


# Load the model to a new object


# Evaluate the model using the test data


# Display evaluation results


# Import our dependencies
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from pathlib import Path
import pandas as pd
import tensorflow as tf
from sklearn.metrics import classification_report

# Import data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m18/lesson_2/datasets/sports-articles.csv"
df = pd.read_csv(file_path)
df.head()

# Drop the label to create the X data
X = df.drop('Label', axis=1)
# Create the y set from the "Label" column
y = df["Label"]

# Split training/test datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

# Encode the y data with the label encoder
# Create an instance of the label encoder
le = LabelEncoder()

# Fit and transform the y training and testing data using the label encoder
y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.transform(y_test)

# Scale the X data by using StandardScaler()
scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define the deep learning model
nn_model = tf.keras.models.Sequential()
nn_model.add(tf.keras.layers.Dense(units=5, activation="tanh", input_dim=len(X_train.columns)))

nn_model.add(tf.keras.layers.Dense(units=3, activation="tanh"))
nn_model.add(tf.keras.layers.Dense(units=9, activation="tanh"))
nn_model.add(tf.keras.layers.Dense(units=3, activation="relu"))
nn_model.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

# Compile the Sequential model together and customize metrics
nn_model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])

# Train the model
fit_model = nn_model.fit(X_train_scaled, y_train_encoded, epochs=100)

# Evaluate the model using the test data
model_loss, model_accuracy = nn_model.evaluate(X_test_scaled,y_test_encoded,verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")

# Make predictions with the test data
predictions = nn_model.predict(X_test_scaled,verbose=2)
predictions

# Save the predictions to a DataFrame and round the predictions to binary results
predictions_df = pd.DataFrame(columns=["predictions"], data=predictions)
predictions_df["predictions"] = round(predictions_df["predictions"],0)
predictions_df

# Print the classification report with the y test data and predictions
print(classification_report(y_test_encoded, predictions_df["predictions"].values))

# Set the model's file path
file_path = Path("sports-articles.keras")

# Export your model to a keras file
nn_model.save(file_path)

# Set the model's file path
file_path = Path("sports-articles.keras")

# Load the model to a new object
nn_imported = tf.keras.models.load_model(file_path)

# Evaluate the model using the test data
model_loss, model_accuracy = nn_imported.evaluate(X_test_scaled, y_test_encoded, verbose=2)

# Display evaluation results
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import classification_report

# Load in data
df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_3/datasets/sports-articles.csv')
df

# Drop the label to create the X data
X = df.drop('Label', axis=1)
X

# Create the y set from the "Label" column
y = df["Label"]
y

# Split the data into training and testing sets using random_state=1
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

# Encode the y data with the label encoder
# Create an instance of the label encoder
le = LabelEncoder()

# Fit and transform the y training and testing data using the label encoder
y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.transform(y_test)
y_train_encoded

# Scale the X data by using StandardScaler()
scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_train_scaled

# Transform the test dataset based on the fit from the training dataset
X_test_scaled = scaler.transform(X_test)
X_test_scaled

# Create the logistic regression classifier model with a random_state of 1
lr_model = LogisticRegression(random_state=1)

# Fit the model to the training data
lr_model.fit(X_train_scaled, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {lr_model.score(X_train_scaled, y_train_encoded)}")
print(f"Testing Data Score: {lr_model.score(X_test_scaled, y_test_encoded)}")

# Make predictions and produce the classification report
predictions = lr_model.predict(X_test_scaled)
print(classification_report(y_test_encoded, predictions))

# Create the support vector machine classifier model with a 'rbf' kernel
svm_model = SVC(kernel='rbf')

# Fit the model to the training data
svm_model.fit(X_train_scaled, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {svm_model.score(X_train_scaled, y_train_encoded)}")
print(f"Testing Data Score: {svm_model.score(X_test_scaled, y_test_encoded)}")

# Make predictions and produce the classification report
predictions = svm_model.predict(X_test_scaled)
print(classification_report(y_test_encoded, predictions))

# Create the KNN model with 11 neighbors
knn_model = KNeighborsClassifier(n_neighbors=11)

# Fit the model to the training data
knn_model.fit(X_train_scaled, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {knn_model.score(X_train_scaled, y_train_encoded)}")
print(f"Testing Data Score: {knn_model.score(X_test_scaled, y_test_encoded)}")

# Make predictions and produce the classification report
predictions = knn_model.predict(X_test_scaled)
print(classification_report(y_test_encoded, predictions))

# Create the decision tree classifier model
dt_model = DecisionTreeClassifier()

# Fit the model to the training data
dt_model.fit(X_train_scaled, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {dt_model.score(X_train_scaled, y_train_encoded)}")
print(f"Testing Data Score: {dt_model.score(X_test_scaled, y_test_encoded)}")

# Make predictions and produce the classification report
predictions = dt_model.predict(X_test_scaled)
print(classification_report(y_test_encoded, predictions))

# Create the random forest classifier model
# with n_estimators=128 and random_state=1
rf_model = RandomForestClassifier(n_estimators=128, random_state=1)

# Fit the model to the training data
rf_model.fit(X_train_scaled, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {rf_model.score(X_train_scaled, y_train_encoded)}")
print(f"Testing Data Score: {rf_model.score(X_test_scaled, y_test_encoded)}")

# Make predictions and produce the classification report
predictions = rf_model.predict(X_test_scaled)
print(classification_report(y_test_encoded, predictions))

# Train the Gradient Boosting classifier
clf = GradientBoostingClassifier(random_state=1).fit(X_train_scaled, y_train_encoded)

# Evaluate the model
print(f'Training Score: {clf.score(X_train_scaled, y_train_encoded)}')
print(f'Testing Score: {clf.score(X_test_scaled, y_test_encoded)}')

# Make predictions and produce the classification report
predictions = clf.predict(X_test_scaled)
print(classification_report(y_test_encoded, predictions))

# Train the AdaBoostClassifier
clf = AdaBoostClassifier(random_state=1).fit(X_train_scaled, y_train_encoded)

# Evaluate the model
print(f'Training Score: {clf.score(X_train_scaled, y_train_encoded)}')
print(f'Testing Score: {clf.score(X_test_scaled, y_test_encoded)}')

# Make predictions and produce the classification report
predictions = clf.predict(X_test_scaled)
print(classification_report(y_test_encoded, predictions))



# Import our dependencies
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from pathlib import Path
import pandas as pd
import tensorflow as tf
from sklearn.metrics import classification_report

# Import data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m18/lesson_2/datasets/sports-articles.csv"
df = pd.read_csv(file_path)
df.head()

# Drop the label to create the X data
X = df.drop('Label', axis=1)
# Create the y set from the "Label" column
y = df["Label"]

# Split training/test datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

# Encode the y data with the label encoder
# Create an instance of the label encoder
le = LabelEncoder()

# Fit and transform the y training and testing data using the label encoder
y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.transform(y_test)

# Scale the X data by using StandardScaler()
scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define the deep learning model


# Compile the Sequential model together and customize metrics


# Train the model


# Evaluate the model using the test data


# Make predictions with the test data


# Save the predictions to a DataFrame and round the predictions to binary results


# Print the classification report with the y test data and predictions


# Set the model's file path


# Export your model to a keras file


# Set the model's file path


# Load the model to a new object


# Evaluate the model using the test data


# Display evaluation results


import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import classification_report

# Load in data
df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_3/datasets/sports-articles.csv')
df

# Drop the label to create the X data
X = df.drop('Label', axis=1)
X

# Create the y set from the "Label" column
y = df["Label"]
y

# Split the data into training and testing sets using random_state=1
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

# Encode the y data with the label encoder
# Create an instance of the label encoder
le = LabelEncoder()

# Fit and transform the y training and testing data using the label encoder
y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.transform(y_test)
y_train_encoded

# Scale the X data by using StandardScaler()
scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_train_scaled

# Transform the test dataset based on the fit from the training dataset
X_test_scaled = scaler.transform(X_test)
X_test_scaled

# Create the logistic regression classifier model with a random_state of 1
lr_model = LogisticRegression(random_state=1)

# Fit the model to the training data
lr_model.fit(X_train_scaled, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {lr_model.score(X_train_scaled, y_train_encoded)}")
print(f"Testing Data Score: {lr_model.score(X_test_scaled, y_test_encoded)}")

# Make predictions and produce the classification report
predictions = lr_model.predict(X_test_scaled)
print(classification_report(y_test_encoded, predictions))

# Create the support vector machine classifier model with a 'rbf' kernel
svm_model = SVC(kernel='rbf')

# Fit the model to the training data
svm_model.fit(X_train_scaled, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {svm_model.score(X_train_scaled, y_train_encoded)}")
print(f"Testing Data Score: {svm_model.score(X_test_scaled, y_test_encoded)}")

# Make predictions and produce the classification report
predictions = svm_model.predict(X_test_scaled)
print(classification_report(y_test_encoded, predictions))

# Create the KNN model with 11 neighbors
knn_model = KNeighborsClassifier(n_neighbors=11)

# Fit the model to the training data
knn_model.fit(X_train_scaled, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {knn_model.score(X_train_scaled, y_train_encoded)}")
print(f"Testing Data Score: {knn_model.score(X_test_scaled, y_test_encoded)}")

# Make predictions and produce the classification report
predictions = knn_model.predict(X_test_scaled)
print(classification_report(y_test_encoded, predictions))

# Create the decision tree classifier model
dt_model = DecisionTreeClassifier()

# Fit the model to the training data
dt_model.fit(X_train_scaled, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {dt_model.score(X_train_scaled, y_train_encoded)}")
print(f"Testing Data Score: {dt_model.score(X_test_scaled, y_test_encoded)}")

# Make predictions and produce the classification report
predictions = dt_model.predict(X_test_scaled)
print(classification_report(y_test_encoded, predictions))

# Create the random forest classifier model
# with n_estimators=128 and random_state=1
rf_model = RandomForestClassifier(n_estimators=128, random_state=1)

# Fit the model to the training data
rf_model.fit(X_train_scaled, y_train_encoded)

# Validate the model by checking the model accuracy with model.score
print(f"Training Data Score: {rf_model.score(X_train_scaled, y_train_encoded)}")
print(f"Testing Data Score: {rf_model.score(X_test_scaled, y_test_encoded)}")

# Make predictions and produce the classification report
predictions = rf_model.predict(X_test_scaled)
print(classification_report(y_test_encoded, predictions))

# Train the Gradient Boosting classifier
clf = GradientBoostingClassifier(random_state=1).fit(X_train_scaled, y_train_encoded)

# Evaluate the model
print(f'Training Score: {clf.score(X_train_scaled, y_train_encoded)}')
print(f'Testing Score: {clf.score(X_test_scaled, y_test_encoded)}')

# Make predictions and produce the classification report
predictions = clf.predict(X_test_scaled)
print(classification_report(y_test_encoded, predictions))

# Train the AdaBoostClassifier
clf = AdaBoostClassifier(random_state=1).fit(X_train_scaled, y_train_encoded)

# Evaluate the model
print(f'Training Score: {clf.score(X_train_scaled, y_train_encoded)}')
print(f'Testing Score: {clf.score(X_test_scaled, y_test_encoded)}')

# Make predictions and produce the classification report
predictions = clf.predict(X_test_scaled)
print(classification_report(y_test_encoded, predictions))



# Pandas for reading and preparing the data
import pandas as pd
# TensorFlow library. Used to implement machine learning models
import tensorflow as tf
# Numpy contains helpful functions for efficient mathematical calculations
import numpy as np
# Graph plotting library
import matplotlib.pyplot as plt
%matplotlib inline

# Read the data
ratings_df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m18/lesson_3/datasets/review-ratings.csv")
ratings_df

# Drop irrelevant column
ratings_df = ratings_df.drop("Unnamed: 25", axis=1)
# Check the data types
ratings_df.dtypes

# Set "User" as the index
ratings_df = ratings_df.set_index("User")
ratings_df

# Find problems for converting "Category 11" to float
try:
  ratings_df["Category 11"] = ratings_df["Category 11"].astype("float64")
except Exception as e:
  print(e)


# Find row with problem string
ratings_df.loc[ratings_df["Category 11"] == "2\t2.",:]


# Remove User 2713
ratings_df = ratings_df.drop("User 2713")

# Convert Category 11 to float
ratings_df["Category 11"] = ratings_df["Category 11"].astype("float64")

# Update the column names
travel_categories = ["churches",
                     "resorts",
                     "beaches",
                     "parks",
                     "theatres",
                     "museums",
                     "malls",
                     "zoo",
                     "restaurants",
                     "pubs/bars",
                     "local services",
                     "burger/pizza shops",
                     "hotels/other lodgings",
                     "juice bars",
                     "art galleries",
                     "dance clubs",
                     "swimming pools",
                     "gyms",
                     "bakeries",
                     "beauty & spas",
                     "cafes",
                     "view points",
                     "monuments",
                     "gardens"]
ratings_df.columns = travel_categories
ratings_df.head()

# Create variable for normalization
# Ratings are between 0-5
normalization_factor = 5

# Normalize the ratings
normalized_ratings = ratings_df / normalization_factor
normalized_ratings

# Create the training data
X_train = normalized_ratings.values
X_train

# Set values for our hidden nodes and visible nodes
hiddenUnits = 15
visibleUnits =  len(ratings_df.columns)

# Set the bias of the visible layer to 0. This should use the number of unique travel categories.
visible_layer_bias = tf.Variable(tf.zeros([visibleUnits]), tf.float32)

# Set the bias of the hidden layer to 0. This will use hiddenUnits, which is
# the number of features we're going to learn
hidden_layer_bias = tf.Variable(tf.zeros([hiddenUnits]), tf.float32)

# Set the Weights to 0
W = tf.Variable(tf.zeros([visibleUnits, hiddenUnits]), tf.float32)

v0 = tf.zeros([visibleUnits], tf.float32)
# testing to see if the matrix product works
tf.matmul([v0], W)

# Phase 1: Input Processing
# Define a function to return only the generated hidden states
def hidden_layer(v0_state, W, hb):
    # probabilities of the hidden units
    h0_prob = tf.nn.sigmoid(tf.matmul([v0_state], W) + hb)
    # sample_h_given_X
    h0_state = tf.nn.relu(tf.sign(h0_prob - tf.random.uniform(tf.shape(h0_prob))))
    return h0_state

# Print output of zeros input
h0 = hidden_layer(v0, W, hidden_layer_bias)
print("first 15 hidden states: ", h0[0][0:15])

# Define a function to return the reconstructed output
def reconstructed_output(h0_state, W, vb):
    v1_prob = tf.nn.sigmoid(tf.matmul(h0_state, tf.transpose(W)) + vb)
    # sample_v_given_h
    v1_state = tf.nn.relu(tf.sign(v1_prob - tf.random.uniform(tf.shape(v1_prob))))
    return v1_state[0]


# Get reconstructed output from zeros input
v1 = reconstructed_output(h0, W, visible_layer_bias)
print("hidden state shape: ", h0.shape)
print("v0 state shape:  ", v0.shape)
print("v1 state shape:  ", v1.shape)

# Set the error function, which in this case will be the Mean Absolute Error Function.
def error(v0_state, v1_state):
    return tf.reduce_mean(tf.square(v0_state - v1_state))

err = tf.reduce_mean(tf.square(v0 - v1))
print("error" , err.numpy())

# Set the training variables
epochs = 1
batchsize = 200
errors = []
weights = []
K=1
alpha = 0.1

# Create dataset batches
train_ds = \
    tf.data.Dataset.from_tensor_slices((np.float32(X_train))).batch(batchsize)



v0_state=v0

# Train the model
for epoch in range(epochs):
    batch_number = 0
    for batch_x in train_ds:

        for i_sample in range(len(batch_x)):
            for k in range(K):
                v0_state = batch_x[i_sample]
                h0_state = hidden_layer(v0_state, W, hidden_layer_bias)
                v1_state = reconstructed_output(h0_state, W, visible_layer_bias)
                h1_state = hidden_layer(v1_state, W, hidden_layer_bias)

                delta_W = tf.matmul(tf.transpose([v0_state]), h0_state) - tf.matmul(tf.transpose([v1_state]), h1_state)

                # Update weights
                W = W + alpha * delta_W

                # Update biases
                visible_layer_bias = visible_layer_bias + alpha * tf.reduce_mean(v0_state - v1_state, 0)
                hidden_layer_bias = hidden_layer_bias + alpha * tf.reduce_mean(h0_state - h1_state, 0)

                v0_state = v1_state

            if i_sample == len(batch_x)-1:
                err = error(batch_x[i_sample], v1_state)
                errors.append(err)
                weights.append(W)
                print ( 'Epoch: %d' % (epoch + 1),
                       "batch #: %i " % batch_number, "of %i" % (len(X_train)/batchsize),
                       "sample #: %i" % i_sample,
                       'reconstruction error: %f' % err)
        batch_number += 1




plt.plot(errors)
plt.ylabel('Error')
plt.xlabel('Batch')
plt.show()

# Create a test user ID to generate recommendations
mock_user_id = 24

# Select the input user
inputUser = X_train[mock_user_id-1].reshape(1, -1)

inputUser = tf.convert_to_tensor(X_train[mock_user_id-1],"float32")
v0 = inputUser

print(v0)
v0.shape

# Create a test tensor
v0test = tf.zeros([visibleUnits], tf.float32)
v0test.shape

# Feed in the user and reconstruct the input

hh0 = tf.nn.sigmoid(tf.matmul([v0], W) + hidden_layer_bias)

vv1 = tf.nn.sigmoid(tf.matmul(hh0, tf.transpose(W)) + visible_layer_bias)

rec = vv1

tf.maximum(rec,1)
for i in vv1:
    print(i)

# Create a DataFrame of recommendations
scored_travel_df_mock = pd.DataFrame({"category": ratings_df.columns})
scored_travel_df_mock = scored_travel_df_mock.assign(RecommendationScore = rec[0])
scored_travel_df_mock.sort_values(["RecommendationScore"], ascending=False).head(20)

# Create a DataFrame for our mock user
# Note that 0 scores indicate the user has not rated anything in that category
travel_df_mock = pd.DataFrame(ratings_df.loc["User " + str(mock_user_id),:]).reset_index().rename(columns={"index": "category"})
travel_df_mock

# Merge travel_df_mock with scored_travel_df_mock by category
merged_df_mock = scored_travel_df_mock.merge(travel_df_mock, on='category', how='outer')
merged_df_mock["Normalized User score"] = merged_df_mock["User " + str(mock_user_id)] / normalization_factor
merged_df_mock = merged_df_mock.drop(columns=["User " + str(mock_user_id)])
merged_df_mock.sort_values(["RecommendationScore"], ascending=False).head(20)

# Pandas for reading and preparing the data
import pandas as pd
# TensorFlow library. Used to implement machine learning models
import tensorflow as tf
# Numpy contains helpful functions for efficient mathematical calculations
import numpy as np
# Graph plotting library
import matplotlib.pyplot as plt
%matplotlib inline

# Read the data
ratings_df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m18/lesson_3/datasets/review-ratings.csv")
ratings_df

# Drop irrelevant column
ratings_df = ratings_df.drop("Unnamed: 25", axis=1)
# Check the data types
ratings_df.dtypes

# Set "User" as the index
ratings_df = ratings_df.set_index("User")
ratings_df

# Find problems for converting "Category 11" to float
try:
  ratings_df["Category 11"] = ratings_df["Category 11"].astype("float64")
except Exception as e:
  print(e)


# Find row with problem string
ratings_df.loc[ratings_df["Category 11"] == "2\t2.",:]


# Remove User 2713
ratings_df = ratings_df.drop("User 2713")

# Convert Category 11 to float
ratings_df["Category 11"] = ratings_df["Category 11"].astype("float64")

# Update the column names
travel_categories = ["churches",
                     "resorts",
                     "beaches",
                     "parks",
                     "theatres",
                     "museums",
                     "malls",
                     "zoo",
                     "restaurants",
                     "pubs/bars",
                     "local services",
                     "burger/pizza shops",
                     "hotels/other lodgings",
                     "juice bars",
                     "art galleries",
                     "dance clubs",
                     "swimming pools",
                     "gyms",
                     "bakeries",
                     "beauty & spas",
                     "cafes",
                     "view points",
                     "monuments",
                     "gardens"]
ratings_df.columns = travel_categories
ratings_df.head()

# Create variable for normalization
# Ratings are between 0-5
normalization_factor = 5

# Normalize the ratings
normalized_ratings = ratings_df / normalization_factor
normalized_ratings

# Create the training data
X_train = normalized_ratings.values
X_train

# Set values for our hidden nodes and visible nodes


# Set the bias of the visible layer to 0. This should use the number of unique travel categories.


# Set the bias of the hidden layer to 0. This will use hiddenUnits, which is
# the number of features we're going to learn


# Set the Weights to 0


v0 = tf.zeros([visibleUnits], tf.float32)
# testing to see if the matrix product works
tf.matmul([v0], W)

# Phase 1: Input Processing
# Define a function to return only the generated hidden states

    # probabilities of the hidden units

    # sample_h_given_X


# Print output of zeros input
h0 = hidden_layer(v0, W, hidden_layer_bias)
print("first 15 hidden states: ", h0[0][0:15])

# Define a function to return the reconstructed output

    # sample_v_given_h



# Get reconstructed output from zeros input
v1 = reconstructed_output(h0, W, visible_layer_bias)
print("hidden state shape: ", h0.shape)
print("v0 state shape:  ", v0.shape)
print("v1 state shape:  ", v1.shape)

# Set the error function, which in this case will be the Mean Absolute Error Function.


# Set the training variables


# Create dataset batches



v0_state=v0

# Train the model
for epoch in range(epochs):
    batch_number = 0
    for batch_x in train_ds:

        for i_sample in range(len(batch_x)):
            for k in range(K):
                v0_state = batch_x[i_sample]
                h0_state = hidden_layer(v0_state, W, hidden_layer_bias)
                v1_state = reconstructed_output(h0_state, W, visible_layer_bias)
                h1_state = hidden_layer(v1_state, W, hidden_layer_bias)

                delta_W = tf.matmul(tf.transpose([v0_state]), h0_state) - tf.matmul(tf.transpose([v1_state]), h1_state)

                # Update weights
                W = W + alpha * delta_W

                # Update biases
                visible_layer_bias = visible_layer_bias + alpha * tf.reduce_mean(v0_state - v1_state, 0)
                hidden_layer_bias = hidden_layer_bias + alpha * tf.reduce_mean(h0_state - h1_state, 0)

                v0_state = v1_state

            if i_sample == len(batch_x)-1:
                err = error(batch_x[i_sample], v1_state)
                errors.append(err)
                weights.append(W)
                print ( 'Epoch: %d' % (epoch + 1),
                       "batch #: %i " % batch_number, "of %i" % (len(X_train)/batchsize),
                       "sample #: %i" % i_sample,
                       'reconstruction error: %f' % err)
        batch_number += 1




plt.plot(errors)
plt.ylabel('Error')
plt.xlabel('Batch')
plt.show()

# Create a test user ID to generate recommendations
mock_user_id = 24

# Select the input user
inputUser = X_train[mock_user_id-1].reshape(1, -1)

inputUser = tf.convert_to_tensor(X_train[mock_user_id-1],"float32")
v0 = inputUser

print(v0)
v0.shape

# Create a test tensor
v0test = tf.zeros([visibleUnits], tf.float32)
v0test.shape

# Feed in the user and reconstruct the input

hh0 = tf.nn.sigmoid(tf.matmul([v0], W) + hidden_layer_bias)

vv1 = tf.nn.sigmoid(tf.matmul(hh0, tf.transpose(W)) + visible_layer_bias)

rec = vv1

tf.maximum(rec,1)
for i in vv1:
    print(i)

# Create a DataFrame of recommendations
scored_travel_df_mock = pd.DataFrame({"category": ratings_df.columns})
scored_travel_df_mock = scored_travel_df_mock.assign(RecommendationScore = rec[0])
scored_travel_df_mock.sort_values(["RecommendationScore"], ascending=False).head(20)

# Create a DataFrame for our mock user
# Note that 0 scores indicate the user has not rated anything in that category
travel_df_mock = pd.DataFrame(ratings_df.loc["User " + str(mock_user_id),:]).reset_index().rename(columns={"index": "category"})
travel_df_mock

# Merge travel_df_mock with scored_travel_df_mock by category
merged_df_mock = scored_travel_df_mock.merge(travel_df_mock, on='category', how='outer')
merged_df_mock["Normalized User score"] = merged_df_mock["User " + str(mock_user_id)] / normalization_factor
merged_df_mock = merged_df_mock.drop(columns=["User " + str(mock_user_id)])
merged_df_mock.sort_values(["RecommendationScore"], ascending=False).head(20)

import pandas as pd

# Read the data
ratings_df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m18/lesson_3/datasets/ratings.txt",
                         sep=" ",
                         header=None,
                         names=["user_id", "movie_id", "rating"])
ratings_df

# Remove duplicates on user_id and movie_id, keeping the last row
ratings_df = ratings_df.drop_duplicates(subset=["user_id", "movie_id"], keep="last")
ratings_df

# Pivot the DataFrame so user_id is the index, movie_id is columns,
# and ratings are the values
ratings_matrix = ratings_df.pivot(index="user_id", columns="movie_id", values="rating")
ratings_matrix

# Fill NAs with 0
ratings_matrix = ratings_matrix.fillna(0)
ratings_matrix

# Create variable for normalization
# Ratings are between 0-5
normalization_factor = 5

# Normalize the ratings
normalized_ratings = ratings_matrix / normalization_factor
normalized_ratings

# Save the training data as an array of values from the normalized DataFrame
X_train = normalized_ratings.values
X_train



import pandas as pd

# Read the data
ratings_df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m18/lesson_3/datasets/ratings.txt",
                         sep=" ",
                         header=None,
                         names=["user_id", "movie_id", "rating"])
ratings_df

# Remove duplicates on user_id and movie_id, keeping the last row


# Pivot the DataFrame so user_id is the index, movie_id is columns,
# and ratings are the values


# Fill NAs with 0


# Create variable for normalization
# Ratings are between 0-5


# Normalize the ratings


# Save the training data as an array of values from the normalized DataFrame




import pandas as pd
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

# Read the data
ratings_df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m18/lesson_3/datasets/ratings.txt",
                         sep=" ",
                         header=None,
                         names=["user_id", "movie_id", "rating"])
ratings_df

# Remove duplicates on user_id and movie_id, keeping the last row
ratings_df = ratings_df.drop_duplicates(subset=["user_id", "movie_id"], keep="last")
ratings_df

# Pivot the DataFrame and fill NAs
ratings_matrix = ratings_df.pivot(index="user_id", columns="movie_id", values="rating").fillna(0)
ratings_matrix

# Create variable for normalization
# Ratings are between 0-5
normalization_factor = 5

# Normalize the ratings
normalized_ratings = ratings_matrix / normalization_factor
normalized_ratings

X_train = normalized_ratings.values
X_train

# Set the number of neurons for the layers
hiddenUnits = 20
visibleUnits =  len(ratings_matrix.columns)

# Set the bias of the visible layer to 0. This should use the number of unique movies.
visible_layer_bias = tf.Variable(tf.zeros([visibleUnits]), tf.float32)

# Set the bias of the hidden layer to 0. This will use hiddenUnits, which is
# the number of features we're going to learn
hidden_layer_bias = tf.Variable(tf.zeros([hiddenUnits]), tf.float32)

# Set the Weights to 0
W = tf.Variable(tf.zeros([visibleUnits, hiddenUnits]), tf.float32)

v0 = tf.zeros([visibleUnits], tf.float32)
# testing to see if the matrix product works
tf.matmul([v0], W)

# Phase 1: Input Processing
# Define a function to return only the generated hidden states
def hidden_layer(v0_state, W, hb):
    # probabilities of the hidden units
    h0_prob = tf.nn.sigmoid(tf.matmul([v0_state], W) + hb)
    # sample_h_given_X
    h0_state = tf.nn.relu(tf.sign(h0_prob - tf.random.uniform(tf.shape(h0_prob))))
    return h0_state

# Print output of zeros input
h0 = hidden_layer(v0, W, hidden_layer_bias)
print("first 15 hidden states: ", h0[0][0:15])

# Define a function to return the reconstructed output
def reconstructed_output(h0_state, W, vb):
    v1_prob = tf.nn.sigmoid(tf.matmul(h0_state, tf.transpose(W)) + vb)
    # sample_v_given_h
    v1_state = tf.nn.relu(tf.sign(v1_prob - tf.random.uniform(tf.shape(v1_prob))))
    return v1_state[0]

# Get reconstructed output from zeros input
v1 = reconstructed_output(h0, W, visible_layer_bias)
print("hidden state shape: ", h0.shape)
print("v0 state shape:  ", v0.shape)
print("v1 state shape:  ", v1.shape)

# Set the error function, which in this case will be the Mean Absolute Error Function.
def error(v0_state, v1_state):
    return tf.reduce_mean(tf.square(v0_state - v1_state))

err = tf.reduce_mean(tf.square(v0 - v1))
print("error" , err.numpy())

# Set the training variables
epochs = 5
batchsize = 200
errors = []
weights = []
K=1
alpha = 0.1

# Create dataset batches
train_ds = \
    tf.data.Dataset.from_tensor_slices((np.float32(X_train))).batch(batchsize)



v0_state=v0

# Train the model
for epoch in range(epochs):
    batch_number = 0
    for batch_x in train_ds:

        for i_sample in range(len(batch_x)):
            for k in range(K):
                v0_state = batch_x[i_sample]
                h0_state = hidden_layer(v0_state, W, hidden_layer_bias)
                v1_state = reconstructed_output(h0_state, W, visible_layer_bias)
                h1_state = hidden_layer(v1_state, W, hidden_layer_bias)

                delta_W = tf.matmul(tf.transpose([v0_state]), h0_state) - tf.matmul(tf.transpose([v1_state]), h1_state)

                # Update weights
                W = W + alpha * delta_W

                # Update biases
                visible_layer_bias = visible_layer_bias + alpha * tf.reduce_mean(v0_state - v1_state, 0)
                hidden_layer_bias = hidden_layer_bias + alpha * tf.reduce_mean(h0_state - h1_state, 0)

                v0_state = v1_state

            if i_sample == len(batch_x)-1:
                err = error(batch_x[i_sample], v1_state)
                errors.append(err)
                weights.append(W)
                print ( 'Epoch: %d' % (epoch + 1),
                       "batch #: %i " % batch_number, "of %i" % (len(X_train)/batchsize),
                       "sample #: %i" % i_sample,
                       'reconstruction error: %f' % err)
        batch_number += 1

# Plot the errors
plt.plot(errors)
plt.ylabel('Error')
plt.xlabel('Epoch')
plt.show()

# Collect the weights and biases so we can rebuild the model without re-training
print("Weights: ", W)
print("Hidden Layer Biases: ", hidden_layer_bias)
print("Visible Layer Biases: ", visible_layer_bias)

# Convert the weights into a Pandas DataFrame
weight_settings = pd.DataFrame(data=W.numpy())

# Save weights to CSV
weight_settings.to_csv("rbm_weights.csv", index=False)

# Convert the biases to Pandas DataFrame and export to CSVs
hidden_bias_settings = pd.DataFrame(data=hidden_layer_bias.numpy())
hidden_bias_settings.to_csv("hidden_layer_bias.csv", index=False)
visible_bias_settings = pd.DataFrame(visible_layer_bias.numpy())
visible_bias_settings.to_csv("visible_layer_bias.csv", index=False)

# Read weights and convert back to Tensor
weight_settings = pd.read_csv("rbm_weights.csv")
weights_tensor = tf.constant(weight_settings.values, tf.float32)
weights_tensor

# Read hidden layer biases and convert back to Tensor
hidden_bias_settings = pd.read_csv("hidden_layer_bias.csv")
hidden_tensor = tf.constant(hidden_bias_settings.values, tf.float32)
hidden_tensor

# Read visible layer biases and convert back to Tensor
visible_bias_settings = pd.read_csv("visible_layer_bias.csv")
visible_tensor = tf.constant(visible_bias_settings.values, tf.float32)
visible_tensor

# Create a function to reconstruct ratings data
def get_user_recommendations(user_id):
    inputUser = tf.convert_to_tensor(normalized_ratings.loc[user_id].values,"float32")
    v0 = inputUser

    hh0 = tf.nn.sigmoid(tf.matmul([v0], weights_tensor) + hidden_tensor)

    vv1 = tf.nn.sigmoid(tf.matmul(hh0, tf.transpose(weights_tensor)) + tf.transpose(visible_tensor))

    rec = vv1
    return rec

# Test recommendation
test_user = 1024

recommendation = get_user_recommendations(test_user)
recommendation

# Convert recommendation to DataFrame

rec_df = pd.DataFrame({"movie_id": ratings_matrix.columns, "user_id": test_user})
rec_df = rec_df.assign(RecommendationScore = recommendation[0])

# Sort recommendations
rec_df.sort_values(["RecommendationScore"], ascending=False).head(20)

# Merge recommendation scores with original dataset ratings
merged_df = rec_df.merge(ratings_df, on=['movie_id', 'user_id'], how='outer')
merged_df.sort_values(["RecommendationScore"], ascending=False).head(20)

import pandas as pd
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

# Read the data
ratings_df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m18/lesson_3/datasets/ratings.txt",
                         sep=" ",
                         header=None,
                         names=["user_id", "movie_id", "rating"])
ratings_df

# Remove duplicates on user_id and movie_id, keeping the last row
ratings_df = ratings_df.drop_duplicates(subset=["user_id", "movie_id"], keep="last")
ratings_df

# Pivot the DataFrame and fill NAs
ratings_matrix = ratings_df.pivot(index="user_id", columns="movie_id", values="rating").fillna(0)
ratings_matrix

# Create variable for normalization
# Ratings are between 0-5
normalization_factor = 5

# Normalize the ratings
normalized_ratings = ratings_matrix / normalization_factor
normalized_ratings

X_train = normalized_ratings.values
X_train

# Set the number of neurons for the layers


# Set the bias of the visible layer to 0. This should use the number of unique movies.


# Set the bias of the hidden layer to 0. This will use hiddenUnits, which is
# the number of features we're going to learn


# Set the Weights to 0


v0 = tf.zeros([visibleUnits], tf.float32)
# testing to see if the matrix product works
tf.matmul([v0], W)

# Phase 1: Input Processing
# Define a function to return only the generated hidden states
def hidden_layer(v0_state, W, hb):
    # probabilities of the hidden units
    h0_prob = tf.nn.sigmoid(tf.matmul([v0_state], W) + hb)
    # sample_h_given_X
    h0_state = tf.nn.relu(tf.sign(h0_prob - tf.random.uniform(tf.shape(h0_prob))))
    return h0_state

# Print output of zeros input
h0 = hidden_layer(v0, W, hidden_layer_bias)
print("first 15 hidden states: ", h0[0][0:15])

# Define a function to return the reconstructed output
def reconstructed_output(h0_state, W, vb):
    v1_prob = tf.nn.sigmoid(tf.matmul(h0_state, tf.transpose(W)) + vb)
    # sample_v_given_h
    v1_state = tf.nn.relu(tf.sign(v1_prob - tf.random.uniform(tf.shape(v1_prob))))
    return v1_state[0]

# Get reconstructed output from zeros input
v1 = reconstructed_output(h0, W, visible_layer_bias)
print("hidden state shape: ", h0.shape)
print("v0 state shape:  ", v0.shape)
print("v1 state shape:  ", v1.shape)

# Set the error function, which in this case will be the Mean Absolute Error Function.
def error(v0_state, v1_state):
    return tf.reduce_mean(tf.square(v0_state - v1_state))

err = tf.reduce_mean(tf.square(v0 - v1))
print("error" , err.numpy())

# Set the training variables


# Create dataset batches




v0_state=v0

# Train the model
for epoch in range(epochs):
    batch_number = 0
    for batch_x in train_ds:

        for i_sample in range(len(batch_x)):
            for k in range(K):
                v0_state = batch_x[i_sample]
                h0_state = hidden_layer(v0_state, W, hidden_layer_bias)
                v1_state = reconstructed_output(h0_state, W, visible_layer_bias)
                h1_state = hidden_layer(v1_state, W, hidden_layer_bias)

                delta_W = tf.matmul(tf.transpose([v0_state]), h0_state) - tf.matmul(tf.transpose([v1_state]), h1_state)

                # Update weights
                W = W + alpha * delta_W

                # Update biases
                visible_layer_bias = visible_layer_bias + alpha * tf.reduce_mean(v0_state - v1_state, 0)
                hidden_layer_bias = hidden_layer_bias + alpha * tf.reduce_mean(h0_state - h1_state, 0)

                v0_state = v1_state

            if i_sample == len(batch_x)-1:
                err = error(batch_x[i_sample], v1_state)
                errors.append(err)
                weights.append(W)
                print ( 'Epoch: %d' % (epoch + 1),
                       "batch #: %i " % batch_number, "of %i" % (len(X_train)/batchsize),
                       "sample #: %i" % i_sample,
                       'reconstruction error: %f' % err)
        batch_number += 1

# Plot the errors
plt.plot(errors)
plt.ylabel('Error')
plt.xlabel('Epoch')
plt.show()

# Collect the weights and biases so we can rebuild the model without re-training


# Convert the weights into a Pandas DataFrame


# Save weights to CSV


# Convert the biases to Pandas DataFrame and export to CSVs


# Read weights and convert back to Tensor


# Read hidden layer biases and convert back to Tensor


# Read visible layer biases and convert back to Tensor


# Create a function to reconstruct ratings data


# Test recommendation


# Convert recommendation to DataFrame



# Sort recommendations


# Merge recommendation scores with original dataset ratings


import pandas as pd
import tensorflow as tf
import numpy as np

# Import our utilities functions
import utils

# Get the original DataFrame
df = utils.get_data()
df.head()

# Get the normalized ratings
normalized_ratings = utils.get_normalized_data()
normalized_ratings.head()

# Get the weights and bias tensors
W = utils.weights()
hb = utils.hidden_bias()
vb = utils.visible_bias()

# Test generating user recommendation
test_user = 1024

# Get user ratings
user_ratings = normalized_ratings.loc[test_user]

# Generate recommendations for user
rec = utils.generate_recommendation(user_ratings, W, vb, hb)

# Construct user recommendation DataFrame
user_recommendation = pd.DataFrame({"movie_id": normalized_ratings.columns, "user_id": test_user})
user_recommendation = user_recommendation.assign(RecommendationScore = rec[0].numpy())

# View sorted user recommendation
user_recommendation.sort_values(["RecommendationScore"], ascending=False).head(20)

# Merge user recommendation scores with original dataset ratings
merged_df = user_recommendation.merge(df, on=['movie_id', 'user_id'], how='outer')
merged_df.sort_values(["RecommendationScore"], ascending=False).head(20)

import pandas as pd
import tensorflow as tf
import numpy as np

# Import our utilities functions
import utils

# Get the original DataFrame
df = utils.get_data()
df.head()

# Get the normalized ratings
normalized_ratings = utils.get_normalized_data()
normalized_ratings.head()

# Get the weights and bias tensors
W = utils.weights()
hb = utils.hidden_bias()
vb = utils.visible_bias()

# Test generating user recommendation
test_user = 1024

# Get user ratings
user_ratings = normalized_ratings.loc[test_user]

# Generate recommendations for user
rec = utils.generate_recommendation(user_ratings, W, vb, hb)

# Construct user recommendation DataFrame
user_recommendation = pd.DataFrame({"movie_id": normalized_ratings.columns, "user_id": test_user})
user_recommendation = user_recommendation.assign(RecommendationScore = rec[0].numpy())

# View sorted user recommendation
user_recommendation.sort_values(["RecommendationScore"], ascending=False).head(20)

# Merge user recommendation scores with original dataset ratings
merged_df = user_recommendation.merge(df, on=['movie_id', 'user_id'], how='outer')
merged_df.sort_values(["RecommendationScore"], ascending=False).head(20)

import pandas as pd
import tensorflow as tf
import numpy as np

# Import our utilities functions
import utils

# Get the original DataFrame
df = utils.get_data()
df.head()

# Get the normalized ratings
normalized_ratings = utils.get_normalized_data()
normalized_ratings.head()

# Get the weights and bias tensors
W = utils.weights()
hb = utils.hidden_bias()
vb = utils.visible_bias()

# Get the users so we can send the users back into the model
users = normalized_ratings.index
users

# Create an empty DataFrame to store the recommendations
recommendation_scores = pd.DataFrame(columns=["movie_id", "user_id", "RecommendationScore"])
recommendation_scores

# Get recommendations for every user
for user in users:
    # Get user's ratings
    user_ratings = normalized_ratings.loc[user]

    # Generate the recommendations
    rec = utils.generate_recommendation(user_ratings, W, vb, hb)

    # Construct user DataFrame
    new_recommendation = pd.DataFrame({"movie_id": normalized_ratings.columns, "user_id": user})
    new_recommendation = new_recommendation.assign(RecommendationScore = rec[0].numpy())

    # Add recommendation to DataFrame
    recommendation_scores = pd.concat([recommendation_scores, new_recommendation])

recommendation_scores

# Merge all recommendation scores with original dataset ratings
# Inner merge because we cannot perform calculations on NaN values to evaluate the model
merged_df = recommendation_scores.merge(df, on=['movie_id', 'user_id'], how='inner')
merged_df

# normalize rating column
merged_df["rating"] = merged_df["rating"] / 5
merged_df

# Calculate RMSE
from sklearn.metrics import mean_squared_error

rmse = mean_squared_error(merged_df["rating"], merged_df["RecommendationScore"], squared=False)
print(rmse)

import pandas as pd
import tensorflow as tf
import numpy as np

# Import our utilities functions
import utils

# Get the original DataFrame
df = utils.get_data()
df.head()

# Get the normalized ratings
normalized_ratings = utils.get_normalized_data()
normalized_ratings.head()

# Get the weights and bias tensors
W = utils.weights()
hb = utils.hidden_bias()
vb = utils.visible_bias()

# Get the users so we can send the users back into the model


# Create an empty DataFrame to store the recommendations


# Get recommendations for every user

    # Get user's ratings


    # Generate the recommendations


    # Construct user DataFrame


    # Add recommendation to DataFrame




# Merge all recommendation scores with original dataset ratings
# Inner merge because we cannot perform calculations on NaN values to evaluate the model


# normalize rating column


# Calculate RMSE
from sklearn.metrics import mean_squared_error



import pandas as pd
import tensorflow as tf
import numpy as np

# Import our utilities functions
import utils

# Get the original DataFrame for new users
df = utils.get_new_data()
df.head()

# Get the normalized ratings
normalized_ratings = utils.get_normalized_new_data()
normalized_ratings.head()

# Get the weights and bias tensors
W = utils.weights()
hb = utils.hidden_bias()
vb = utils.visible_bias()

# Get new users
user_list = normalized_ratings.index

# Select a test user
test_user = user_list[0]

# Get the ratings row for the test user
user_ratings = normalized_ratings.loc[test_user]
user_ratings

# Get the recommendations for the user
rec = utils.generate_recommendation(user_ratings, W, vb, hb)

# Construct user recommendation DataFrame
user_recommendation = pd.DataFrame({"movie_id": normalized_ratings.columns, "user_id": test_user})
user_recommendation = user_recommendation.assign(RecommendationScore = rec[0].numpy())

# View sorted user recommendation
user_recommendation.sort_values(["RecommendationScore"], ascending=False).head(20)

# Merge user recommendation scores with original dataset ratings
merged_df = user_recommendation.merge(df, on=['movie_id', 'user_id'], how='outer')
merged_df.sort_values(["RecommendationScore"], ascending=False).head(20)

# Reduce the merged DataFrame to locate the unrated movies that have a
# recommendation score higher than 0.5 to find the movies to recommend
movies_to_recommend = merged_df.loc[
    (merged_df["RecommendationScore"] > 0.5) & (pd.isna(merged_df["rating"])),
    "movie_id"
].to_list()
movies_to_recommend

import pandas as pd
import tensorflow as tf
import numpy as np

# Import our utilities functions
import utils

# Get the original DataFrame for new users


# Get the normalized ratings


# Get the weights and bias tensors
W = utils.weights()
hb = utils.hidden_bias()
vb = utils.visible_bias()

# Get new users


# Select a test user


# Get the ratings row for the test user


# Get the recommendations for the user


# Construct user recommendation DataFrame


# View sorted user recommendation


# Merge user recommendation scores with original dataset ratings


# Reduce the merged DataFrame to locate the unrated movies that have a
# recommendation score higher than 0.5 to find the movies to recommend


# Import dependencies
import requests
from PIL import Image

img_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_2/datasets/an2i_left_angry_open.png"

# First lets try the Image.open function
Image.open(img_url)

# The Image.open function cannot open web urls directly
# To use Image.open, we have to start with the requests library

# Note the use of requests with the .raw attribute and the stream parameter
response = requests.get(img_url, stream=True).raw
img = Image.open(response)
img

# Once imported, the Image library has lots of useful functions

img.size

img.mode

img.format

img.resize((128, 120))

img.rotate(90)

img.transpose(Image.FLIP_LEFT_RIGHT)

# Import dependencies
import requests
from PIL import Image

img_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_2/datasets/an2i_left_angry_open.png"

# First lets try the Image.open function


# The Image.open function cannot open web urls directly
# To use Image.open, we have to start with the requests library

# Note the use of requests with the .raw attribute and the stream parameter


# Once imported, the Image library has lots of useful functions












# Import dependencies
import requests
from PIL import Image

img_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_1/datasets/fungi_images/H1_100a_1.jpg"

# Use the requests library to request the image
response = requests.get(img_url, stream=True).raw

# Use Image.open to open the image
img = Image.open(response)

# View the image
img

# NOTE: This dataset is of microscopic fungi samples
# The images may not look recognizable but that is expected

# View the size of the img
img.size

# View the format of the img
img.format

# Resize the image to 128,128
img.resize((128, 128))

# Rotate the image 45 degrees
img.rotate(45)

# Research what other methods are available
# https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image
# Try a few other methods here

img.crop((100, 50, 400, 400))

img.effect_spread(50)

from PIL import ImageFilter
img.filter(ImageFilter.SHARPEN)



# Import dependencies
import requests
from PIL import Image

img_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_1/datasets/fungi_images/H1_100a_1.jpg"

# Use the requests library to request the image

# Use Image.open to open the image

# View the image


# NOTE: This dataset is of microscopic fungi samples
# The images may not look recognizable but that is expected

# View the size of the img


# View the format of the img


# Resize the image to 128,128


# Rotate the image 45 degrees


# Research what other methods are available
# https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image
# Try a few other methods here





# Import dependencies
import requests
from PIL import Image

import pandas as pd

path = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_1/datasets/csvs/files_list.csv"

filenames_df = pd.read_csv(path)
filenames_df.head()

# Build a list of images using a for loop

# Define the base_url
base_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_1/datasets/faces_data/"

# Create an empty list for the urls
img_urls = []

# Loop through the DataFrame and build and append the full image urls
for i in range(len(filenames_df)):
    filename = filenames_df.iloc[i,0]
    img_urls.append(base_url + filename)

img_urls[0:5]

# Check the number of urls
len(img_urls)

# Create and empty list for images
imgs = []

# Loop through ONLY THE FIRST FIVE image_urls to open and append each image
num_imgs = 5

for i in range(num_imgs):
    img_url = img_urls[i]
    # Print a statement to show progress
    print(f"{i}/{num_imgs}: Attempting to import {img_url}")

    # Use requests.get along with the stream parameter and raw attribute
    response = requests.get(img_url, stream=True).raw

    # Append each img to the imgs list
    imgs.append(Image.open(response))

# View the first image to confirm
imgs[0]


# After confirming the code works, copy it to this cell and import all images
# Create and empty list for images
imgs = []

# Loop through ALL image_urls to open and append each image
num_imgs = len(img_urls)

for i in range(num_imgs):
    img_url = img_urls[i]
    # Print a statement to show progress
    print(f"{i}/{num_imgs}: Attempting to import {img_url}")

    # Use requests.get along with the stream parameter and raw attribute
    response = requests.get(img_url, stream=True).raw

    # Append each img to the imgs list
    imgs.append(Image.open(response))

# View the first image to confirm
imgs[0]

# Import the modules
from google.colab import drive
import pickle

drive.mount('/content/drive')

# Open a new file named 'img.pkl' with write permission
with open('/content/drive/My Drive/img.pkl', 'wb') as file:
    # Use pickle.dump to store the list of images
    pickle.dump(imgs, file)

# Load the images from the pkl file
with open('/content/drive/My Drive/img.pkl', 'rb') as file:
    recalled_imgs = pickle.load(file)

recalled_imgs[0]

# You can use this code to open the pkl file in other colab notebooks
from google.colab import drive
import pickle
drive.mount('/content/drive')

with open('/content/drive/My Drive/img.pkl', 'rb') as file:
    imgs = pickle.load(file)

imgs[0]

# Import dependencies
import requests
from PIL import Image

import pandas as pd

path = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_1/datasets/csvs/files_list.csv"

filenames_df = pd.read_csv(path)
filenames_df.head()

# Build a list of images using a for loop

# Define the base_url
base_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_1/datasets/faces_data/"

# Create an empty list for the urls


# Loop through the DataFrame and build and append the full image urls


# Check the number of urls


# Create and empty list for images

# Loop through ONLY THE FIRST FIVE image_urls to open and append each image

    # Print a statement to show progress

    # Use requests.get along with the stream parameter and raw attribute

    # Append each img to the imgs list
    
    
# View the first image to confirm



# After confirming the code works, copy it to this cell and import all images
# Create and empty list for images
# Loop through ALL image_urls to open and append each image

    # Print a statement to show progress

    # Use requests.get along with the stream parameter and raw attribute

    # Append each img to the imgs list

# View the first image to confirm


# Import the modules
from google.colab import drive
import pickle

drive.mount('/content/drive')

# Open a new file named 'img.pkl' with write permission

    # Use pickle.dump to store the list of images


# Load the images from the pkl file


# You can use this code to open the pkl file in other colab notebooks
from google.colab import drive
import pickle
drive.mount('/content/drive')

with open('/content/drive/My Drive/img.pkl', 'rb') as file:
    imgs = pickle.load(file)

imgs[0]

# Import dependencies
import requests
from PIL import Image
import pandas as pd

path = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_1/datasets/csvs/fungi_files.csv"

# Read the csv file in and view the first few rows
filenames_df = pd.read_csv(path)
filenames_df.head()

# Build a list of images using a for loop

# Define the base_url
base_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_1/datasets/fungi_images/"

# Create an empty list for the urls
img_urls = []

# Loop through the DataFrame and build and append the full image urls
for i in range(len(filenames_df)):
    filename = filenames_df.iloc[i,0]
    img_urls.append(base_url + filename)

img_urls[0:5]

# Check the number of urls
len(img_urls)

# Create and empty list for images
imgs = []

# Loop through ONLY THE FIRST 20 image_urls to open and append each image
num_imgs = 20

for i in range(num_imgs):
    img_url = img_urls[i]
    # Print a statement to show progress
    print(f"{i}/{num_imgs}: Attempting to import {img_url}")

    # Use requests.get along with the stream parameter and raw attribute
    response = requests.get(img_url, stream=True).raw

    # Append each img to the imgs list
    imgs.append(Image.open(response))

# View the first image to confirm
imgs[0]


# Import the modules
from google.colab import drive
import pickle

drive.mount('/content/drive/')

# Open a new file named 'fungi.pkl' with write permission
with open('/content/drive/MyDrive/fungi.pkl', 'wb') as file:
    # Use pickle.dump to store the list of images
    pickle.dump(imgs, file)

# Load the images from the pkl file
with open('/content/drive/MyDrive/fungi.pkl', 'rb') as file:
    recalled_imgs = pickle.load(file)

recalled_imgs[0]

# Import dependencies
import requests
from PIL import Image
import pandas as pd

path = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_1/datasets/csvs/fungi_files.csv"

# Read the csv file in and view the first few rows
filenames_df = pd.read_csv(path)
filenames_df.head()

# Build a list of images using a for loop

# Define the base_url
base_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_1/datasets/fungi_images/"

# Create an empty list for the urls


# Loop through the DataFrame and build and append the full image urls


# Check the number of urls


# Create and empty list for images

# Loop through ONLY THE FIRST 20 image_urls to open and append each image


    # Print a statement to show progress
    
    # Use requests.get along with the stream parameter and raw attribute

    # Append each img to the imgs list

# View the first image to confirm


# Import the modules
from google.colab import drive
import pickle

drive.mount('/content/drive/')

# Open a new file named 'fungi.pkl' with write permission


    # Use pickle.dump to store the list of images
    

# Load the images from the pkl file



import pandas as pd
import numpy as np
from PIL import Image

# Import images from a pickle file
# This pickle file is hosted at a url, so some
# additional code is needed
import pickle
import requests
import io

path = 'https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_1/datasets/pickles/face_images.pkl'

imgs = pickle.load(io.BytesIO(requests.get(path).content))

imgs[1]

# Check the size of the second image
imgs[1].size

# Get all the sizes into a list, then convert to a set
sizes = set([img.size for img in imgs])
sizes

# Convert the images to the middle 64, 60 size
# Lets try with the first image

target_size = (64, 60)
imgs[1].resize(target_size, resample = Image.LANCZOS)

# Using the same syntax, use a for loop to apply resizing to all images

resized_imgs = [img.resize(target_size, resample = Image.LANCZOS) for img in imgs]
resized_imgs[1]

# Lets examine our data numerically.
# This is most easily done by converting the image to a numpy array

# Convert one image to a NumPy array
pixel_values = np.array(resized_imgs[0])

# Display the pixel values
print("Pixel Values:")
print(pixel_values)

# Convert all images to floating point numpy arrays
float_images = [np.array(img).astype(np.float32) for img in resized_imgs]

# Display the pixel values of the first image
print("Pixel Values:")
print(float_images[0])

# To normalize pixel values to a range between 0 and 1,
# we need to divide all pixel values by the max of 255

normalized_images = [img/255 for img in float_images]

# Display the pixel values of the first image
print("Pixel Values:")
print(normalized_images[0])

# Import the modules
from google.colab import drive
import pickle

drive.mount('/content/drive/')

# Open a new file named 'img_preprocessed.pkl' with write permission
with open('/content/drive/MyDrive/img_preprocessed.pkl', 'wb') as file:
    # Use pickle.dump to store the list of images
    pickle.dump(imgs, file)

# Save our preprocessed images to a new pickle file

with open('/content/drive/My Drive/img_preprocessed.pkl', 'wb') as file:
    pickle.dump(normalized_images, file)



import pandas as pd
import numpy as np
from PIL import Image

# Import images from a pickle file
# This pickle file is hosted at a url, so some
# additional code is needed
import pickle
import requests
import io

path = 'https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_1/datasets/pickles/face_images.pkl'

imgs = pickle.load(io.BytesIO(requests.get(path).content))

imgs[1]

# Check the size of the second image


# Get all the sizes into a list, then convert to a set


# Convert the images to the middle 64, 60 size
# Lets try with the first image


# Using the same syntax, use a for loop to apply resizing to all images



# Lets examine our data numerically.
# This is most easily done by converting the image to a numpy array

# Convert one image to a NumPy array

# Display the pixel values


# Convert all images to floating point numpy arrays

# Display the pixel values of the first image


# To normalize pixel values to a range between 0 and 1,
# we need to divide all pixel values by the max of 255


# Display the pixel values of the first image


# Import the modules
from google.colab import drive
import pickle

drive.mount('/content/drive/')

# Open a new file named 'img_preprocessed.pkl' with write permission
with open('/content/drive/MyDrive/img_preprocessed.pkl', 'wb') as file:
    # Use pickle.dump to store the list of images
    pickle.dump(imgs, file)

# Save our preprocessed images to a new pickle file

with open('/content/drive/My Drive/img_preprocessed.pkl', 'wb') as file:
    pickle.dump(normalized_images, file)

import pandas as pd
import numpy as np
from PIL import Image

# Import images from the pickle file
# Import images from a pickle file
# This pickle file is hosted at a url, so some
# additional code is needed
import pickle
import requests
import io

path = 'https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_1/datasets/pickles/fungi_full.pkl'

imgs = pickle.load(io.BytesIO(requests.get(path).content))

imgs[1]

# Check the size of the second image
imgs[1].size

# Get all the sizes into a list, then convert to a set
sizes = set([img.size for img in imgs])

# View the set of sizes
sizes

# Convert the images to the middle 250, 250 size
# Lets try with the first image

target_size = (250,250)
imgs[1].resize(target_size, resample = Image.LANCZOS)

# Using the same syntax, use a for loop to apply resizing to all images
resized_imgs = [img.resize(target_size, resample = Image.LANCZOS) for img in imgs]
resized_imgs[1]

# Lets examine our data numerically.
# This is most easily done by converting the image to a numpy array

# Convert one image to a NumPy array
pixel_values = np.array(resized_imgs[0])

# Display the pixel values
print("Pixel Values:")
print(pixel_values)

# Convert all images to floating point numpy arrays
float_images = [np.array(img).astype(np.float32) for img in resized_imgs]

# Display the pixel values of the first image
print("Pixel Values:")
print(float_images[0])

# To normalize pixel values to a range between 0 and 1,
# we need to divide all pixel values by the max of 255

normalized_images = [img/255 for img in float_images]

# Display the pixel values of the first image
print("Pixel Values:")
print(normalized_images[0])

from google.colab import drive

drive.mount('/content/drive/')
path = '/content/drive/My Drive/preprocessed_fungi.pkl'

# Save our preprocessed images to a new pickle file
with open(path, 'wb') as file:
    pickle.dump(normalized_images, file)

import pandas as pd
import numpy as np
from PIL import Image

# Import images from the pickle file
# Import images from a pickle file
# This pickle file is hosted at a url, so some
# additional code is needed
import pickle
import requests
import io

path = 'https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_1/datasets/pickles/fungi_full.pkl'

imgs = pickle.load(io.BytesIO(requests.get(path).content))

imgs[1]

# Check the size of the second image


# Get all the sizes into a list, then convert to a set

# View the set of sizes


# Convert the images to the middle 250, 250 size
# Lets try with the first image



# Using the same syntax, use a for loop to apply resizing to all images


# Lets examine our data numerically.
# This is most easily done by converting the image to a numpy array

# Convert one image to a NumPy array


# Display the pixel values


# Convert all images to floating point numpy arrays

# Display the pixel values of the first image


# To normalize pixel values to a range between 0 and 1,
# we need to divide all pixel values by the max of 255


# Display the pixel values of the first image


from google.colab import drive

drive.mount('/content/drive/')
path = '/content/drive/My Drive/preprocessed_fungi.pkl'

# Save our preprocessed images to a new pickle file
with open(path, 'wb') as file:
    pickle.dump(normalized_images, file)

import pandas as pd
import numpy as np
from PIL import Image

# Print the first few image filenames
path = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_1/datasets/csvs/files_list.csv"

filenames_df = pd.read_csv(path)
filenames_df.head()

# First, remove the .png file extension, then split into four new columns.
filenames_df[['userid', 'pose', 'expression', 'eyes']] = filenames_df['files']\
                                                            .str.replace('.png', '', regex=False)\
                                                            .str.split('_', expand=True)
filenames_df.head()

from google.colab import drive
import pickle
drive.mount('/content/drive')
# For our purposes, we'll select the eyes column as 'y'
y = filenames_df['eyes']

# And we'll export this as another pkl file
with open('/content/drive/My Drive/y.pkl', 'wb') as file:
    pickle.dump(y, file)



import pandas as pd
import numpy as np
from PIL import Image

# Print the first few image filenames
path = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_1/datasets/csvs/files_list.csv"



# First, remove the .png file extension, then split into four new columns.


from google.colab import drive
import pickle
drive.mount('/content/drive')
# For our purposes, we'll select the eyes column as 'y'

# And we'll export this as another pkl file




import pandas as pd
import numpy as np
from PIL import Image

# Print the first few image filenames
path = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_1/datasets/csvs/fungi_files.csv"

filenames_df = pd.read_csv(path)
filenames_df.head()

# First, remove the .jpg file extension, 
# then split into three new columns: 'class', 'sample', and 'image'
filenames_df[['class', 'sample', 'image']] = filenames_df['name']\
                                                            .str.replace('.jpg', '', regex=False)\
                                                            .str.split('_', expand=True)
filenames_df.head()

from google.colab import drive
import pickle
drive.mount('/content/drive')
# For our purposes, we'll select the class column as 'y'
y = filenames_df['class']

# And we'll export this as another pkl file
with open('/content/drive/My Drive/fungi_y.pkl', 'wb') as file:
    pickle.dump(y, file)



import pandas as pd
import numpy as np
from PIL import Image

path = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_1/datasets/csvs/fungi_files.csv"

# Read the csv into a DataFrame

# Display the first few rows


# First, remove the .jpg file extension, 
# then split into three new columns: 'class', 'sample', and 'image'



from google.colab import drive
import pickle
drive.mount('/content/drive')
# For our purposes, we'll select the class column as 'y'


# And we'll export this as another pkl file




import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import numpy as np
import requests
import pickle
import io

# Import the preprocessed data
X_preprocessed_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_2/datasets/pickles/face_images_preprocessed.pkl"
y_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_2/datasets/pickles/sunglasses_y.pkl"

X = pickle.load(io.BytesIO(requests.get(X_preprocessed_url).content))
y = pickle.load(io.BytesIO(requests.get(y_url).content))

print(X[0])
print(y.head())


# Label encode the y data
y_encoder = LabelEncoder().fit(y)
y = y_encoder.transform(y)

# Convert values to numpy arrays
X = np.array(X)

# Split the training dataset into training and validation sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define a CNN model
model = keras.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(60, 64, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(2, activation='sigmoid')
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
batch_size = 32
epochs = 10
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=epochs
)



import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import numpy as np
import requests
import pickle
import io

# Import the preprocessed data
X_preprocessed_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_2/datasets/pickles/face_images_preprocessed.pkl"
y_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_2/datasets/pickles/sunglasses_y.pkl"

X = pickle.load(io.BytesIO(requests.get(X_preprocessed_url).content))
y = pickle.load(io.BytesIO(requests.get(y_url).content))

print(X[0])
print(y.head())


# Label encode the y data
y_encoder = LabelEncoder().fit(y)
y = y_encoder.transform(y)

# Convert values to numpy arrays
X = np.array(X)

# Split the training dataset into training and validation sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define a CNN model



# Compile the model


# Train the model







import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import numpy as np
import requests
import pickle
import io

# Import the preprocessed data
X_preprocessed_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_2/datasets/pickles/preprocessed_fungi.pkl"
y_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_2/datasets/pickles/fungi_y.pkl"

X = pickle.load(io.BytesIO(requests.get(X_preprocessed_url).content))
y = pickle.load(io.BytesIO(requests.get(y_url).content))

print(X[0])
print(y.head())


# Label encode the y data
y_encoder = LabelEncoder().fit(y)
y = y_encoder.transform(y)

# Convert values to numpy arrays
X = np.array(X)

# Split the training dataset into training and validation sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define a CNN model; make sure to use the correct values for input shape!
# HINT: Check the preprocessing activity to see the dimensions we used.
model = keras.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(250, 250, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(2, activation='sigmoid')
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
batch_size = 32
epochs = 10
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=epochs
)



import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import numpy as np
import requests
import pickle
import io

# Import the preprocessed data
X_preprocessed_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_2/datasets/pickles/preprocessed_fungi.pkl"
y_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_2/datasets/pickles/fungi_y.pkl"

X = pickle.load(io.BytesIO(requests.get(X_preprocessed_url).content))
y = pickle.load(io.BytesIO(requests.get(y_url).content))

print(X[0])
print(y.head())


# Label encode the y data
y_encoder = LabelEncoder().fit(y)
y = y_encoder.transform(y)

# Convert values to numpy arrays
X = np.array(X)

# Split the training dataset into training and validation sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define a CNN model; make sure to use the correct values for input shape!
# HINT: Check the preprocessing activity to see the dimensions we used.


# Compile the model


# Train the model





from tensorflow.keras.preprocessing.image import ImageDataGenerator
from matplotlib import pyplot as plt
from PIL import Image
import requests
import numpy as np
import pickle

# Import an image
img_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_2/datasets/an2i_left_angry_open.png"
example_image = Image.open(requests.get(img_url, stream=True).raw)
example_image

# Do some quick preprocessing
## Resize
example_image = example_image.resize((60, 64), Image.LANCZOS)
example_image

# Convert to floating point and normalize
float_image = np.array(example_image).astype(np.float32) / 255
float_image


# Look at the shape
float_image.shape

# The image generator requires a "batch" dimension
# We can add that using the "expand dims" function

# Add batch dimension
reshaped_image_array = np.expand_dims(float_image, axis=0)

# For grayscale images, the "channels" dimension must be added
# Add a channel dimension for grayscale images
reshaped_image_array = np.expand_dims(reshaped_image_array, axis=-1)

# View the shape
reshaped_image_array.shape

# Create an ImageDataGenerator with augmentation settings
datagen = ImageDataGenerator(
    rotation_range=20,      # Random rotation (degrees)
    fill_mode='nearest'     # Fill mode for handling newly created pixels
)

# Create an image
augmented_image = datagen.flow(reshaped_image_array, batch_size=1).next()[0]

# View the shape of the new image
augmented_image.shape

# Using a plotting library like matplot lib, we can view this image
# Plot the new image
# Note that we select all data from the first two dimensions, and specifically
# select the first value from the third dimension (color scale). We
# multiply by 255 to undo the normalization for plotting.

plt.imshow((augmented_image[:, :, 0]*255).astype('uint8'), cmap='gray')
plt.show()


# Plot the original for comparison
plt.imshow((reshaped_image_array[0, :, :, 0]* 255).astype('uint8'), cmap='gray')

# We can randomly apply multiple transformations at once
# to add variety to the data
# Create an ImageDataGenerator with augmentation settings
datagen = ImageDataGenerator(
    rotation_range=20,      # Random rotation (degrees)
    width_shift_range=0.1,  # Random horizontal shift
    height_shift_range=0.1, # Random vertical shift
    shear_range=0.2,        # Shear intensity
    zoom_range=0.2,         # Random zoom
    horizontal_flip=True,   # Random horizontal flip
    vertical_flip=False,    # No vertical flip for face images
    fill_mode='nearest'     # Fill mode for handling newly created pixels
)


# Generate augmented images
augmented_images = []
for _ in range(5):  # Augment the image 5 times for variety
    augmented_image = datagen.flow(reshaped_image_array, batch_size=1).next()[0]
    augmented_images.append(augmented_image)

# Visualize the original and augmented images
plt.figure(figsize=(12, 6))
for i in range(6):
    plt.subplot(2, 3, i + 1)
    if i == 0:
        plt.imshow((reshaped_image_array[0, :, :, 0]*255).astype('uint8'), cmap='gray')  # Original image
    else:
        plt.imshow((augmented_images[i - 1][:, :, 0]*255).astype('uint8'), cmap='gray')
    plt.axis('off')

plt.show()

import pickle
import io
import requests
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import pandas as pd

# Import the preprocessed data
X_preprocessed_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_2/datasets/pickles/face_images_preprocessed.pkl"
y_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_2/datasets/pickles/sunglasses_y.pkl"

X = pickle.load(io.BytesIO(requests.get(X_preprocessed_url).content))
y = pickle.load(io.BytesIO(requests.get(y_url).content))

# Label encode the y data
y_encoder = LabelEncoder().fit(y)
y = y_encoder.transform(y)

# Convert values to numpy arrays
X = np.array(X)

# Split the training dataset into training and validation sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Create the image generator
datagen = ImageDataGenerator(
    rotation_range=20,      # Random rotation (degrees)
    width_shift_range=0.1,  # Random horizontal shift
    height_shift_range=0.1, # Random vertical shift
    shear_range=0.2,        # Shear intensity
    zoom_range=0.2,         # Random zoom
    horizontal_flip=True,   # Random horizontal flip
    vertical_flip=False,    # No vertical flip for face images
    fill_mode='nearest'     # Fill mode for handling newly created pixels
)

# Create an empty list for both X and y augmentations

X_train_aug = []
y_train_aug = []

# Loop through each image in the training data
for i in range(len(X_train)):
    # Select the image and its y label
    img = X_train[i]
    label = y_train[i]

    # Add a channel dimension for grayscale images
    img = np.expand_dims(img, axis=-1)

    # Add the batch dimension
    img = np.expand_dims(img, axis=0)

    # Use a loop to create 5 new images
    # Append each to X_train_aug
    # For each image, use concat to add the correct label to y_train_aug
    for j in range(5):
        X_train_aug.append(datagen.flow(img, batch_size=1).next()[0])
        y_train_aug.append(label)

# Print the lengths of both augmented sets to ensure they are the same length
print(len(X_train_aug))
print(len(y_train_aug))

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from matplotlib import pyplot as plt
from PIL import Image
import requests
import numpy as np
import pickle

# Import an image
img_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_2/datasets/an2i_left_angry_open.png"
example_image = Image.open(requests.get(img_url, stream=True).raw)
example_image

# Do some quick preprocessing
## Resize


# Convert to floating point and normalize




# Look at the shape


# The image generator requires a "batch" dimension
# We can add that using the "expand dims" function

# Add batch dimension


# For grayscale images, the "channels" dimension must be added
# Add a channel dimension for grayscale images


# View the shape


# Create an ImageDataGenerator with augmentation settings


# Create an image


# View the shape of the new image


# Using a plotting library like matplot lib, we can view this image
# Plot the new image
# Note that we select all data from the first two dimensions, and specifically
# select the first value from the third dimension (color scale). We
# multiply by 255 to undo the normalization for plotting.




# Plot the original for comparison


# We can randomly apply multiple transformations at once
# to add variety to the data

# Create an ImageDataGenerator with augmentation settings


# Generate augmented images


# Visualize the original and augmented images
plt.figure(figsize=(12, 6))
for i in range(6):
    plt.subplot(2, 3, i + 1)
    if i == 0:
        plt.imshow((reshaped_image_array[0, :, :, 0]*255).astype('uint8'), cmap='gray')  # Original image
    else:
        plt.imshow((augmented_images[i - 1][:, :, 0]*255).astype('uint8'), cmap='gray')
    plt.axis('off')

plt.show()


import pickle
import io
import requests
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import pandas as pd

# Import the preprocessed data
X_preprocessed_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_2/datasets/pickles/face_images_preprocessed.pkl"
y_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_2/datasets/pickles/sunglasses_y.pkl"

X = pickle.load(io.BytesIO(requests.get(X_preprocessed_url).content))
y = pickle.load(io.BytesIO(requests.get(y_url).content))

# Label encode the y data
y_encoder = LabelEncoder().fit(y)
y = y_encoder.transform(y)

# Convert values to numpy arrays
X = np.array(X)

# Split the training dataset into training and validation sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Create the image generator


# Create an empty list for both X and y augmentations


# Loop through each image in the training data

    # Select the image and its y label


    # Add a channel dimension for grayscale images


    # Add the batch dimension


    # Use a loop to create 5 new images
    # Append each to X_train_aug
    # For each image, use concat to add the correct label to y_train_aug

# Print the lengths of both augmented sets to ensure they are the same length


from tensorflow.keras.preprocessing.image import ImageDataGenerator
from matplotlib import pyplot as plt
from PIL import Image
import requests
import numpy as np
import pickle

# Import an image
img_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_2/datasets/H2_122c_4.jpg"
example_image = Image.open(requests.get(img_url, stream=True).raw)
example_image

# Do some quick preprocessing
## Resize to 250 by 250
example_image = example_image.resize((250, 250), Image.LANCZOS)
example_image

# Convert to floating point and normalize
float_image = np.array(example_image).astype(np.float32) / 255
float_image


# Look at the shape
float_image.shape

# The image generator requires a "batch" dimension
# We can add that using the "expand dims" function

# Add batch dimension
reshaped_image_array = np.expand_dims(float_image, axis=0)

# View the shape
reshaped_image_array.shape

# Create an ImageDataGenerator with augmentation settings
datagen = ImageDataGenerator(
    rotation_range=20,      # Random rotation (degrees)
    fill_mode='nearest'     # Fill mode for handling newly created pixels
)

# Create a new image based on the original image
augmented_image = datagen.flow(reshaped_image_array, batch_size=1).next()[0]

# View the shape of the new image
augmented_image.shape

# Using a plotting library like matplot lib, we can view this image
# Plot the new image
# Note that we select all data from the first two dimensions, and specifically
# select the first value from the third dimension (color scale). We
# multiply by 255 to undo the normalization for plotting.

plt.imshow((augmented_image*255).astype('uint8'))
plt.show()


# Plot the original for comparison
plt.imshow((reshaped_image_array[0, :, :, :] * 255).astype('uint8'))

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from matplotlib import pyplot as plt
from PIL import Image
import requests
import numpy as np
import pickle

# Import an image
img_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_2/datasets/H2_122c_4.jpg"
example_image = Image.open(requests.get(img_url, stream=True).raw)
example_image

# Do some quick preprocessing
## Resize to 250 by 250


# Convert to floating point and normalize



# Look at the shape


# The image generator requires a "batch" dimension
# We can add that using the "expand dims" function

# Add batch dimension


# View the shape



# Create an ImageDataGenerator with augmentation settings

# Create a new image based on the original image

# View the shape of the new image



# Using a plotting library like matplot lib, we can view this image
# Plot the new image
# Note that we select all data from the first two dimensions, and specifically
# select the first value from the third dimension (color scale). We
# multiply by 255 to undo the normalization for plotting.



# Plot the original for comparison



from tensorflow.keras.preprocessing.image import ImageDataGenerator
from matplotlib import pyplot as plt
from PIL import Image
import requests
import numpy as np
import pandas as pd
import io
import pickle

# Import the preprocessed data
X_preprocessed_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_2/datasets/pickles/preprocessed_fungi.pkl"
y_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_2/datasets/pickles/fungi_y.pkl"

X = pickle.load(io.BytesIO(requests.get(X_preprocessed_url).content))
y = pickle.load(io.BytesIO(requests.get(y_url).content))

print(X[0])
print(y.head())

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# Label encode the y data
y_encoder = LabelEncoder().fit(y)
y = y_encoder.transform(y)

# Convert values to numpy arrays
X = np.array(X)

# Split the training dataset into training and validation sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Create an ImageDataGenerator
datagen = ImageDataGenerator(
    rotation_range=20,      # Random rotation (degrees)
    width_shift_range=0.1,  # Random horizontal shift
    height_shift_range=0.1, # Random vertical shift
    shear_range=0.2,        # Shear intensity
    zoom_range=0.2,         # Random zoom
    horizontal_flip=True,   # Random horizontal flip
    vertical_flip=False,    # No vertical flip for face images
    fill_mode='nearest'     # Fill mode for handling newly created pixels
)


# Create an emtpty list for X and y augmentations
X_train_aug = []
y_train_aug = []

# Loop through the entire X_train set
for i in range(len(X_train)):
    # Select the original image and its y label
    img = X_train[i]
    label = y_train[i]

    # Ensure that the input data has the correct shape
    img = np.expand_dims(img, axis=0)  # Add batch dimension

    # Add 5 new images for every original
    for j in range(5):
        # Create and append the image
        X_train_aug.append(datagen.flow(img, batch_size=1).next()[0])
        # Append the original label
        y_train_aug.append(label)

# Print the length of the augmented images and the labels
print(len(X_train_aug))
print(len(y_train_aug))

# Export our final variables to a pickle file using a dictionary
from google.colab import drive
drive.mount('/content/drive')

# Create the dictionary
fungi_dict = {
    'X_train': X_train_aug,
    'X_test': X_test,
    'y_train': y_train_aug,
    'y_test': y_test
}

# Store the dictionary in a pickle file
with open('/content/drive/My Drive/fungi_dict.pkl', 'wb') as file:
    pickle.dump(fungi_dict, file)



from tensorflow.keras.preprocessing.image import ImageDataGenerator
from matplotlib import pyplot as plt
from PIL import Image
import requests
import numpy as np
import pandas as pd
import io
import pickle

# Import the preprocessed data
X_preprocessed_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_2/datasets/pickles/preprocessed_fungi.pkl"
y_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_2/datasets/pickles/fungi_y.pkl"

X = pickle.load(io.BytesIO(requests.get(X_preprocessed_url).content))
y = pickle.load(io.BytesIO(requests.get(y_url).content))

print(X[0])
print(y.head())

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# Label encode the y data
y_encoder = LabelEncoder().fit(y)
y = y_encoder.transform(y)

# Convert values to numpy arrays
X = np.array(X)

# Split the training dataset into training and validation sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Create an ImageDataGenerator


# Create an emtpty list for X and y augmentations


# Loop through the entire X_train set
    # Select the original image and its y label
 

    # Ensure that the input data has the correct shape

    # Add 5 new images for every original

        # Create and append the image
        
        # Append the original label


# Print the length of the augmented images and the labels


# Export our final variables to a pickle file using a dictionary
from google.colab import drive
drive.mount('/content/drive')

# Create the dictionary
fungi_dict = {
    'X_train': X_train_aug,
    'X_test': X_test,
    'y_train': y_train_aug,
    'y_test': y_test
}

# Store the dictionary in a pickle file
with open('/content/drive/My Drive/fungi_dict.pkl', 'wb') as file:
    pickle.dump(fungi_dict, file)



from PIL import Image
import pandas as pd
import requests
import numpy as np

# Reading the meta file containing all image file names
path = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_1/datasets/csvs/files_list.csv"

filenames_df = pd.read_csv(path)
filenames_df.head()

# Build a list of imported images
base_path = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_1/datasets/faces_data/"
images = []
for i in range(len(filenames_df)):
    filename = filenames_df.iloc[i,0]
    path = base_path + filename
    print(f'{i} of {len(filenames_df)}: Attempting to import {filename}')
    try:
        response = requests.get(path, stream=True).raw
        images.append(Image.open(response))
    except:
        print(f'FAILED: {filename}')

# Print a random image from the list to ensure the import was successful
images[40]

# Check the size of the second image
images[1].size

# Get all the sizes into a list, then convert to a set
sizes = set([img.size for img in images])
sizes

# Use a for loop to resize all images to 64 by 60
target_size = (64, 60)

resized_images = [img.resize(target_size, resample = Image.LANCZOS) for img in images]
resized_images[1]

# Verify the resizing of all images
# Get all the sizes into a list, then convert to a set
sizes = set([img.size for img in resized_images])
sizes

# Convert all images to floating point numpy arrays
float_images = [np.array(img).astype(np.float32) for img in resized_images]

# Display the pixel values of the first image
print("Pixel Values:")
print(float_images[0])

# To normalize images to a range between 0 and 1,
# we need to divide all pixel values by the max of 255

normalized_images = [img/255 for img in float_images]

# Display the pixel values of the first image
print("Pixel Values:")
print(normalized_images[0])

# Print the first few image filenames
filenames_df.head()

# First, remove the .png file extension, then split into four new columns.
filenames_df[['userid', 'pose', 'expression', 'eyes']] = filenames_df['files']\
                                                            .str.replace('.png', '', regex=False)\
                                                            .str.split('_', expand=True)
filenames_df.head()

# Now we can call our preprocessed pixel data 'X'
X = normalized_images

# For our purposes, we'll select the userid column as 'y'
y = np.array(filenames_df['userid'])

# Check the total number of classes
y.nunique()

# Now we'll split our data into training and testing sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y)

# Apply augmentation to the whole training dataset
# Create an ImageDataGenerator
datagen = ImageDataGenerator(
    rotation_range=20,      # Random rotation (degrees)
    width_shift_range=0.1,  # Random horizontal shift
    height_shift_range=0.1, # Random vertical shift
    shear_range=0.2,        # Shear intensity
    zoom_range=0.2,         # Random zoom
    horizontal_flip=True,   # Random horizontal flip
    vertical_flip=False,    # No vertical flip for face images
    fill_mode='nearest'     # Fill mode for handling newly created pixels
)

# Create variables to hold the X and y training data
X_train_aug = []
y_train_aug = []

# Loop through all the images.
for i in range(len(X_train)):
    # Select the image
    img = X_train[i]
    # Select the label from the training data
    label = y_train[i]
    
    # Add a channel dimension for grayscale images
    img = np.expand_dims(img, axis=-1)  # Add channel dimension

    # Ensure that the input data has the correct shape
    img = np.expand_dims(img, axis=0)  # Add batch dimension

    # Add 5 images for every original image
    for j in range(5):
        # Append a new image to the X list
        X_train_aug.append(datagen.flow(img, batch_size=1).next()[0])
        # Append the label for the original image to the y list
        y_train_aug.append(label)

# Print the length of each list
print(len(X_train_aug))
print(len(y_train_aug))



# Reshape test data for the model
X_test_np = []
for img in X_test:
    # Add a channel dimension for grayscale images
    img = np.expand_dims(img, axis=-1)  # Add channel dimension
    # Append the image to the list
    X_test_np.append(img)

# Convert to numpy array
X_test_np = np.array(X_test_np)

# Check the shape of the first image
X_test_np[0].shape

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
import numpy as np

# One hot encode the y data
y_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False).fit(np.array(y_train_aug).reshape(-1, 1))
y_train_aug_enc = y_encoder.transform(np.array(y_train_aug).reshape(-1, 1))
y_test_enc = y_encoder.transform(np.array(y_test).reshape(-1, 1))

# Convert values to numpy arrays
X_train_aug_np = np.array(X_train_aug)
X_test_np = np.array(X_test_np)
y_train_aug_np = np.array(y_train_aug_enc)
y_test_np = np.array(y_test_enc)

# Load and preprocess your CMU Face Images dataset (Ensure each image is labeled as "with sunglasses" or "without sunglasses")
# The following code assumes that you have already loaded and preprocessed your dataset into 'X' and 'y' (features and labels).

# Split the training dataset into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_train_aug_np, y_train_aug_np, test_size=0.2, random_state=42)

# Print the total number of one_hot_encoded columns
np.array(y_train).shape

# Define a CNN model
model = keras.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(60, 64, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(20, activation='sigmoid')  # 20 classes
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
batch_size = 32
epochs = 10
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=epochs
)

model.evaluate(X_test_np, y_test_np)



from PIL import Image
import pandas as pd
import requests
import numpy as np

# Reading the meta file containing all image file names
path = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_1/datasets/csvs/files_list.csv"

filenames_df = pd.read_csv(path)
filenames_df.head()

# Build a list of imported images
base_path = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_1/datasets/faces_data/"
images = []
for i in range(len(filenames_df)):
    filename = filenames_df.iloc[i,0]
    path = base_path + filename
    print(f'{i} of {len(filenames_df)}: Attempting to import {filename}')
    try:
        response = requests.get(path, stream=True).raw
        images.append(Image.open(response))
    except:
        print(f'FAILED: {filename}')

# Print a random image from the list to ensure the import was successful
images[40]

# Check the size of the second image


# Get all the sizes into a list, then convert to a set


# Use a for loop to resize all images to 64 by 60


# Verify the resizing of all images
# Get all the sizes into a list, then convert to a set


# Convert all images to floating point numpy arrays

# Display the pixel values of the first image


# To normalize images to a range between 0 and 1,
# we need to divide all pixel values by the max of 255


# Display the pixel values of the first image



# Print the first few image filenames



# First, remove the .png file extension, then split into four new columns.



# Now we can call our preprocessed pixel data 'X'

# For our purposes, we'll select the userid column as 'y'



# Now we'll split our data into training and testing sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y)

# Apply augmentation to the whole training dataset
# Create an ImageDataGenerator

# Create variables to hold the X and y training data


# Loop through all the images.

    # Select the image

    # Select the label from the training data
    
    # Add a channel dimension for grayscale images

    # Ensure that the input data has the correct shape

    # Add 5 images for every original image

        # Append a new image to the X list

        # Append the label for the original image to the y list


# Print the length of each list





# Reshape test data for the model

    # Add a channel dimension for grayscale images

    # Append the image to the list


# Convert to numpy array

# Check the shape of the first image


import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
import numpy as np

# One hot encode the y data


# Convert values to numpy arrays


# Load and preprocess your CMU Face Images dataset (Ensure each image is labeled as "with sunglasses" or "without sunglasses")
# The following code assumes that you have already loaded and preprocessed your dataset into 'X' and 'y' (features and labels).

# Split the training dataset into training and validation sets


# Print the total number of one_hot_encoded columns



# Define a CNN model


# Compile the model


# Train the model



# Evaluate the model using the testing data




from tensorflow.keras import layers, models, Model
import numpy as np
import pandas as pd

# Import the data
path = 'https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_3/datasets/wine_quality.csv'
df = pd.read_csv(path)
df.head()

# Preprocess y
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# Preprocess "quality" column (one-hot encoding)
quality_encoder = OneHotEncoder(sparse_output=False)
quality_encoded = quality_encoder.fit_transform(df[['quality']])
quality_columns = quality_encoder.get_feature_names_out(['quality'])
df_quality_encoded = pd.DataFrame(quality_encoded, columns=quality_columns)

# Preprocess "color" column (label encoding for binary; one-hot encoding for multiple categories)
color_encoder = LabelEncoder()
df['color_encoded'] = color_encoder.fit_transform(df['color'])

# Concatenate the encoded columns to the original DataFrame
df_processed = pd.concat([df, df_quality_encoded], axis=1)

# Drop the original "quality" and "color" columns
df_processed = df_processed.drop(['quality', 'color'], axis=1)

df_processed.head()


# Split data into X and two separate y variables
X = df_processed.drop(columns=['quality_good', 'quality_ok', 'quality_bad', 'color_encoded'])

y_color = df_processed['color_encoded']

y_quality = df_processed[['quality_good', 'quality_ok', 'quality_bad']]

# Split data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_color_train, y_color_test, y_quality_train, y_quality_test = train_test_split(X, y_color, y_quality)

# Create the shared layers of the model

# Input layer
input_layer = layers.Input(shape=(X.shape[1],), name='input_features')

# Shared hidden layers
shared_layer1 = layers.Dense(64, activation='relu')(input_layer)
shared_layer2 = layers.Dense(32, activation='relu')(shared_layer1)

# Branch for quality prediction
quality_output = layers.Dense(3, activation='softmax', name='quality_output')(shared_layer2)

# Branch for color prediction
color_output = layers.Dense(1, activation='sigmoid', name='color_output')(shared_layer2)

# Create the model
model = Model(inputs=input_layer, outputs=[quality_output, color_output])

# Compile the model
model.compile(optimizer='adam',
              loss={'quality_output': 'categorical_crossentropy', 'color_output': 'binary_crossentropy'},
              metrics={'quality_output': 'accuracy', 'color_output': 'accuracy'})

# Display the model summary
model.summary()

# Fit the model
model.fit(
    X,
    {'quality_output': y_quality, 'color_output': y_color},
    epochs=10,
    batch_size=32,
    validation_split=0.2
)

# Evaluate the model with the testing data
test_results = model.evaluate(X_test, {'quality_output': y_quality_test, 'color_output': y_color_test})
test_results

# Print the quality and color accuracy
print(f"Quality Accuracy: {test_results[3]}")
print(f"Color Accuracy: {test_results[4]}")




from tensorflow.keras import layers, models, Model
import numpy as np
import pandas as pd

# Import the data
path = 'https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_3/datasets/wine_quality.csv'
df = pd.read_csv(path)
df.head()

# Preprocess y
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# Preprocess "quality" column (one-hot encoding)


# Preprocess "color" column (label encoding for binary; one-hot encoding for multiple categories)


# Concatenate the encoded columns to the original DataFrame


# Drop the original "quality" and "color" columns



# Split data into X and two separate y variables



# Split data into training and testing sets
from sklearn.model_selection import train_test_split



# Create the shared layers of the model

# Input layer

# Shared hidden layers


# Branch for quality prediction

# Branch for color prediction


# Create the model

# Compile the model


# Display the model summary


# Fit the model


# Evaluate the model with the testing data


# Print quality and color accuracy




# Build a list of images using a for loop
import pandas as pd
import numpy as np
import requests
from PIL import Image
import io
from tensorflow.keras import layers, models, Model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, LabelEncoder

# Define the base_url
path = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_3/datasets/csvs/files_list.csv"

# Import the file and display the first few rows
filenames_df = pd.read_csv(path)
filenames_df.head()



# Define the base_url
base_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_3/datasets/faces_data/"

# Create an empty list for the urls
img_urls = []

# Loop through the DataFrame and build and append the full image urls
for i in range(len(filenames_df)):
    filename = filenames_df.iloc[i,0]
    img_urls.append(base_url + filename)

img_urls[0:5]

# Create and empty list for images
imgs = []

# Loop through ALL image_urls to open and append each image
num_imgs = len(img_urls)

for i in range(num_imgs):
    img_url = img_urls[i]
    # Print a statement to show progress
    print(f"{i}/{num_imgs}: Attempting to import {img_url}")

    # Use requests.get along with the stream parameter and raw attribute
    response = requests.get(img_url, stream=True).raw

    # Append each img to the imgs list
    imgs.append(Image.open(response))

# View the first image to confirm
imgs[0]

# Get all the sizes into a list, then convert to a set
sizes = set([img.size for img in imgs])
sizes

# Convert the images to the middle 64, 60 size

target_size = (64, 60)
resized_imgs = [img.resize(target_size, resample = Image.LANCZOS) for img in imgs]
resized_imgs[1]

# Convert all images to floating point numpy arrays
float_images = [np.array(img).astype(np.float32) for img in resized_imgs]

# Display the pixel values of the first image
print("Pixel Values:")
print(float_images[0])

# To normalize pixel values to a range between 0 and 1,
# we need to divide all pixel values by the max of 255

normalized_images = [img/255 for img in float_images]

# Display the pixel values of the first image
print("Pixel Values:")
print(normalized_images[0])

# Look at the filenames DataFrame
filenames_df.head()

# First, remove the .png file extension, then split into four new columns.
filenames_df[['userid', 'pose', 'expression', 'eyes']] = filenames_df['files']\
                                                            .str.replace('.png', '', regex=False)\
                                                            .str.split('_', expand=True)

# Make a new df without the "files" column
y_df = filenames_df[['userid', 'pose', 'expression', 'eyes']].copy()
y_df.head()

# Start with the userid column
# Look at the value counts to decide which encoder to use
y_df['userid'].value_counts()


# OneHotEncode the userid column
# Create an encoder
userid_encoder = OneHotEncoder(sparse_output=False)

# Fit_transform the userid column
userid_encoded = userid_encoder.fit_transform(y_df[['userid']])

# Get the feature names from the encoder
userid_columns = userid_encoder.get_feature_names_out(['userid'])

# Create a new DataFrame with the data
userid_encoded_df = pd.DataFrame(userid_encoded, columns=userid_columns)

# View the first few rows of the data
userid_encoded_df.head()


# Repeat the process for the pose column
# Look at the value counts to decide which encoder to use
y_df['pose'].value_counts()

# Create an encoder
pose_encoder = OneHotEncoder(sparse_output=False)

# Fit_transform the pose column
pose_encoded = pose_encoder.fit_transform(y_df[['pose']])

# Get the feature names from the encoder
pose_columns = pose_encoder.get_feature_names_out(['pose'])

# Create a new DataFrame with the data
pose_encoded_df = pd.DataFrame(pose_encoded, columns=pose_columns)

# View the first few rows of the data
pose_encoded_df.head()

# Repeat the process for the expression column
# Start with the userid column
# Look at the value counts to decide which encoder to use
y_df['expression'].value_counts()

# Create an encoder
expression_encoder = OneHotEncoder(sparse_output=False)

# Fit_transform the expression column
expression_encoded = expression_encoder.fit_transform(y_df[['expression']])

# Get the feature names from the encoder
expression_columns = expression_encoder.get_feature_names_out(['expression'])

# Create a new DataFrame with the data
expression_encoded_df = pd.DataFrame(expression_encoded, columns=expression_columns)

# View the first few rows of the data
expression_encoded_df.head()

# Repeat the process for the eyes column
# Start with the userid column
# Look at the value counts to decide which encoder to use
y_df['eyes'].value_counts()

# Create an encoder
eyes_encoder = OneHotEncoder(sparse_output=False, drop='first')

# Fit_transform the eyes column
eyes_encoded = eyes_encoder.fit_transform(y_df[['eyes']])

# Get the feature names from the encoder
eyes_columns = eyes_encoder.get_feature_names_out(['eyes'])

# Create a new DataFrame with the data
eyes_encoded_df = pd.DataFrame(eyes_encoded, columns=eyes_columns)

# View the first few rows of the data
eyes_encoded_df.head()

# Combine the processed y data into a new DataFrame
y_processed_df = pd.concat([
    userid_encoded_df,
    pose_encoded_df,
    expression_encoded_df,
    eyes_encoded_df
    ],
    axis=1
    )

# Display the first few rows
y_processed_df.head()

# Convert X to a numpy array
X = np.array(normalized_images)

# Loop through each image to include the channel dimension
X_processed = []
for img in X:
    # Add channel dimension
    img = np.expand_dims(img, axis=-1)

    # Append the image to the array
    X_processed.append(img)

# Split X and y into train and test sets; do not augment the test set!
X_train, X_test, y_train, y_test = train_test_split(X_processed, y_processed_df)



import pickle
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Augment the images
# Create the ImageDataGenerator
datagen = ImageDataGenerator(
    rotation_range=20,      # Random rotation (degrees)
    width_shift_range=0.1,  # Random horizontal shift
    height_shift_range=0.1, # Random vertical shift
    shear_range=0.2,        # Shear intensity
    zoom_range=0.2,         # Random zoom
    horizontal_flip=True,   # Random horizontal flip
    vertical_flip=False,    # No vertical flip for face images
    fill_mode='nearest'     # Fill mode for handling newly created pixels
)

# Create an empty list to hold the augmented images
X_train_aug = []

# Create an empty DataFrame to hold the new y training data
# Use the column names from the processed y DataFrame
y_train_aug = pd.DataFrame(columns=y_processed_df.columns)

# Loop through the images
for i in range(len(X_train)):
    # Select the image
    img = X_train[i]

    # Select the label row from the y data
    label = y_train.iloc[[i]]

    # Add the batch dimension
    img = np.expand_dims(img, axis=0)

    # Add 5 new images for each original
    for j in range(5):
        # Append the image and the label
        X_train_aug.append(datagen.flow(img, batch_size=1).next()[0])
        y_train_aug = pd.concat([y_train_aug, label]).reset_index(drop=True)

print(len(X_train_aug))
print(len(y_train_aug))


# Convert X_train_aug and X_test to numpy arrays
X_train_aug = np.array(X_train_aug)
X_test = np.array(X_test)

print(X_train_aug.shape)
print(X_test.shape)

# Look at the first few rows of y_train_aug
y_train_aug.head()

# Using a creative list comprehension, we can easily gather
# columns related to the userid in the training data
y_train_userid = y_train_aug[[col for col in y_train_aug.columns if 'userid' in col]]

# Repeat this for the following in the training data
# pose
y_train_pose = y_train_aug[[col for col in y_train_aug.columns if 'pose' in col]]

# expression
y_train_expression = y_train_aug[[col for col in y_train_aug.columns if 'expression' in col]]

# eyes
y_train_eyes = y_train_aug[[col for col in y_train_aug.columns if 'eyes' in col]]

# Now repeat all 4 selections with the y_test data
y_test_userid = y_test[[col for col in y_test.columns if 'userid' in col]]
y_test_pose = y_test[[col for col in y_test.columns if 'pose' in col]]
y_test_expression = y_test[[col for col in y_test.columns if 'expression' in col]]
y_test_eyes = y_test[[col for col in y_test.columns if 'eyes' in col]]

# Display the first few rows of one of the DataFrames to confirm your work
y_train_pose.head()

# Create a dictionary containing all the data
preprocessed_data = {
    'X_train': X_train_aug,
    'X_test': X_test,
    'y_train_userid': y_train_userid,
    'y_train_pose': y_train_pose,
    'y_train_expression': y_train_expression,
    'y_train_eyes': y_train_eyes,
    'y_test_userid': y_test_userid,
    'y_test_pose': y_test_pose,
    'y_test_expression': y_test_expression,
    'y_test_eyes': y_test_eyes
}

# Store the dictionary as a pickle file
from google.colab import drive
import pickle

drive.mount('/content/drive')

with open('/content/drive/My Drive/preprocessed_faces_data.pkl', 'wb') as file:
    pickle.dump(preprocessed_data, file)



# Build a list of images using a for loop
import pandas as pd
import numpy as np
import requests
from PIL import Image
import io
from tensorflow.keras import layers, models, Model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, LabelEncoder

# Define the base_url
path = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_3/datasets/csvs/files_list.csv"

# Import the file and display the first few rows
filenames_df = pd.read_csv(path)
filenames_df.head()



# Define the base_url
base_url = "https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_3/datasets/faces_data/"

# Create an empty list for the urls
img_urls = []

# Loop through the DataFrame and build and append the full image urls
for i in range(len(filenames_df)):
    filename = filenames_df.iloc[i,0]
    img_urls.append(base_url + filename)

img_urls[0:5]

# Create and empty list for images
imgs = []

# Loop through ALL image_urls to open and append each image
num_imgs = len(img_urls)

for i in range(num_imgs):
    img_url = img_urls[i]
    # Print a statement to show progress
    print(f"{i}/{num_imgs}: Attempting to import {img_url}")

    # Use requests.get along with the stream parameter and raw attribute
    response = requests.get(img_url, stream=True).raw

    # Append each img to the imgs list
    imgs.append(Image.open(response))

# View the first image to confirm
imgs[0]

# Get all the sizes into a list, then convert to a set
sizes = set([img.size for img in imgs])
sizes

# Convert the images to the middle 64, 60 size

target_size = (64, 60)
resized_imgs = [img.resize(target_size, resample = Image.LANCZOS) for img in imgs]
resized_imgs[1]

# Convert all images to floating point numpy arrays
float_images = [np.array(img).astype(np.float32) for img in resized_imgs]

# Display the pixel values of the first image
print("Pixel Values:")
print(float_images[0])

# To normalize pixel values to a range between 0 and 1,
# we need to divide all pixel values by the max of 255

normalized_images = [img/255 for img in float_images]

# Display the pixel values of the first image
print("Pixel Values:")
print(normalized_images[0])

# Look at the filenames DataFrame
filenames_df.head()

# First, remove the .png file extension, then split into four new columns.


# Make a new df without the "files" column



# Start with the userid column
# Look at the value counts to decide which encoder to use



# OneHotEncode the userid column
# Create an encoder


# Fit_transform the userid column


# Get the feature names from the encoder


# Create a new DataFrame with the data


# View the first few rows of the data



# Repeat the process for the pose column
# Look at the value counts to decide which encoder to use



# Create an encoder


# Fit_transform the pose column


# Get the feature names from the encoder


# Create a new DataFrame with the data


# View the first few rows of the data



# Repeat the process for the expression column
# Start with the userid column
# Look at the value counts to decide which encoder to use



# Create an encoder


# Fit_transform the expression column


# Get the feature names from the encoder


# Create a new DataFrame with the data


# View the first few rows of the data


# Repeat the process for the eyes column
# Start with the userid column
# Look at the value counts to decide which encoder to use


# Create an encoder

# Fit_transform the eyes column

# Get the feature names from the encoder

# Create a new DataFrame with the data

# View the first few rows of the data


# Combine the processed y data into a new DataFrame


# Display the first few rows


# Convert X to a numpy array

# Loop through each image to include the channel dimension

    # Add channel dimension


    # Append the image to the array


# Split X and y into train and test sets; do not augment the test set!




import pickle
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Augment the images
# Create the ImageDataGenerator


# Create an empty list to hold the augmented images


# Create an empty DataFrame to hold the new y training data
# Use the column names from the processed y DataFrame

# Loop through the images

    # Select the image


    # Select the label row from the y data


    # Add the batch dimension


    # Add 5 new images for each original

        # Append the image and the label





# Convert X_train_aug and X_test to numpy arrays





# Look at the first few rows of y_train_aug



# Using a creative list comprehension, we can easily gather
# columns related to the userid in the training data


# Repeat this for the following in the training data
# pose


# expression


# eyes



# Now repeat all 4 selections with the y_test data



# Display the first few rows of one of the DataFrames to confirm your work



# Create a dictionary containing all the data



# Store the dictionary as a pickle file
from google.colab import drive
import pickle

drive.mount('/content/drive')

with open('/content/drive/My Drive/preprocessed_faces_data.pkl', 'wb') as file:
    pickle.dump(preprocessed_data, file)



import pickle
import requests
import io
import numpy as np

pickle_file = 'https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_3/datasets/pickles/preprocessed_faces_data.pkl'
data = pickle.load(io.BytesIO(requests.get(pickle_file).content))
data.keys()


X_train = data['X_train']
X_test = data['X_test']

y_train_userid = data['y_train_userid']
y_train_pose = data['y_train_pose']
y_train_expression = data['y_train_expression']
y_train_eyes = data['y_train_eyes']

y_test_userid = data['y_test_userid']
y_test_pose = data['y_test_pose']
y_test_expression = data['y_test_expression']
y_test_eyes = data['y_test_eyes']

from tensorflow.keras import layers, models, Model
# First we build the input layer
input_layer = layers.Input(shape=(60, 64, 1), name='input_layer')

# Shared layers (common across all tasks)
# The second layer should be a Conv2D layer built off the input_layer
conv1 = layers.Conv2D(32, (3, 3), activation='relu')(input_layer)

# The third layer should be a MaxPooling2D layer built off the second layer
maxpool1 = layers.MaxPooling2D((2, 2))(conv1)

# The fourth layer should be a Conv2D layer built off the third layer
conv2 = layers.Conv2D(64, (3, 3), activation='relu')(maxpool1)

# The fifth layer should be a MaxPooling2D layer built off the fourth layer
maxpool2 = layers.MaxPooling2D((2, 2))(conv2)

# The sixth layer should be a Conv2D layer built off the fifth layer
conv3 = layers.Conv2D(64, (3, 3), activation='relu')(maxpool2)

# The seventh layer should be a Flatten layer built off the sixth layer
flatten = layers.Flatten()(conv3)

# Lastly, build one dense layer before branching to the different y branches
dense_shared = layers.Dense(64, activation='relu')(flatten)


# Build the branches for each of the y variables
# Include a dense hidden layer in each along with the output layer.
# Remember to include the correct number of nodes for the output!

# userid
userid_dense = layers.Dense(64, activation='relu')(dense_shared)
userid_output = layers.Dense(len(y_train_userid.columns),
                             activation='sigmoid',
                             name='userid_output')(userid_dense)

# pose
pose_dense = layers.Dense(64, activation='relu')(dense_shared)
pose_output = layers.Dense(len(y_train_pose.columns),
                           activation='softmax',
                             name='pose_output')(pose_dense)

# expression
expression_dense = layers.Dense(64, activation='relu')(dense_shared)
expression_output = layers.Dense(len(y_train_expression.columns),
                                 activation='softmax',
                             name='expression_output')(expression_dense)

# eyes
eyes_dense = layers.Dense(64, activation='relu')(dense_shared)
eyes_output = layers.Dense(len(y_train_eyes.columns),
                           activation='sigmoid',
                             name='eyes_output')(eyes_dense)

# Build the model
model = Model(inputs=input_layer, outputs=[
    userid_output,
    pose_output,
    expression_output,
    eyes_output
])

# Compile the model
model.compile(optimizer='adam',
              loss={'userid_output': 'categorical_crossentropy',
                    'pose_output': 'categorical_crossentropy',
                    'expression_output': 'categorical_crossentropy',
                    'eyes_output': 'binary_crossentropy'},
              metrics={'userid_output': 'accuracy',
                       'pose_output': 'accuracy',
                       'expression_output': 'accuracy',
                       'eyes_output': 'accuracy'})

# Train the model with the training data
model.fit(
    X_train,
    {
        'userid_output': y_train_userid,
        'pose_output': y_train_pose,
        'expression_output': y_train_expression,
        'eyes_output': y_train_eyes
    },
    epochs=10,  # You can adjust the number of epochs based on your needs
    batch_size=32,  # You can adjust the batch size based on your available memory
    validation_split=0.2  # You can specify the validation split if you have a separate validation set
)

# Evaluate the model using the test data
results = model.evaluate(np.array(X_test), {
        'userid_output': y_test_userid,
        'pose_output': y_test_pose,
        'expression_output': y_test_expression,
        'eyes_output': y_test_eyes
    })

# Print the accuracy for each category
pred_categories = ['userid', 'pose', 'expression', 'eyes']
for i, cat in enumerate(pred_categories):
    print(f"{cat} accuracy: {results[i+5]}")



import pickle
import requests
import io

pickle_file = 'https://static.bc-edx.com/ai/ail-v-1-0/m19/lesson_3/datasets/pickles/preprocessed_faces_data.pkl'
data = pickle.load(io.BytesIO(requests.get(pickle_file).content))
data.keys()


# Collect the data into individual variables
X_train = data['X_train']
X_test = data['X_test']

y_train_userid = data['y_train_userid']
y_train_pose = data['y_train_pose']
y_train_expression = data['y_train_expression']
y_train_eyes = data['y_train_eyes']

y_test_userid = data['y_test_userid']
y_test_pose = data['y_test_pose']
y_test_expression = data['y_test_expression']
y_test_eyes = data['y_test_eyes']

from tensorflow.keras import layers, models, Model
# First we build the input layer


# Shared layers (common across all tasks)
# The second layer should be a Conv2D layer built off the input_layer


# The third layer should be a MaxPooling2D layer built off the second layer


# The fourth layer should be a Conv2D layer built off the third layer


# The fifth layer should be a MaxPooling2D layer built off the fourth layer


# The sixth layer should be a Conv2D layer built off the fifth layer


# The seventh layer should be a Flatten layer built off the sixth layer


# Lastly, build one dense layer before branching to the different y branches




# Build the branches for each of the y variables
# Include a dense hidden layer in each along with the output layer.
# Remember to include the correct number of nodes for the output!

# userid


# pose


# expression


# eyes


# Assemble the model


# Compile the model


# Train the model with the training data



# Evaluate the model using the testing data
results = model.evaluate(np.array(X_test), {
        'userid_output': y_test_userid,
        'pose_output': y_test_pose,
        'expression_output': y_test_expression,
        'eyes_output': y_test_eyes
    })

pred_categories = ['userid', 'pose', 'expression', 'eyes']
for i, cat in enumerate(pred_categories):
    print(f"{cat} accuracy: {results[i+5]}")



# Import reuters database from the nltk corpus 
from nltk.corpus import reuters
# Import tokenizers
from nltk.tokenize import sent_tokenize, word_tokenize

# Download the reuters corpora and the "punkt" sentence tokenizer.
import nltk
nltk.download("reuters")
nltk.download('punkt')

# The reuters corpus includes over 10,000 news articles, many of which are about financial markets
# These articles are tagged by topic, or category
# Get the categories
print(reuters.categories())

# We'll find the first article about cocoa.
reuters.fileids(categories = 'cocoa')[0]

# Get the raw text from the first article. 
article = reuters.raw('test/15095')
print(article)

# We can mimic tokenization first by using `split()` on the article.
article.split('.')

# Then we split the first sentence on the whitespace.
sent = article.split('.')[0]
print(sent.split(' '))

# NLTK tokenizes in similar way by using the `sent_tokenize` function
sent_tokenize(article)

# We can tokenize the first sentence with the `word_tokenize` function.
sent = sent_tokenize(article)[0]
print(word_tokenize(sent))



# Import reuters database from the nltk corpus 
from nltk.corpus import reuters
# Import tokenizers
from nltk.tokenize import sent_tokenize, word_tokenize

# Download the reuters corpora and the "punkt" sentence tokenizer.
import nltk
nltk.download("reuters")
nltk.download('punkt')

# The reuters corpus includes over 10,000 news articles, many of which are about financial markets
# These articles are tagged by topic, or category
# Get the categories
print(reuters.categories())

# We'll find the first article about cocoa.
reuters.fileids(categories = 'cocoa')[0]

# Get the raw text from the first article. 
article = reuters.raw('test/15095')
print(article)

# We can mimic tokenization first by using `split()` on the article.


# Then we split the first sentence on the whitespace.


# NLTK tokenizes in similar way by using the `sent_tokenize` function


# We can tokenize the first sentence with the `word_tokenize` function.




# Import the Reuters database from the nltk corpus 
from nltk.corpus import reuters
# Import tokenizers and pandas
from nltk.tokenize import sent_tokenize, word_tokenize
import pandas as pd

# Import nltk and download the Reuters corpora and the "punkt" sentence tokenizer.
import nltk
nltk.download('reuters')
nltk.download('punkt')

# Search through all categories
print(reuters.categories())

# Get all fileids associated with income.
income_ids = reuters.fileids(categories = 'income')
print(income_ids)

# Get all raw stories and the ids in separate lists.
raw_stories = [reuters.raw(id) for id in income_ids]
# Remove the "test/" from the ids so only the id number is retained.
ids = [id.replace('test/','') for id in income_ids]

# Sentence tokenize the stories.
sentence_tokenized = [sent_tokenize(i) for i in raw_stories]

# Word tokenize all sentences using for loops. 
# Create an empty list for the tokenized words
word_tokenized = []

# Write a for loop to get each story from the tokenized sentences.
for story in sentence_tokenized:
    # Write a for loop to get all words for each story and add the words to a list.
    words = []
    for sentence in story:
        words = words + word_tokenize(sentence)
    # Append all words for each article to the word_tokenized list
    word_tokenized.append(words)

# Put the raw stories, tokenized sentences, and words into a DataFrame.
reuters_income = pd.DataFrame({'raw_stories': raw_stories,
                             'sentence_tokenized': sentence_tokenized,
                             'word_tokenized': word_tokenized
                            })

# Make the index the story ids.
reuters_income.index = ids
# Display the DataFrame
reuters_income.head()



# Import the Reuters database from the nltk corpus 
from nltk.corpus import reuters
# Import tokenizers and pandas
from nltk.tokenize import sent_tokenize, word_tokenize
import pandas as pd

# Import nltk and download the Reuters corpora and the "punkt" sentence tokenizer.
import nltk
nltk.download('reuters')
nltk.download('punkt')

# Search through all categories


# Get all fileids associated with income.


# Get all raw stories and the ids in separate lists.
# Remove the "test/" from the ids so only the id number is retained.


# Sentence tokenize the stories.


# Word tokenize all sentences using for loops. 
# Create an empty list for the tokenized words

# Write a for loop to get each story from the tokenized sentences.

    # Write a for loop to get all words for each story and add the words to a list.

    
    # Append all words for each article to the word_tokenized list


# Put the raw stories, tokenized sentences, and words into a DataFrame.


# Make the index the story ids.

# Display the DataFrame




# Import the Gutenberg and stopwords databases from the nltk corpus 
from nltk.corpus import gutenberg, stopwords
# Import tokenizers
from nltk.tokenize import word_tokenize, sent_tokenize

# Import nltk and download  the sentence tokenizer.
import nltk
nltk.download('punkt')

# Get all the fileids 
print(gutenberg.fileids())

# Get Jane Austen's book, Persuasion.
persuasion_book = gutenberg.raw(fileids=('austen-persuasion.txt'))
print(persuasion_book)

# Use the sentence tokenizer on a random sentence in Persuasion.
one_sentence = sent_tokenize(persuasion_book)[8]
print(one_sentence)

# Get all the words in the sentence.
all_words = word_tokenize(one_sentence)
print(all_words)

# Get all the nltk stopwords
sw = set(stopwords.words('english'))
print(sw)

# Filter out all the stopwords from the words in the sentence.
first_result = [word.lower() for word in all_words if word.lower() not in sw]
print(first_result)

# We can define our own list of stopwords to add to the default nltk stopwords
sw_addon = {'still', 'fifty-four'}
second_result = [word.lower() for word in all_words if word.lower() not in sw.union(sw_addon)]
print(second_result)

# Import regular expressions library
import re

# Substitute everything that is not a letter with an empty string
regex = re.compile("[^a-zA-Z ]")
re_clean = regex.sub(' ', one_sentence)
print(re_clean)

# Retrieve everything that is not a letter with an empty string
re_clean_2 = re.findall("[^a-zA-Z ]", one_sentence)
print(re_clean_2)

# Remove all the stopwords from our cleaned regular expression.
re_words = word_tokenize(re_clean)
re_result = [word.lower() for word in re_words if word.lower() not in sw.union(sw_addon)]
print(re_result)



# Import the Gutenberg and stopwords databases from the nltk corpus 
from nltk.corpus import gutenberg, stopwords
# Import tokenizers
from nltk.tokenize import word_tokenize, sent_tokenize

# Import nltk and download the sentence tokenizer.
import nltk
nltk.download('punkt')

# Get all the fileids 


# Get Jane Austen's book, Persuasion.


# Use the sentence tokenizer on a random sentence in Persuasion.


# Get all the words in the sentence.


# Get all the nltk stopwords


# Filter out all the stopwords from the words in the sentence.


# We can define our own list of stopwords to add to the default nltk stopwords


# Import regular expressions library
import re

# Substitute everything that is not a letter with an empty string


# Retrieve everything that is not a letter with an empty string


# Remove all the stopwords from our cleaned regular expression.




# Import the Reuters database from the nltk corpus 
from nltk.corpus import reuters, stopwords
# Import tokenizers and pandas
from nltk.tokenize import sent_tokenize, word_tokenize
# Import regular expressions
import re

# Import nltk the sentence tokenizer.
import nltk
nltk.download('punkt')

# Get the second article from the crude category of the Reuters library and print out the article.
crude_article = reuters.raw(fileids=reuters.fileids(categories='crude')[2])
print(crude_article)

# Write a function to clean the article using stopwords and regular expressions.
def clean_text(article):
    """
    Preprocesses a given text article by performing the following steps:
    
    1. Removes stopwords (common words in English language).
    2. Uses regular expressions to remove non-alphabet characters (e.g., punctuation).
    3. Tokenizes the cleaned text into words.
    4. Filters out words that are in the stopwords list.
    
    Parameters:
        article (str): The input text article to be processed.

    Returns:
        list of str: A list of preprocessed words from the input article.
    """
    # Get the stopwords
    sw = set(stopwords.words('english'))
    # Use regex to substitute everything that is not a letter with an empty string.
    regex = re.compile("[^a-zA-Z ]") 
    re_clean = regex.sub(' ', article)
    # Tokenize the words 
    words = word_tokenize(re_clean)
    # Retrieve only the words that aren't in the stopwords.
    output = [word.lower() for word in words if word.lower() not in sw]
    return output

# Call the function with the article and print out the unique words. 
result = clean_text(crude_article)
print(set(result))

# Write a second function that does the same as the first function, but adds custom stopwords to the NLTK stopwords.
def clean_text_again(article):
    """
    Preprocesses a given text article by performing the following steps:
    
    1. Removes stopwords (common words in English language).
    2. Creates a custom dictionary of stopwords. 
    3. Uses regular expressions to remove non-alphabet characters (e.g., punctuation).
    4. Tokenizes the cleaned text into words.
    5. Filters out words that are not stopwords.
    
    Parameters:
        article (str): The input text article to be processed.

    Returns:
        list of str: A list of preprocessed words from the input article.
    """
    # Get the stopwords
    sw = set(stopwords.words('english'))
    # Create a custom dictionary of stopwords. 
    sw_addons = {'said', 'sent', 'found', 'including', 'today', 'announced', 'week', 'basicly','also'}
    # Use regex to substitute everything that is not a letter with an empty string.
    regex = re.compile("[^a-zA-Z ]")
    re_clean = regex.sub('', article)
    # Tokenize the words 
    words = word_tokenize(re_clean)
    # Retrieve only the words not in the stopwords. Create a union of the sw and sw_addons.
    output = [word.lower() for word in words if word.lower() not in sw.union(sw_addons)]
    return output

# Call the function with the article and print out the unique words.
result2 = clean_text_again(crude_article)
print(set(result2))



# Import the Reuters database from the nltk corpus 
from nltk.corpus import reuters, stopwords
# Import tokenizers and pandas
from nltk.tokenize import sent_tokenize, word_tokenize
# Import regular expressions
import re

# Import nltk and the sentence tokenizer.
import nltk

nltk.download('punkt')

# Get the second article from the crude category of the Reuters library and print out the article.


# Write a function to clean the article using stopwords and regular expressions.
def clean_text(article):
    """
    Preprocesses a given text article by performing the following steps:
    
    1. Removes stopwords (common words in English language).
    2. Uses regular expressions to remove non-alphabet characters (e.g., punctuation).
    3. Tokenizes the cleaned text into words.
    4. Filters out words that are in the stopwords list.
    
    Parameters:
        article (str): The input text article to be processed.

    Returns:
        list of str: A list of preprocessed words from the input article.
    """
    # Get the stopwords

    # Use regex to substitute everything that is not a letter with an empty string.

    # Tokenize the words 

    # Retrieve only the words that aren't in the stopwords.


# Call the function with the article and print out the unique words. 


# Write a second function that does the same as the first function, but adds custom stopwords to the NLTK stopwords.
def clean_text_again(article):
    """
    Preprocesses a given text article by performing the following steps:
    
    1. Removes stopwords (common words in English language).
    2. Creates a custom dictionary of stopwords. 
    3. Uses regular expressions to remove non-alphabet characters (e.g., punctuation).
    4. Tokenizes the cleaned text into words.
    5. Filters out words that are not stopwords.
    
    Parameters:
        article (str): The input text article to be processed.

    Returns:
        list of str: A list of preprocessed words from the input article.
    """
    # Get the stopwords

    # Create a custom dictionary if stopwords.

    # Use regex to substitute everything that is not a letter with an empty string.

    # Tokenize the words 

    # Retrieve only the words not in the stopwords. Create a union of the sw and sw_addons.


# Call the function with the article and print out the unique words.




# Import WordNetLemmatizer class and word tokenizer
from nltk.stem import WordNetLemmatizer 
from nltk.tokenize import word_tokenize
# Download "punkt" sentence tokenizer and "wordnet" that the lemmatizer uses.
import nltk
nltk.download('punkt')
nltk.download('wordnet')

# Instantiate the lemmatizer
lemmatizer = WordNetLemmatizer()

# Lemmatize the plural of a word
print(lemmatizer.lemmatize('boxes'))
print(lemmatizer.lemmatize('geese'))

# Lemmatize the past tense of a word
print(lemmatizer.lemmatize('boxed'))
# Lemmatize an adjective of a word
print(lemmatizer.lemmatize('greater'))

# Lemmatize the past tense of a word and indicate the part of speech as a verb "v".
print(lemmatizer.lemmatize('boxed',pos='v'))
# Lemmatize an adjective of a word and indicate the part of speech as an "a".
print(lemmatizer.lemmatize('greater', pos='a'))

# Lemmatize each word from a list of words
sentence = "The foxes walked into the boxes."
words = word_tokenize(sentence)

# Apply the lemmatizer to the list of words.
result = [lemmatizer.lemmatize(word) for word in words]
print(result)



# Import NLTK and the full PorterStemmer library
from nltk.stem.porter import PorterStemmer
# Import the word tokenizer
from nltk.tokenize import word_tokenize

# Instantiate the PorterStemmer() class
p_stemmer = PorterStemmer()

# Create a list of words with different tenses.
words = ['drive', 'driven','driver','driving','drove','easily','fairly']

# Print the word and the stem of the word.
stem_words = [word+' --> '+p_stemmer.stem(word) for word in words]
stem_words

# Import the SnowballStemmer class
from nltk.stem.snowball import SnowballStemmer
# Instantiate the Snowball Stemmer and pass in the "english" language parameter.
s_stemmer = SnowballStemmer(language='english')

# Print the word and the stem of the word.
[word+' --> '+s_stemmer.stem(word) for word in words]

# Tokenize the sentence 
sentence = "The foxes walked into the boxes."
words = word_tokenize(sentence)
# Apply the Porter Stemmer to the list of words.
s_result = [p_stemmer.stem(word) for word in words]
print(s_result)



# Import WordNetLemmatizer class and word tokenizer
from nltk.stem import WordNetLemmatizer 
from nltk.tokenize import word_tokenize
# Download "punkt" sentence tokenizer and "wordnet" that the lemmatizer uses.
import nltk
nltk.download('punkt')
nltk.download('wordnet')

# Instantiate the lemmatizer
lemmatizer = WordNetLemmatizer()

# Lemmatize the plural of a word


# Lemmatize the past tense of a word

# Lemmatize an adjective of a word


# Lemmatize the past tense of a word and indicate the part of speech as a verb "v".

# Lemmatize an adjective of a word and indicate the part of speech as an "a".


# Lemmatize each word from a list of words


# Apply the lemmatizer to the list of words.




# Import NLTK and the full PorterStemmer library
import nltk
from nltk.stem.porter import PorterStemmer
# Import the word tokenizer
from nltk.tokenize import word_tokenize

# Instantiate the PorterStemmer() class
p_stemmer = PorterStemmer()

# Create a list of words with different tenses.


# Print the word and the stem of the word.


# Import the SnowballStemmer class
from nltk.stem.snowball import SnowballStemmer
# Instantiate the Snowball Stemmer and pass in the "english" language parameter.
s_stemmer = SnowballStemmer(language='english')

# Print the word and the stem of the word.


# Tokenize the sentence 

# Apply the Porter Stemmer to the list of words.




# Import the WordNetLemmatizer class 
from nltk.stem import WordNetLemmatizer 
# Import the SnowballStemmer class
from nltk.stem.snowball import SnowballStemmer
# Import the word tokenizer
from nltk.tokenize import word_tokenize
# Import gutenberg and stopwords 
from nltk.corpus import gutenberg, stopwords
# Import regular expressions
import re

# Download "punkt" sentence tokenizer and "wordnet" that the lemmatizer uses.
import nltk
nltk.download('punkt')
nltk.download('wordnet')

# Instantiate the lemmatizer
lemmatizer = WordNetLemmatizer()
# Instantiate the Snowball Stemmer and pass in the "english" language parameter.
s_stemmer = SnowballStemmer(language='english')

# Read in the entire novel
book = gutenberg.raw(fileids=('melville-moby_dick.txt'))

# Use regular expression to get all the text after "CHATPER 1".
import re

match = re.search(r'CHAPTER 1.*', book, re.DOTALL)
if match:
    # Print the matched text and everything that follows
    moby_dick = match.group(0)
print(moby_dick)

# Write a function that processes the words in Moby Dick and gets the stem of the words.
def process_text_stemming(book):
    """
    Preprocesses a given text article by performing the following steps:
    
    1. Removes stopwords (common words in English language).
    2. Uses regular expressions to remove non-alphabet characters (e.g., punctuation).
    3. Tokenizes the cleaned text into words.
    4. Gets the stem of the word.
    5. Filters out words that are not stopwords.
    
    Parameters:
        article (str): The input text article to be processed.

    Returns:
        list of str: A list of preprocessed words from the input article.
    """
    # Get the stopwords
    sw = set(stopwords.words('english'))
    # Use regex to substitute everything that is not a letter with an empty string.
    regex = re.compile("[^a-zA-Z ]")
    re_clean = regex.sub(' ', book)
    # Tokenize the words 
    words = word_tokenize(re_clean)
    # Get the stem of the words
    stem = [s_stemmer.stem(word) for word in words]
    # Retrieve only the words that aren't in the stopwords.
    output = [word.lower() for word in stem if word.lower() not in sw]
    return output

# Print the processed book as a set. 
print(set(process_text_stemming(moby_dick)))

# Write a function that processes the words in Moby Dick and lemmatizes the words to their root words.
def process_text_lemmatizaton(book):
    """
    Preprocesses a given text article by performing the following steps:
    
    1. Removes stopwords (common words in English language).
    2. Uses regular expressions to remove non-alphabet characters (e.g., punctuation).
    3. Tokenizes the cleaned text into words.
    4. Lemmatizes the words to their base form.
    5. Filters out words that are not stopwords.
    
    Parameters:
        article (str): The input text article to be processed.

    Returns:
        list of str: A list of preprocessed words from the input article.
    """
    # Get the stopwords
    sw = set(stopwords.words('english'))
    # Use regex to substitute everything that is not a letter with an empty string.
    regex = re.compile("[^a-zA-Z ]")
    re_clean = regex.sub(' ', book)
    # Tokenize the words 
    words = word_tokenize(re_clean)
    # Lemmatize the words
    lem = [lemmatizer.lemmatize(word) for word in words]
    # Retrieve only the words that aren't in the stopwords.
    output = [word.lower() for word in lem if word.lower() not in sw]
    return output

# Print the processed book as a set. 
print(set(process_text_lemmatizaton(moby_dick)))

# Write a function that processes the words in Moby Dick and lemmatizes the adverbs to their root words.
def process_adv_lemmatizaton(book):
    """
    Preprocesses a given text article by performing the following steps:
    
    1. Removes stopwords (common words in English language).
    2. Uses regular expressions to remove non-alphabet characters (e.g., punctuation).
    3. Tokenizes the cleaned text into words.
    4. Lemmatizes the words to their verb form.
    5. Filters out words that are not stopwords.
    
    Parameters:
        article (str): The input text article to be processed.

    Returns:
        list of str: A list of preprocessed words from the input article.
    """
    # Get the stopwords
    sw = set(stopwords.words('english'))
    # Use regex to substitute everything that is not a letter with an empty string.
    regex = re.compile("[^a-zA-Z ]")
    re_clean = regex.sub(' ', book)
    # Tokenize the words 
    words = word_tokenize(re_clean)
    # Lemmatize the words
    lem = [lemmatizer.lemmatize(word,pos='r') for word in words]
    # Retrieve only the words that aren't in the stopwords.
    output = [word.lower() for word in lem if word.lower() not in sw]
    return output

# Print the processed book as a set. 
print(set(process_adv_lemmatizaton(moby_dick)))



# Import the WordNetLemmatizer class 
from nltk.stem import WordNetLemmatizer 
# Import the SnowballStemmer class
from nltk.stem.snowball import SnowballStemmer
# Import the word tokenizer
from nltk.tokenize import word_tokenize
# Import gutenberg and stopwords 
from nltk.corpus import gutenberg, stopwords
# Import regular expressions
import re

# Download "punkt" sentence tokenizer and "wordnet" that the lemmatizer uses.
import nltk
nltk.download('punkt')
nltk.download('wordnet')

# Instantiate the lemmatizer

# Instantiate the Snowball Stemmer and pass in the "english" language parameter.


# Read in the entire novel
book = gutenberg.raw(fileids=('melville-moby_dick.txt'))

# Use regular expression to get all the text after "CHATPER 1".
import re

match = re.search(r'CHAPTER 1.*', book, re.DOTALL)
if match:
    # Print the matched text and everything that follows
    moby_dick = match.group(0)
print(moby_dick)

# Write a function that processes the words in Moby Dick and gets the stem of the words.
def process_text_stemming(book):
    """
    Preprocesses a given text article by performing the following steps:
    
    1. Removes stopwords (common words in English language).
    2. Uses regular expressions to remove non-alphabet characters (e.g., punctuation).
    3. Tokenizes the cleaned text into words.
    4. Gets the stem of the word.
    5. Filters out words that are not stopwords..
    
    Parameters:
        article (str): The input text article to be processed.

    Returns:
        list of str: A list of preprocessed words from the input article.
    """
    # Get the stopwords
    
    # Use regex to substitute everything that is not a letter with an empty string.
    
    
    # Tokenize the words 
    
    # Get the stem of the words
    
    # Retrieve only the words that aren't in the stopwords.
    
    

# Print the processed book as a set. 


# Write a function that processes the words in Moby Dick and lemmatizes the words to their root words.
def process_text_lemmatizaton(book):
    """
    Preprocesses a given text article by performing the following steps:
    
    1. Removes stopwords (common words in English language).
    2. Uses regular expressions to remove non-alphabet characters (e.g., punctuation).
    3. Tokenizes the cleaned text into words.
    4. Lemmatizes the words to their base form.
    5. Filters out words that are not stopwords.
    
    Parameters:
        article (str): The input text article to be processed.

    Returns:
        list of str: A list of preprocessed words from the input article.
    """
    # Get the stopwords
    
    # Use regex to substitute everything that is not a letter with an empty string.
    
    
    # Tokenize the words 
    
    # Lemmatize the words
    
    # Retrieve only the words that aren't in the stopwords.
    
    

# Print the processed book as a set. 


# Write a function that processes the words in Moby Dick and lemmatizes the verbs to their root words.
def process_text_lemmatizaton(book):
    """
    Preprocesses a given text article by performing the following steps:
    
    1. Removes stopwords (common words in English language).
    2. Uses regular expressions to remove non-alphabet characters (e.g., punctuation).
    3. Tokenizes the cleaned text into words.
    4. Lemmatizes the words to their noun, verb, adjective or adverb form.
    5. Filters out words that are not stopwords..
    
    Parameters:
        article (str): The input text article to be processed.

    Returns:
        list of str: A list of preprocessed words from the input article.
    """
    # Get the stopwords
    
    # Use regex to substitute everything that is not a letter with an empty string.
    
    
    # Tokenize the words 
    
    # Lemmatize the verbs
    
    # Retrieve only the words that aren't in the stopwords.
    
    

# Print the processed book as a set. 




# Import reuters and stopwords 
from nltk.corpus import reuters, stopwords
# Import ngrams
from nltk.util import ngrams
# Import the WordNetLemmatizer class 
from nltk.stem import WordNetLemmatizer 
# Import the word tokenizer
from nltk.tokenize import word_tokenize
# Import regular expressions
import re
# Download "punkt" sentence tokenizer and "wordnet" that the lemmatizer uses.
import nltk
nltk.download('punkt')
nltk.download('wordnet')

# Instantiate the lemmatizer
lemmatizer = WordNetLemmatizer()

# Get the categories
print(reuters.categories())

# Get a random article from the consumer price index (cpi) category
cpi_article = reuters.raw(reuters.fileids(categories='cpi')[2])
print(cpi_article)

# Write a function that processes the words for the article and and lemmatizes the words to their root words.
def process_text(article):
    """
    Preprocesses a given text article by performing the following steps:
    
    1. Removes stopwords (common words in English language).
    2. Uses regular expressions to remove non-alphabet characters (e.g., punctuation).
    3. Tokenizes the cleaned text into words.
    4. Lemmatizes the words to their base form.
    5. Filters out words that are not stopwords.
    
    Parameters:
        article (str): The input text article to be processed.

    Returns:
        list of str: A list of preprocessed words from the input article.
    """
    # Get the stopwords
    sw = set(stopwords.words('english'))
    # Use regex to substitute everything that is not a letter with an empty string.
    regex = re.compile("[^a-zA-Z ]")
    re_clean = regex.sub('', article)
    # Tokenize the words 
    words = word_tokenize(re_clean)
    # Lemmatize the words
    lem = [lemmatizer.lemmatize(word) for word in words]
    # Retrieve only the words that aren't in the stopwords
    output = [word.lower() for word in lem if word.lower() not in sw]
    return output

# Pass the article to function and print the processed text.
processed_article = process_text(cpi_article)
print(processed_article)

# Import the Counter class from the collections library.
from collections import Counter

# Get the word counts by passing in the processed article to the Counter class.
word_counts = Counter(processed_article)
# Print the dictionary of the word counts.
print(dict(word_counts))

# Print the top 10 most common words.
print(dict(word_counts.most_common(10)))

# Get the number of bigrams.
bigram_counts = Counter(ngrams(processed_article, n=2))
print(dict(bigram_counts))

# Print the top 5 most common bigrams
print(dict(bigram_counts.most_common(5)))



# Import reuters and stopwords 
from nltk.corpus import reuters, stopwords
# Import ngrams
from nltk.util import ngrams
# Import the WordNetLemmatizer class 
from nltk.stem import WordNetLemmatizer 
# Import the word tokenizer
from nltk.tokenize import word_tokenize
# Import regular expressions
import re
# Download "punkt" sentence tokenizer and "wordnet" that the lemmatizer uses.
import nltk
nltk.download('punkt')
nltk.download('wordnet')

# Instantiate the lemmatizer


# Get the categories


# Get a random article from the consumer price index (cpi) category


# Write a function that processes the words for the article and and lemmatizes the words to their root words.
def process_text(article):
    """
    Preprocesses a given text article by performing the following steps:
    
    1. Removes stopwords (common words in English language).
    2. Uses regular expressions to remove non-alphabet characters (e.g., punctuation).
    3. Tokenizes the cleaned text into words.
    4. Lemmatizes the words to their base form.
    5. Filters out words that are not stopwords.
    
    Parameters:
        article (str): The input text article to be processed.

    Returns:
        list of str: A list of preprocessed words from the input article.
    """
    # Get the stopwords
    
    # Use regex to substitute everything that is not a letter with an empty string.
    
    
    # Tokenize the words 
    
    # Lemmatize the words
    
    # Retrieve only the words that aren't in the stopwords
    

# Pass the article to function and print the processed text.


# Import the Counter class from the collections library.
from collections import Counter

# Get the word counts by passing in the processed article to the Counter class.

# Print the dictionary of the word counts.


# Print the top 10 most common words.


# Get the number of bigrams.
bigram_counts = Counter(ngrams(processed_article, n=2))


# Print the top 5 most common bigrams




# Import reuters and stopwords 
from nltk.corpus import reuters, stopwords
# Import ngrams
from nltk.util import ngrams
# Import the WordNetLemmatizer class 
from nltk.stem import WordNetLemmatizer 
# Import the word tokenizer
from nltk.tokenize import word_tokenize
# Import regular expressions
import re
# Import pandas 
import pandas as pd
# Import the Counter class from the collections library.
from collections import Counter

# Download "punkt" sentence tokenizer and "wordnet" that the lemmatizer uses.
import nltk
nltk.download('punkt')
nltk.download('wordnet')

# Instantiate the lemmatizer
lemma = WordNetLemmatizer()

# List the articles about grains.
ids = reuters.fileids(categories='grain')
corpus = [reuters.raw(i) for i in ids]

# Define the process_text function that processes the words for the article and and lemmatizes the words to their root.
def process_text(article):
    """
    Preprocesses a given text article by performing the following steps:
    
    1. Removes stopwords (common words in English language).
    2. Uses regular expressions to remove non-alphabet characters (e.g., punctuation).
    3. Tokenizes the cleaned text into words.
    4. Lemmatizes the words to their base form.
    5. Filters out words that are in the stopwords list.
    
    Parameters:
        article (str): The input text article to be processed.

    Returns:
        list of str: A list of preprocessed words from the input article.
    """
    # Get the stopwords
    sw = set(stopwords.words('english'))
    # Use regex to substitute everything that is not a letter with an empty string.
    regex = re.compile("[^a-zA-Z ]")
    re_clean = regex.sub('', article)
    # Tokenize the words 
    words = word_tokenize(re_clean)
    # Lemmatize the words
    lem = [lemma.lemmatize(word) for word in words]
    # Retrieve only the words that aren't in the stopwords
    output = [word.lower() for word in lem if word.lower() not in sw]
    return output

# Create a word_counter function that takes all the articles and processes them with the process_text function.
def word_counter(corpus): 
    """
    Counts and returns the top ten most common words in a given corpus of text.

    This function processes the text in the input corpus by:
    1. Combining all articles into one large string.
    2. Passing the combined text to the `process_text` function, which removes stopwords, 
       non-alphabet characters, tokenizes the text, and lemmatizes words.
    3. Counts the occurrences of each word and retrieves the top ten most common words.

    Parameters:
        corpus (list of str): A list of text articles to be analyzed.

    Returns:
        pandas.DataFrame: A DataFrame containing the top ten words found in the corpus,
        with two columns: "word" (the word itself) and "count" (the number of times it appears).
    """
    # Combine all articles in corpus into one large string
    big_string = ' '.join(corpus)
    # Pass the combined articles to the process_text function
    processed = process_text(big_string)
    # Get the top ten most common words.
    top_10 = dict(Counter(processed).most_common(10))
    # Create a DataFrame with the top ten words with "word" and "count" columns.
    return pd.DataFrame(list(top_10.items()), columns=['word', 'count'])

# Pass the corpus of articles to the word_counter function.
word_counter(corpus)

def bigram_counter(corpus): 
    """
    Counts and returns the top ten most common bigrams in a given corpus of text.

    This function processes the text in the input corpus by:
    1. Combining all articles into one large string.
    2. Passing the combined text to the `process_text` function, which removes stopwords, 
       non-alphabet characters, tokenizes the text, and lemmatizes words.
    3. Creates bigrams (pairs of adjacent words) from the processed text.
    4. Counts the occurrences of each bigram and retrieves the top ten most common bigrams.

    Parameters:
        corpus (list of str): A list of text articles to be analyzed.

    Returns:
        pandas.DataFrame: A DataFrame containing the top ten bigrams found in the corpus,
        with two columns: "bigram" (the pair of adjacent words) and "count" (the number of times it appears).
    """
    # Combine all articles in corpus into one large string
    big_string = ' '.join(corpus)
    # Pass the combined articles to the process_text function
    processed = process_text(big_string)
    # Create bigrams from the processed text.
    bigrams = ngrams(processed, n=2)
    # Get the top ten most common bigrams
    top_10 = dict(Counter(bigrams).most_common(10))
    # Create a DataFrame with the top ten bigrams with "bigram" and "count" columns.
    return pd.DataFrame(list(top_10.items()), columns=['bigram', 'count'])

# Pass the corpus of articles to the bigram_counter function.
bigram_counter(corpus)



# Import reuters and stopwords 
from nltk.corpus import reuters, stopwords
# Import ngrams
from nltk.util import ngrams
# Import the WordNetLemmatizer class 
from nltk.stem import WordNetLemmatizer 
# Import the word tokenizer
from nltk.tokenize import word_tokenize
# Import regular expressions
import re
# Import pandas 
import pandas as pd
# Import the Counter class from the collections library.
from collections import Counter

# Download "punkt" sentence tokenizer and "wordnet" that the lemmatizer uses.
import nltk
nltk.download('punkt')
nltk.download('wordnet')

# Instantiate the lemmatizer
lemma = WordNetLemmatizer()

# List the articles about grains.
ids = reuters.fileids(categories='grain')
corpus = [reuters.raw(i) for i in ids]

# Define the process_text function that processes the words for the article and and lemmatizes the words to their root.
def process_text(article):
    """
    Preprocesses a given text article by performing the following steps:
    
    1. Removes stopwords (common words in English language).
    2. Uses regular expressions to remove non-alphabet characters (e.g., punctuation).
    3. Tokenizes the cleaned text into words.
    4. Lemmatizes the words to their base form.
    5. Filters out words that are not stopwords.
    
    Parameters:
        article (str): The input text article to be processed.

    Returns:
        list of str: A list of preprocessed words from the input article.
    """
    # Get the stopwords
    sw = set(stopwords.words('english'))
    # Use regex to substitute everything that is not a letter with an empty string.
    regex = re.compile("[^a-zA-Z ]")
    re_clean = regex.sub('', article)
    # Tokenize the words 
    words = word_tokenize(re_clean)
    # Lemmatize the words
    lem = [lemma.lemmatize(word) for word in words]
    # Retrieve only the words that aren't in the stopwords
    output = [word.lower() for word in lem if word.lower() not in sw]
    return output

# Create a word_counter function that takes all the articles and processes them with the process_text function.
def word_counter(corpus): 
    """
    Counts and returns the top ten most common words in a given corpus of text.

    This function processes the text in the input corpus by:
    1. Combining all articles into one large string.
    2. Passing the combined text to the `process_text` function, which removes stopwords, 
       non-alphabet characters, tokenizes the text, and lemmatizes words.
    3. Counts the occurrences of each word and retrieves the top ten most common words.

    Parameters:
        corpus (list of str): A list of text articles to be analyzed.

    Returns:
        pandas.DataFrame: A DataFrame containing the top ten words found in the corpus,
        with two columns: "word" (the word itself) and "count" (the number of times it appears).
    """
    # Combine all articles in corpus into one large string

    # Pass the combined articles to the process_text function

    # Get the top ten most common words.

    # Create a DataFrame with the top ten words with "word" and "count" columns.

    

# Pass the corpus of articles to the word_counter function.


def bigram_counter(corpus): 
    """
    Counts and returns the top ten most common bigrams in a given corpus of text.

    This function processes the text in the input corpus by:
    1. Combining all articles into one large string.
    2. Passing the combined text to the `process_text` function, which removes stopwords, 
       non-alphabet characters, tokenizes the text, and lemmatizes words.
    3. Creates bigrams (pairs of adjacent words) from the processed text.
    4. Counts the occurrences of each bigram and retrieves the top ten most common bigrams.

    Parameters:
        corpus (list of str): A list of text articles to be analyzed.

    Returns:
        pandas.DataFrame: A DataFrame containing the top ten bigrams found in the corpus,
        with two columns: "bigram" (the pair of adjacent words) and "count" (the number of times it appears).
    """
    # Combine all articles in corpus into one large string

    # Pass the combined articles to the process_text function

    # Create bigrams from the processed text.

    # Get the top ten most common bigrams

    # Create a DataFrame with the top ten bigrams with "bigram" and "count" columns.

    

# Pass the corpus of articles to the bigram_counter function.




# Import nltk, numpy and pandas.
import nltk
import numpy as np
import pandas as pd
# Import Reuters
from nltk.corpus import reuters
# Import CountVectorizer, TfidfVectorizer from sklearn
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# Download the Reuters dataset if you didn't install it.
# nltk.download("reuters")

# Get the categories
print(reuters.categories())

# Count the total number of documents in the collection
doc_ids = reuters.fileids()
# Retrieve the number of documents in the corpus.
print(f"Total number of docs in the corpus: {len(doc_ids)}")

# Select and print a single document of text.
doc_text = reuters.raw(doc_ids[2])
print(doc_text)

# Create an instance of the CountVectorizer and define the English stopwords to be ignored.
vectorizer = CountVectorizer(stop_words="english")

# Tokenize the text into numerical features and occurrence of each word.
X = vectorizer.fit_transform([doc_text])

# X contains the occurrence of each term in the document.
# We have 1 document, the first number in the tuple represents the document number, i.e., 0.
# The second number in the tuple represents the index of the word in the vocabulary created by fit_transform.
# The last number represents how many times the word appears.
print(X)

# Retrieve unique words list
words = vectorizer.get_feature_names_out()
print(words)

# Get the length of the words and find a specific word or term.
print(len(words))
print(words[25])

# Print the number of times each word appears from the document. 
occurrences = X.toarray()[0]
print(occurrences)

# Convert the sparse matrix to a DataFrame to get our Bag-of-Words for the document. 
bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())

# Display some first 20 columns of the DataFrame.
bow_df.iloc[:,0:20:]

# Melt the Bag-of-Words DataFrame to convert columns into rows.
melted_bow = bow_df.melt(var_name='Word', value_name='Word_Counts')
melted_bow.head()

# Sort the DataFrame by Word_Counts if needed
sorted_bow = melted_bow.sort_values(by='Word_Counts', ascending=False).reset_index(drop=True)
sorted_bow.head()

# Alternatively you can do the following:
# Create a list to hold the words using the vectorizer.get_feature_names_out()
words = list(vectorizer.get_feature_names_out())
# Create a list to hold the frequency using np.ravel(X.sum(axis=0))
frequency = list(np.ravel(X.sum(axis=0)))

# Create a DataFrame of the TFâ€“IDF weights for each word in the working corpus.
words_df = pd.DataFrame({
  "Word": words,
  "Word_Count": frequency})

# Alternatively you can use:
# words_df = pd.DataFrame(list(zip(words, np.ravel(X.sum(axis=0)))), columns=["Word", "Word_Count"])

# Sort the DataFrame on the Word_Count in descending order and reset the index.
sorted_words= words_df.sort_values(by=["Word_Count"], ascending=False).reset_index(drop=True)
sorted_words.head()

# Getting the first 1000 articles from Reuters.
corpus_id = doc_ids[0:1000]
corpus = [reuters.raw(doc) for doc in corpus_id]

# Print sample document
print(corpus[50])

# Create an instance of the TfidfVectorizer and define the English stopwords to be ignored.
vectorizer = TfidfVectorizer(stop_words="english")
# Tokenize the 1,000 articles into numerical features.
X_corpus = vectorizer.fit_transform(corpus)

# Print the sparse matrix of the transformed data.
# We have 1,000 documents, the first number in the tuple represents the document number.
# The second number in the tuple represents the index of the word in the vocabulary created by fit_transform.
# The last number represents the value of the TF-IDF score for the vocabulary word.
print(X_corpus)

# Get the matrix info.
print(f"Matrix shape: {X_corpus.shape}")
print(f"Total number of documents: {X_corpus.shape[0]}")
print(f"Total number of unique words (tokens): {X_corpus.shape[1]}")

# Retrieve words list from corpus
words_corpus = vectorizer.get_feature_names_out()
print(words_corpus)

# Get the TF-IDF weights of each word in corpus as DataFrame
words_corpus_df = pd.DataFrame(
    list(zip(words_corpus, np.ravel(X_corpus.mean(axis=0)))), columns=["Word", "TF-IDF"])

# Sort the DataFrame to show the top TF-IDF values.
sorted_words_corpus = words_corpus_df.sort_values(by=["TF-IDF"], ascending=False).reset_index(drop=True)

# Highest 10 TF-IDF scores
sorted_words_corpus.head(10)

# Lowest 10 TF-IDF scores
sorted_words_corpus.tail(10)



# Import nltk, numpy and pandas.
import nltk
import numpy as np
import pandas as pd
# Import Reuters
from nltk.corpus import reuters
# Import CountVectorizer, TfidfVectorizer from sklearn
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# Download the Reuters dataset if you didn't install it.
# nltk.download("reuters")

# Get the categories


# Count the total number of documents in the collection

# Retrieve the number of documents in the corpus.
print(f"Total number of docs in the corpus: {len(doc_ids)}")

# Select and print a single document of text.


# Create an instance of the CountVectorizer and define the English stopwords to be ignored.


# Tokenize the text into numerical features and occurrence of each word.


# X contains the occurrence of each term in the document.
# We have 1 document, the first number in the tuple represents the document number, i.e., 0.
# The second number in the tuple represents the index of the word in the vocabulary created by fit_transform.
# The last number represents how many times the word appears.


# Retrieve unique words list


# Get the length of the words and find a specific word or term.


# Print the number of times each word appears from the document. 


# Convert the sparse matrix to a DataFrame to get our Bag-of-Words for the document. 



# Display some first 20 columns of the DataFrame.


# Melt the Bag-of-Words DataFrame to convert columns into rows.


# Sort the DataFrame by Word_Counts if needed


# Alternatively you can do the following:
# Create a list to hold the words using the vectorizer.get_feature_names_out()
words = list(vectorizer.get_feature_names_out())
# Create a list to hold the frequency using np.ravel(X.sum(axis=0))
frequency = list(np.ravel(X.sum(axis=0)))

# Create a DataFrame of the TFâ€“IDF weights for each word in the working corpus.
words_df = pd.DataFrame({
  "Word": words,
  "Word_Count": frequency})

# Alternatively you can use:
# words_df = pd.DataFrame(list(zip(words, np.ravel(X.sum(axis=0)))), columns=["Word", "Word_Count"])

# Sort the DataFrame on the Word_Count in descending order and reset the index.
sorted_words= words_df.sort_values(by=["Word_Count"], ascending=False).reset_index(drop=True)
sorted_words.head()

# Getting the first 1000 articles from Reuters.


# Print sample document
print(corpus[50])

# Create an instance of the TfidfVectorizer and define the English stopwords to be ignored.

# Tokenize the 1,000 articles into numerical features.


# Print the sparse matrix of the transformed data.
# We have 1,000 documents, the first number in the tuple represents the document number.
# The second number in the tuple represents the index of the word in the vocabulary created by fit_transform.
# The last number represents the value of the TF-IDF score for the vocabulary word.


# Get the matrix info.
print(f"Matrix shape: {}")
print(f"Total number of documents: {}")
print(f"Total number of unique words (tokens): {}")

# Retrieve words list from corpus


# Get the TF-IDF weights of each word in corpus as DataFrame


# Sort the DataFrame to show the top TF-IDF values.


# Highest 10 TF-IDF scores


# Lowest 10 TF-IDF scores




# Import nltk, numpy and pandas.
import nltk
import numpy as np
import pandas as pd
# Import Reuters
from nltk.corpus import reuters
# Import TfidfVectorizer from sklearn
from sklearn.feature_extraction.text import  TfidfVectorizer

# Download the Reuters dataset
nltk.download("reuters")

# Get the categories
print(reuters.categories())

# Get all the "fileids" in the "money-fx" and "money-supply" categories.
categories = ["money-fx", "money-supply"]
docs_id = reuters.fileids()

# Use a list comprehension or for loop to get the all the fieldids.
money_news_ids = [
    doc
    for doc in docs_id
    if categories[0] in reuters.categories(doc)
    or categories[1] in reuters.categories(doc)
]

# Print the total number of news articles about money.
print(f"Total number of news articles about money: {len(money_news_ids)}")

# Use a list comprehension or for loop to retrieve the text from the corpus containing all the news articles about money.
money_news = [reuters.raw(doc) for doc in money_news_ids]

# Print a sample article
print(money_news[78])

# Create an instance of the TfidfVectorizer and define the English stopwords to be ignored.
vectorizer = TfidfVectorizer(stop_words="english")
# Tokenize the articles about money into numerical features.
X = vectorizer.fit_transform(money_news)

# Create a list to hold the words using the vectorizer.get_feature_names_out()
words = list(vectorizer.get_feature_names_out())
# Create a list to hold the frequency using np.ravel(X.sum(axis=0))
frequency = list(np.ravel(X.sum(axis=0)))

# Create a DataFrame of the TFâ€“IDF weights for each word in the working corpus.
money_news_df = pd.DataFrame({
    "Word": words,
    "Frequency": frequency})

# Display the DataFrame
money_news_df.head(10)

# Sort the DataFrame by word frequency in descending order and reset the index.
money_news_df = money_news_df.sort_values(by=["Frequency"], ascending=False).reset_index(drop=True)

# Print the top 10 words
money_news_df.head(10)

# Alternative: Create a DataFrame of the TFâ€“IDF weights for each term in the working corpus. 
money_news_df = pd.DataFrame(
    list(zip(vectorizer.get_feature_names_out(), np.ravel(X.sum(axis=0)))),
    columns=["Word", "Frequency"],
)

# Sort the DataFrame by word frequency in descending order and reset the index..
money_news_df = money_news_df.sort_values(by=["Frequency"], ascending=False).reset_index(drop=True)

# Print the top 10 words
money_news_df.head(10)

# Write a function called `retrieve_docs(terms)`that searches the "money_news_ids" list 
# and retrieves the number of articles for a given word or group of words.
def retrieve_docs(terms):
    """
    Retrieve a list of document IDs that contain at least one of the specified terms.

    This function searches through a collection of documents represented by 'money_news_ids'
    to identify documents that contain at least one of the provided terms. It utilizes the
    NLTK Reuters corpus and tokenizes each document to find matches based on the lowercase
    representation of words.

    Parameters:
    terms (list of str): A list of terms to search for within the documents.

    Returns:
    list of str: A list of document IDs that contain at least one of the specified terms.

    Example:
    >>> retrieve_docs(['stock', 'market', 'invest'])
    ['doc1', 'doc3', 'doc5']
    """
    # Create an empty list to hold the results.
    result_docs = []
    # Use a for loop to loop through the money_news_ids.
    for doc_id in money_news_ids:
        # Use a list comprehension or a for loop to extract words from the document using reuters.words(doc_id)) 
        # then populates list comprehension using a conditional statement that checks for any words in 
        # lowercase matching the "terms" passed to the function.
        found_terms = [
            word
            for word in reuters.words(doc_id)
            if any(term in word.lower() for term in terms)]
        # Use a conditional statement that checks whether there are is at least one term from the input 
        # list that was found in the document. If it is found, the append the article id to the list. 
        if len(found_terms) > 0:
            result_docs.append(doc_id)
    return result_docs

len(retrieve_docs(["yen"]))

len(retrieve_docs(["japan", "banks"]))

len(retrieve_docs(["england", "dealers"]))



# Import nltk, numpy and pandas.
import nltk
import numpy as np
import pandas as pd
# Import Reuters
from nltk.corpus import reuters
# Import CountVectorizer, TfidfVectorizer from sklearn
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# Download the Reuters dataset
nltk.download("reuters")

# Get the categories
print(reuters.categories())

# Get all the "fileids" in the "money-fx" and "money-supply" categories.


# Use a list comprehension or for loop to get the all the fieldids.


# Print the total number of news articles about money.
print(f"Total number of news articles about money: {len(money_news_ids)}")

# Use a list comprehension or for loop to retrieve the text from the corpus containing all the news articles about money.


# Print a sample article


# Create an instance of the TfidfVectorizer and define the English stopwords to be ignored.

# Tokenize the articles about money into numerical features.


# Create a list to hold the words using the vectorizer.get_feature_names_out()

# Create a list to hold the frequency using np.ravel(X.sum(axis=0))


# Create a DataFrame of the TFâ€“IDF weights for each word in the working corpus.


# Display the DataFrame

# Sort the DataFrame by word frequency in descending order and reset the index.


# Print the top 10 words


# Use the `retrieve_docs(terms)` function that searches the "money_news_ids" list 
# and retrieves the number of article ids for a given word or group of words.
def retrieve_docs(terms):
    """
    Retrieve a list of document IDs that contain at least one of the specified terms.

    This function searches through a collection of documents represented by 'money_news_ids'
    to identify documents that contain at least one of the provided terms. It utilizes the
    NLTK Reuters corpus and tokenizes each document to find matches based on the lowercase
    representation of words.

    Parameters:
    terms (list of str): A list of terms to search for within the documents.

    Returns:
    list of str: A list of document IDs that contain at least one of the specified terms.

    Example:
    >>> retrieve_docs(['stock', 'market', 'invest'])
    ['doc1', 'doc3', 'doc5']
    """
    # Create an empty list to hold the results.
    
    # Use a for loop to loop through the money_news_ids.
    
        # Use a list comprehension or a for loop to extract words from the document using reuters.words(doc_id)) 
        # then populates list comprehension using a conditional statement that checks for any words in 
        # lowercase matching the "terms" passed to the function.
        
        
        
        # Use a conditional statement that checks whether there are is at least one term from the input 
        # list that was found in the document. If it is found, the append the article id to the list.  
        
        
    return result_docs









# Import pandas and numpy
import pandas as pd
import numpy as np
# Import the required dependencies from sklearn
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline
from sklearn import metrics

# Set the column width to view the text message data.
pd.set_option('max_colwidth', 200)

# Load the dataset into a DataFrame
sms_text_df = pd.read_csv('Resources/SMSSpamCollection.csv')
sms_text_df.head()

# Check for missing values. 
sms_text_df.info()

#  Get the number of "ham" and "spam" from the "label" column:
sms_text_df['label'].value_counts()

# Set the features variable to the text message. 
X = sms_text_df['text_message']  
# Set the target variable to the "label" column.
y = sms_text_df['label']

# Split data into training and testing and set the test_size = 33%
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

# Create an instance of the TfidfVectorizer
vectorizer = TfidfVectorizer(stop_words='english')

# Transform the data and use the original X_train set.
X_train_tfidf = vectorizer.fit_transform(X_train) 
X_train_tfidf.shape

# What stopwords are in the scikit-learn's built-in list
from sklearn.feature_extraction import text
print(text.ENGLISH_STOP_WORDS)

# Create a list to hold the words using the vectorizer.get_feature_names_out()
words = list(vectorizer.get_feature_names_out())
# Create a list to hold the frequency using np.ravel(X.sum(axis=0))
frequency = list(np.ravel(X_train_tfidf.sum(axis=0)))

# Create a DataFrame of the TFâ€“IDF weights for each word in the working corpus.
messages_df = pd.DataFrame({
    "Word": words,
    "Frequency": frequency})

# Sort the DataFrame by word frequency in descending order and reset the index.
messages_df = messages_df.sort_values(by=["Frequency"], ascending=False).reset_index(drop=True)

# Display the DataFrame
messages_df.head(10)

# Display the DataFrame
messages_df.tail(10)

# Train the data on LinearSVC classifier.
linear_svc_model = LinearSVC()
# Fit the model to the transformed  data,
linear_svc_model.fit(X_train_tfidf,y_train)

# Determine predictions. 
# Run this to show that we can't run predcitions on our linear SVC model. 
# We have to transform our testing data the same way as we transformed the training data. 
predictions = linear_svc_model.predict(X_test)

# Transform the testing data like we did with the training data.
X_test_tfidf = vectorizer.transform(X_test) 
# Make predictions 
predictions = linear_svc_model.predict(X_test_tfidf)
print(predictions[:30])

# Validate the model by checking the model accuracy with model.score
print('Train Accuracy: %.3f' % linear_svc_model.score(X_train_tfidf, y_train))
print('Test Accuracy: %.3f' % linear_svc_model.score(X_test_tfidf, y_test))

# Build a pipeline to transform the test set to compare to the training set. 
text_clf = Pipeline([('tfidf', TfidfVectorizer(stop_words='english')),
                     ('clf', LinearSVC()),
])

# Fit the model to the transformed data.
text_clf.fit(X_train, y_train)  

# Validate the model by checking the model accuracy with model.score
print('Train Accuracy: %.3f' % text_clf.score(X_train, y_train))
print('Test Accuracy: %.3f' % text_clf.score(X_test, y_test))

# Form a prediction set
message_predictions = text_clf.predict(X_test)
print(message_predictions[0:30])

# Create the confusion matrix on the test data and predictions
print(metrics.confusion_matrix(y_test,message_predictions))

# Print a classification report
print(metrics.classification_report(y_test,message_predictions))

# Print the overall accuracy
print(metrics.accuracy_score(y_test,message_predictions))

# Create some random text messages. 
text_1 = """You are a lucky winner of $5000!!"""
text_2 = """You won 2 free tickets to the Super Bowl."""
text_3 = """You won 2 free tickets to the Super Bowl text us to claim your prize"""
text_4 = """Thanks for registering. Text 4343 to receive free updates on medicare"""

# Send the text messages to transform the data and predict the classification.
print(text_clf.predict([text_1]))
print(text_clf.predict([text_2]))
print(text_clf.predict([text_3]))
print(text_clf.predict([text_4]))



# Import pandas and numpy
import pandas as pd
import numpy as np
# Import the required dependencies from sklearn
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline
from sklearn import metrics

# Set the column width to view the text message data.
pd.set_option('max_colwidth', 200)

# Load the dataset into a DataFrame
sms_text_df = pd.read_csv('Resources/SMSSpamCollection.csv')
sms_text_df.head()

# Check for missing values. 
sms_text_df.info()

#  Get the number of "ham" and "spam" from the "label" column:
sms_text_df['label'].value_counts()

# Set the features variable to the text message. 
X = sms_text_df['text_message']  
# Set the target variable to the "label" column.
y = sms_text_df['label']

# Split data into training and testing and set the test_size = 33%
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

# Create an instance of the TfidfVectorizer
vectorizer = TfidfVectorizer(stop_words='english')

# Transform the data and use the original X_train set.
X_train_tfidf = vectorizer.fit_transform(X_train) 
X_train_tfidf.shape

# What stopwords are in the scikit-learn's built-in list
from sklearn.feature_extraction import text
print(text.ENGLISH_STOP_WORDS)

# Create a list to hold the words using the vectorizer.get_feature_names_out()
words = list(vectorizer.get_feature_names_out())
# Create a list to hold the frequency using np.ravel(X.sum(axis=0))
frequency = list(np.ravel(X_train_tfidf.sum(axis=0)))

# Create a DataFrame of the TFâ€“IDF weights for each word in the working corpus.
messages_df = pd.DataFrame({
    "Word": words,
    "Frequency": frequency})

# Sort the DataFrame by word frequency in descending order and reset the index.
messages_df = messages_df.sort_values(by=["Frequency"], ascending=False).reset_index(drop=True)

# Display the DataFrame
messages_df.head(10)

# Display the DataFrame
messages_df.tail(10)

# Train the data on LinearSVC classifier.
linear_svc_model = LinearSVC()
# Fit the model to the transformed  data,
linear_svc_model.fit(X_train_tfidf,y_train)

# Determine predictions. 
# Run this to show that we can't run predcitions on our linear SVC model. 
# We have to transform our testing data the same way as we transformed the training data. 
predictions = linear_svc_model.predict(X_test)

# Transform the testing data like we did with the training data.
X_test_tfidf = vectorizer.transform(X_test) 
# Make predictions 
predictions = linear_svc_model.predict(X_test_tfidf)
print(predictions[:30])

# Validate the model by checking the model accuracy with model.score
print('Train Accuracy: %.3f' % linear_svc_model.score(X_train_tfidf, y_train))
print('Test Accuracy: %.3f' % linear_svc_model.score(X_test_tfidf, y_test))

# Build a pipeline to transform the test set to compare to the training set. 
text_clf = Pipeline([('tfidf', TfidfVectorizer(stop_words='english')),
                     ('clf', LinearSVC()),
])

# Fit the model to the transformed data.
text_clf.fit(X_train, y_train)  

# Validate the model by checking the model accuracy with model.score
print('Train Accuracy: %.3f' % text_clf.score(X_train, y_train))
print('Test Accuracy: %.3f' % text_clf.score(X_test, y_test))

# Form a prediction set
message_predictions = text_clf.predict(X_test)
print(message_predictions[0:30])

# Create the confusion matrix on the test data and predictions
print(metrics.confusion_matrix(y_test,message_predictions))

# Print a classification report
print(metrics.classification_report(y_test,message_predictions))

# Print the overall accuracy
print(metrics.accuracy_score(y_test,message_predictions))

# Create some random text messages. 
text_1 = """You are a lucky winner of $5000!!"""
text_2 = """You won 2 free tickets to the Super Bowl."""
text_3 = """You won 2 free tickets to the Super Bowl text us to claim your prize"""
text_4 = """Thanks for registering. Text 4343 to receive free updates on medicare"""

# Send the text messages to transform the data and predict the classification.
print(text_clf.predict([text_1]))
print(text_clf.predict([text_2]))
print(text_clf.predict([text_3]))
print(text_clf.predict([text_4]))



# Import pandas and numpy
import pandas as pd
import numpy as np
# Import the required dependencies from sklearn
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline
from sklearn import metrics

# Set the column width to view the text message data.
pd.set_option('max_colwidth', 200)

# Load the dataset into a DataFrame


# Check for missing values. 


#  Get the number of "ham" and "spam" from the "label" column:


# Set the features variable to the text message. 

# Set the target variable to the "label" column.


# Split data into training and testing and set the test_size = 33%


# Create an instance of the TfidfVectorizer


# Transform the data and use the original X_train set.


# What stopwords are in the scikit-learn's built-in list


# Create a list to hold the words using the vectorizer.get_feature_names_out()

# Create a list to hold the frequency using np.ravel(X.sum(axis=0))


# Create a DataFrame of the TFâ€“IDF weights for each word in the working corpus.



# Sort the DataFrame by word frequency in descending order and reset the index.

# Display the first 10 rows of the DataFrame


# Display the last 10 rows of the DataFrame


# Train the data on LinearSVC classifier.

# Fit the model to the transformed  data,


# Determine predictions.  


# Transform the testing data like we did with the training data.

# Make predictions 


# Validate the model by checking the model accuracy with model.score


# Build a pipeline to transform the test set to compare to the training set. 


# Fit the model to the transformed data.
 

# Validate the model by checking the model accuracy with model.score


# Form a prediction set


# Create the confusion matrix on the test data and predictions


# Print a classification report


# Print the overall accuracy



# Create some random text messages. 
text_1 = """<add your review here>"""
text_2 = """<add your review here>"""
text_3 = """<add your review here>"""
text_4 = """<add your review here>"""

# Send the text messages to transform the data and predict the classification.




# Import pandas and numpy
import pandas as pd
import numpy as np
# Import the required dependencies from sklearn
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn import metrics

# Set the column width to view the text message data.
pd.set_option('max_colwidth', 200)

# Load the movie review dataset.
imdb_reviews_df = pd.read_csv('Resources/imdb_reviews.csv')
# Display the first five rows of the dataset. 
imdb_reviews_df.head()

# Check for missing values. 
imdb_reviews_df.info()

# Get a sample of a review.
imdb_reviews_df["review"][2]

#  Get the number of "pos" and "neg" from the "label" column:
imdb_reviews_df['label'].value_counts()

# Set the features variable to the "review" column.
X = imdb_reviews_df['review']
# Set the target variable to the "label" column.
y = imdb_reviews_df['label']

# Split data into training and testing and use `test_size = 30%`.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

#  Build a pipeline using `TfidfVectorizer()`, without `stopwords='english`, and `LinearSVC()`.
text_clf = Pipeline([('tfidf', TfidfVectorizer()),('clf', LinearSVC()),])

# Fit the model to the transformed data.
text_clf.fit(X_train, y_train)  

# Validate the model by checking the model's training and testing accuracy.
print('Train Accuracy: %.3f' % text_clf.score(X_train, y_train))
print('Test Accuracy: %.3f' % text_clf.score(X_test, y_test))

# Retrieve the first 30 predictions from the model.
test_predictions = text_clf.predict(X_test)
print(test_predictions[:30])

# Create the confusion matrix on the test data and predictions
print(metrics.confusion_matrix(y_test,test_predictions))

# Print a classification report
print(metrics.classification_report(y_test,test_predictions))

# Print the overall accuracy
print(metrics.accuracy_score(y_test,test_predictions))

# Add a review of a movie.
barbie_review = """I was curious to see how they would evolve the "stereotypical Barbie" into something more. 
But the messaging in this movie was so heavy handed that it completely lost the plot. 
I consider myself a proponent of gender equality, and this ain't the way to get it."""

# Print the classification of the review.
print(text_clf.predict([barbie_review])) 

# Build a LinearSVC pipeline using`TfidfVectorizer()`, with `stopwords`, and `LinearSVC()`.
text_clf_2 = Pipeline([('tfidf', TfidfVectorizer(stop_words='english')),('clf', LinearSVC())])

# Fit the data to the model.
text_clf_2.fit(X_train, y_train)

# Validate the model by checking the model accuracy with model.score
print('Train Accuracy: %.3f' % text_clf_2.score(X_train, y_train))
print('Test Accuracy: %.3f' % text_clf_2.score(X_test, y_test))

# Retrieve the first 30 predictions from the model.
test_predictions_2 = text_clf_2.predict(X_test)
print(test_predictions_2[:30])

# Create the confusion matrix on the test data and predictions
print(metrics.confusion_matrix(y_test,test_predictions_2))

# Print a classification report
print(metrics.classification_report(y_test,test_predictions_2))

# Print the overall accuracy
print(metrics.accuracy_score(y_test,test_predictions_2))

# Print the classification of the review.
print(text_clf_2.predict([barbie_review]))  

# Create custom stopwords.
custom_stopwords = ['a', 'about', 'an', 'and', 'are', 'as', 'at', 'be', 'been', 'but', 'by', 'can', \
             'even', 'ever', 'for', 'from', 'get', 'had', 'has', 'have', 'he', 'her', 'hers', 'his', \
             'how', 'i', 'if', 'in', 'into', 'is', 'it', 'its', 'just', 'me', 'my', 'of', 'on', 'or', \
             'see', 'seen', 'she', 'so', 'than', 'that', 'the', 'their', 'there', 'they', 'this', \
             'to', 'was', 'we', 'were', 'what', 'when', 'which', 'who', 'will', 'with', 'you']

# Build a LinearSVC pipeline using`TfidfVectorizer()`, with custom_stopwords, and `LinearSVC()`.
text_clf_3 = Pipeline([('tfidf', TfidfVectorizer(stop_words=custom_stopwords)),('clf', LinearSVC())])

# Fit the data to the model.
text_clf_3.fit(X_train, y_train)

# Validate the model by checking the model accuracy with model.score
print('Train Accuracy: %.3f' % text_clf_3.score(X_train, y_train))
print('Test Accuracy: %.3f' % text_clf_3.score(X_test, y_test))

# Get predictions
test_predictions_3 = text_clf_3.predict(X_test)
print(test_predictions_3[:30])

# Create the confusion matrix on the test data and predictions
print(metrics.confusion_matrix(y_test,test_predictions_3))

# Print a classification report
print(metrics.classification_report(y_test,test_predictions_3))

# Print the overall accuracy
print(metrics.accuracy_score(y_test,test_predictions_3))

# Print the classification of the review.
print(text_clf_3.predict([barbie_review]))  



# Import pandas and numpy
import pandas as pd
import numpy as np
# Import the required dependencies from sklearn
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn import metrics

# Set the column width to view the text message data.
pd.set_option('max_colwidth', 200)

# Load the movie review dataset.

# Display the first five rows of the dataset. 


# Check for missing values. 


# Get a sample of a review.


#  Get the number of "pos" and "neg" from the "label" column:


# Set the features variable to the "review" column.

# Set the target variable to the "label" column.


# Split data into training and testing and use `test_size = 30%`.


#  Build a pipeline using`TfidfVectorizer()`, without `stopwords`, and `LinearSVC()`.


# Fit the model to the transformed data.
 

# Validate the model by checking the model's training and testing accuracy.


# Retrieve the first 30 predictions from the model.


# Create the confusion matrix on the test data and predictions


# Print a classification report


# Print the overall accuracy


# Add a review of a movie of at least 3 sentences or more. 
movie_review = """<add your review here>"""

# Print the classification of the review.
 

# Build a LinearSVC pipeline using`TfidfVectorizer()`, with `stopwords`, and `LinearSVC()`.


# Fit the data to the model.


# Validate the model by checking the model accuracy with model.score


# Retrieve the first 30 predictions from the model.


# Create the confusion matrix on the test data and predictions


# Print a classification report


# Print the overall accuracy


# Print the classification of the review.


# Create custom stopwords.
custom_stopwords = ['a', 'about', 'an', 'and', 'are', 'as', 'at', 'be', 'been', 'but', 'by', 'can', \
             'even', 'ever', 'for', 'from', 'get', 'had', 'has', 'have', 'he', 'her', 'hers', 'his', \
             'how', 'i', 'if', 'in', 'into', 'is', 'it', 'its', 'just', 'me', 'my', 'of', 'on', 'or', \
             'see', 'seen', 'she', 'so', 'than', 'that', 'the', 'their', 'there', 'they', 'this', \
             'to', 'was', 'we', 'were', 'what', 'when', 'which', 'who', 'will', 'with', 'you']

# # Build a LinearSVC pipeline using`TfidfVectorizer()`, with custom_stopwords, and `LinearSVC()`.


# Fit the data to the model.


# Validate the model by checking the model accuracy with model.score



# Get predictions


# Create the confusion matrix on the test data and predictions


# Print a classification report


# Print the overall accuracy


# Print the classification of the review.




# Import spacy library
import spacy
# Load the small English language model for spacy
nlp = spacy.load("en_core_web_sm")

# Provide a sentence to be analyzed using spacy
sentence = "The brown cow jumped over the round moon."

# Tokenize text and parse each token
tokens = nlp(sentence)

# Print POS-Tags for each token
[(token.text+ " ---> " +token.pos_) for token in tokens]

# Retrieve all the nouns in the sentence using a list comprehension
nouns = [token.text for token in tokens if token.pos_ == "NOUN"]

# Print the nouns in the sentence
print(nouns)

# Print grammar dependencies for each word. 
[(token.text + " ---> " + token.dep_) for token in tokens]

# Import the displacy module from spacy
from spacy import displacy

# Show the dependency tree
displacy.render(tokens, style="dep", options={'distance': 125})

# Change the style and color of the relationships of words. 
options = {'distance': 125,
           'compact': 'True',
           'color': 'yellow',
           'bg': 'navy',
           'font': 'Arial'}

# Show the dependency tree
displacy.render(tokens, style="dep", options=options)

# Host the image on a webpage.
# displacy.serve(tokens, style="dep", options=options, port=5050)

# Print the POS-tag and head word of each token
[token.text + " ---> " + token.pos_ + " ---> " + token.head.text for token in tokens]

# Retrieve the adjectives that describe the word "cow"
cow_describers = [token.text for token in tokens if (token.head.text == "cow" and token.pos_ == "ADJ")]

# Print describers
print(cow_describers)



# Import spacy library
import spacy
# Load the small English language model for spacy
nlp = spacy.load("en_core_web_sm")

# Provide a sentence to be analyzed using spacy
sentence = "The brown cow jumped over the round moon."

# Tokenize text and parse each token


# Print POS-Tags for each token


# Retrieve all the nouns in the sentence using a list comprehension


# Print the nouns in the sentence


# Print grammar dependencies for each word. 


# Import the displacy module from spacy
from spacy import displacy

# Show the dependency tree
displacy.render(tokens, style="dep", options={'distance': 125})

# Change the style and color of the relationships of words. 
options = {'distance': 125,
           'compact': 'True',
           'color': 'yellow',
           'bg': 'navy',
           'font': 'Arial'}

# Show the dependency tree
displacy.render(tokens, style="dep", options=options)

# Host the image on a webpage.
# displacy.serve(tokens, style="dep", options=options, port=5050)

# Print the POS-tag and head word of each token


# Retrieve the adjectives that describe the word "cow"


# Print describers
print(cow_describers)



# Import dependencies
import nltk
import spacy
import pandas as pd
from collections import Counter
from nltk.corpus import inaugural

# Download NLTK's inaugural corpus
nltk.download("inaugural")

# Load the English language model for spaCy
nlp = spacy.load("en_core_web_sm")

# Retrieve the IDs of inaugural addresses
ids = inaugural.fileids()

# Retrieve the text of all the inaugural addresses
texts = [inaugural.raw(id) for id in ids]

# Get the length of the ids and text
print(len(ids), len(texts))

# Display sample inaugural address
print(ids[10])
print(texts[10])

# Use the most_common_adjs function to tokenize the text, creates a list of with all the adjectives, 
# and retrieve the most common adjectives and their frequency. 
def most_common_adjs(text):
    """
    Finds and returns the most common adjective in the given text.

    Args:
        text (str): The input text from which adjectives will be extracted.

    Returns:
        tuple: A tuple containing the most common adjectives and their frequency.
               The tuple has the format (adjective, frequency).

    Example:
    >>> text = "The quick brown fox jumps over the lazy dog. The fast fox is brown."
    >>> most_common_adj(text)
    ('brown', 2)
    """
    # Tokenizes text and parse each token
    doc = nlp(text)
    
    # Creates a list with all the adjectives in the text
    adjs = [token.text.lower() for token in doc if token.pos_ == 'ADJ']
    
    # Retrieves the most frequent adjective in the adjectives list using the Counter module
    most_common_adj = Counter(adjs).most_common(1)[0]
    
    return most_common_adj


# Create a list of the most common adjective for each inaugural address
common_adjs = [most_common_adjs(text) for text in texts]

# Print the common adjectives.
print(common_adjs)

# Use list comprehensions to retrieve each adjective and the number of occurrences for each text in separate lists.
adjs = [common_adjs[i][0] for i, _ in enumerate(common_adjs)]
frequency = [common_adjs[i][1] for i, _ in enumerate(common_adjs)]

# Save the year and president as 'inaugural_address' in the following format "1789-Washington"
inaugural_address = [id.replace(".txt", "") for id in ids]

# Create a DataFrame called, adjs_df, that has columns to hold the
# inaugural addresses, the common adjective, and the number of times each adjective appears.
common_adjs_df = pd.DataFrame(
    {
        'inaugural address':inaugural_address,
        'adjective':adjs,
        'frequency':frequency
    }
)

# Sort the DataFrame to display the top 10 adjectives.
presidential_adjs = common_adjs_df.sort_values(by=['frequency'], ascending=False).reset_index(drop=True)

# Display the first ten rows. 
presidential_adjs.head(10)

# Use the all_adjs function to retrieve all the adjectives in a given text.
def all_adjs(text):
    """
    This function retrieves all the adjectives in the given text.
    
    Args:
        text (string): The text to analyze.
        
    Returns:
        list: A list containing all the adjectives found in the text. Adjectives
              are represented as lowercase strings.
    Example:
    >>> text = "The quick brown fox jumps over the lazy dog. The fast fox is brown."
    >>> all_adjs(text)
    ['quick', 'brown', 'lazy', 'fast', 'brown']
    """
    
    # Tokenize the text and parse each token
    doc = nlp(text)
    
    # Create a list with all the adjectives in the text
    adjs = [token.text.lower() for token in doc if token.pos_ == 'ADJ']
    
    return adjs

# Create an empty list to store all the adjectives
all_adjectives = []

# Use a for loop that sends the "text" of the inaugural addresses to the all_adj() function 
# and concatenates the returned adjectives to the all_adjectives list.
for text in texts:
    all_adjectives = all_adjectives + all_adjs(text)
    
# Print sample data
all_adjectives[:10]

# Create a variable, most_freq_adjectives, that stores the three most frequent adjectives 
# used in the inaugural addresses by using the most_common() function from the Counter module.
most_freq_adjectives = Counter(all_adjectives).most_common(3)

# Print the three most frequent adjectives
print(most_freq_adjectives)

# Import the word_tokenize module from NLTK
from nltk.tokenize import word_tokenize
# Use the get_word_counts function to count the occurrences of a word in text.
def get_word_counts(text, word):
    """
    This function counts the occurrences of a word in a text.
    
    Args:
        text (string): The text where word counts will be analyzed.
        word (string): The word to look into the text.
        
    Returns:
        word_count (int): The counts of the word in the given text.
        
    Example:
    >>> text = "The quick brown fox jumps over the lazy dog. The quick brown dog is happy."
    >>> word = "quick"
    >>> get_word_counts(text, word)
    2
    """
    
    # Use the word_tokenize module from NLTK to tokenize the text
    tok = word_tokenize(text)
    
    # Create a list with all the tokens retrieved from the text
    tok = [word.lower() for word in tok]
    
    # Count the occurrences of the word in the text
    word_count = tok.count(word)
    
    return word_count

# Use list comprehensions to create a list with the counts of each top adjective in the inaugural addresses
great_counts = [get_word_counts(text,'great') for text in texts]
other_counts = [get_word_counts(text,'other') for text in texts]
own_counts = [get_word_counts(text,'own') for text in texts]

# Display sample data
print(f"Great counts sample data: {great_counts[:5]}")
print(f"Other counts sample data: {other_counts[:5]}")
print(f"Own counts sample data: {own_counts[:5]}")

# Create a Python list dates to store the year when every inaugural address was delivered. 
dates = [id.split('-')[0] for id in ids]

# Print sample data
print(dates[:5])

# Create a Python list called, presidents,  to store the last name of each U.S. President from each inaugural address.
presidents = [id.split('-')[1].split('.')[0] for id in ids]

# Print sample data
print(presidents[:5])

# Create a DataFrame presidential_adjs_df, that contains columns that hold the President's last name 
# and the number of times each adjective appears in the Presidents' inaugural address.
presidential_adjs_df = {
    'President': presidents,
    'great':great_counts,
    'other':other_counts,
    'own': own_counts
}

# Set the index of the presidential_adjs_df DataFrame equal to the year in the dates list.
presidential_adjs_df = pd.DataFrame(presidential_adjs_df, index=pd.to_datetime(dates).year)

# Display same data
presidential_adjs_df.head()

# Create a bar plot that displays the most common adjectives used throughout the U.S. presidential inaugural addresses.
presidential_adjs_df.plot.bar(
    title = "Most Common Adjectives Used in the U.S. Presidential Inaugural Addresses",
    figsize = (15, 5))

def describe_america(text):
    """
    This function retrieves the adjectives in the text that describe the word 'America'.
    
    Args:
        text (string): The text to analyze.
        
    Returns:
        adjs (list): A list of the adjectives that describe the word "America" in the text.
    """
    
    # Use the spaCy English language model to tokenize the text and parse each token.
    doc = nlp(text)
    
    # Create a list with all the adjectives in the text of each inaugural address that describe the word "America".
    adjs = [token.text.lower() for token in doc if (token.pos_ == 'ADJ' and token.head.text == 'America')]
    
    return adjs

# Create an empty list to store the adjectives
america_adjectives = []

# Write a for loop that sends the "text" of the inaugural addresses to the describe_america() function 
# and concatenates the returned adjectives to the america_adjectives list.  
for text in texts:
    america_adjectives = america_adjectives + describe_america(text)
    
# Print the list of the adjectives describing the word 'America'
america_adjectives



# Import dependencies
import nltk
import spacy
import pandas as pd
from collections import Counter
from nltk.corpus import inaugural

# Download NLTK's inaugural corpus
nltk.download("inaugural")

# Load the English language model for spaCy
nlp = spacy.load("en_core_web_sm")

# Retrieve the IDs of inaugural addresses


# Retrieve the text of all the inaugural addresses


# Get the length of the ids and text
print(len(ids), len(texts))

# Display sample inaugural address
print(ids[10])
print(texts[10])

# Use the most_common_adjs function to tokenize the text, creates a list of with all the adjectives, 
# and retrieve the most common adjectives and their frequency. 
def most_common_adjs(text):
    """
    Finds and returns the most common adjective in the given text.

    Args:
        text (str): The input text from which adjectives will be extracted.

    Returns:
        tuple: A tuple containing the most common adjectives and their frequency.
               The tuple has the format (adjective, frequency).

    Example:
    >>> text = "The quick brown fox jumps over the lazy dog. The fast fox is brown."
    >>> most_common_adj(text)
    ('brown', 2)
    """
    # Tokenizes text and parse each token
    doc = nlp(text)
    
    # Creates a list with all the adjectives in the text
    adjs = [token.text.lower() for token in doc if token.pos_ == 'ADJ']
    
    # Retrieves the most frequent adjective in the adjectives list using the Counter module
    most_common_adj = Counter(adjs).most_common(1)[0]
    
    return most_common_adj


# Create a list of the most common adjective for each inaugural address


# Print the common adjectives.
print(common_adjs)

# Use list comprehensions to retrieve each adjective and the number of occurrences for each text in separate lists.


# Save the year and president as 'inaugural_address' in the following format "1789-Washington"


# Create a DataFrame called, adjs_df, that has columns to hold the
# inaugural addresses, the common adjective, and the number of times each adjective appears.


# Sort the DataFrame.


# Display the first ten rows. 


# Use the all_adjs function to retrieve all the adjectives in a given text.
def all_adjs(text):
    """
    This function retrieves all the adjectives in the given text.
    
    Args:
        text (string): The text to analyze.
        
    Returns:
        list: A list containing all the adjectives found in the text. Adjectives
              are represented as lowercase strings.
    Example:
    >>> text = "The quick brown fox jumps over the lazy dog. The fast fox is brown."
    >>> all_adjs(text)
    ['quick', 'brown', 'lazy', 'fast', 'brown']
    """
    
    # Tokenize the text and parse each token
    doc = nlp(text)
    
    # Create a list with all the adjectives in the text
    adjs = [token.text.lower() for token in doc if token.pos_ == 'ADJ']
    
    return adjs

# Create an empty list to store all the adjectives
all_adjectives = []

# Use a for loop that sends the "text" of the inaugural addresses to the all_adj() function 
# and concatenates the returned adjectives to the all_adjectives list.

    
# Print sample data
all_adjectives[:10]

# Create a variable, most_freq_adjectives, that stores the three most frequent adjectives 
# used in the inaugural addresses by using the most_common() function from the Counter module.


# Print the three most frequent adjectives
print(most_freq_adjectives)

# Import the word_tokenize module from NLTK
from nltk.tokenize import word_tokenize
# Use the get_word_counts function to count the occurrences of a word in text.
def get_word_counts(text, word):
    """
    This function counts the occurrences of a word in a text.
    
    Args:
        text (string): The text where word counts will be analyzed.
        word (string): The word to look into the text.
        
    Returns:
        word_count (int): The counts of the word in the given text.
        
    Example:
    >>> text = "The quick brown fox jumps over the lazy dog. The quick brown dog is happy."
    >>> word = "quick"
    >>> get_word_counts(text, word)
    2
    """
    
    # Use the word_tokenize module from NLTK to tokenize the text
    tok = word_tokenize(text)
    
    # Create a list with all the tokens retrieved from the text
    tok = [word.lower() for word in tok]
    
    # Count the occurrences of the word in the text
    word_count = tok.count(word)
    
    return word_count

# Use list comprehensions to create a list with the counts of each top adjective in the inaugural addresses


# Display sample data
print(f"Great counts sample data: {great_counts[:5]}")
print(f"Other counts sample data: {other_counts[:5]}")
print(f"Own counts sample data: {own_counts[:5]}")

# Create a Python list dates to store the year when every inaugural address was delivered. 


# Print sample data
print(dates[:5])

# Create a Python list called, presidents,  to store the last name of each U.S. President from each inaugural address.


# Print sample data
print(presidents[:5])

# Create a DataFrame presidential_adjs_df, that contains columns that hold the President's last name 
# and the number of times each adjective appears in the Presidents' inaugural address.


# Set the index of the presidential_adjs_df DataFrame equal to the year in the dates list.


# Display same data


# Create a bar plot that displays the most common adjectives used throughout the U.S. presidential inaugural addresses.
# df.plot.bar(title="Most Common Adjectives Used in the U.S. Presidential Inaugural Addresses",figsize=(15, 5))

def describe_america(text):
    """
    This function retrieves the adjectives in the text that describe the word 'America'.
    
    Args:
        text (string): The text to analyze.
        
    Returns:
        adjs (list): A list of the adjectives that describe the word "America" in the text.
    """
    
    # Use the spaCy English language model to tokenize the text and parse each token.
    doc = nlp(text)
    
    # Create a list with all the adjectives in the text of each inaugural address that describe the word "America".
    adjs = [token.text.lower() for token in doc if (token.pos_ == 'ADJ' and token.head.text == 'America')]
    
    return adjs

# Create an empty list to store the adjectives
america_adjectives = []

# Write a for loop that sends the "text" of the inaugural addresses to the describe_america() function 
# and concatenates the returned adjectives to the america_adjectives list.  

    
# Print the list of the adjectives describing the word 'America'
america_adjectives



# Import the dependencies
import spacy
from nltk.corpus import reuters
from spacy import displacy
from collections import Counter
import pandas as pd

# Load the small English language model for spacy
nlp = spacy.load("en_core_web_sm")

# Analyze a sentence using spacy
doc = nlp(u"""Patrick Mahomes is a quarterback for the Kansas City Chiefs in the American Conference, 
which is one of two conferences in the National Football League.""")

# Access the tagged entities with .text and .label_
[ent.text +" ---> "+ ent.label_ for ent in doc.ents]

# Get all the categories in the Reuters corpus. 
categories = reuters.categories()
print(categories)

# Locate and store a single article from the Reuters stories with the category "coffee".
article = reuters.raw(fileids = reuters.fileids(categories='coffee')[3])
print(article)

# Analyze the article with spacy
doc = nlp(article)

# Render NER visualization with displacy to determine entities for extraction
displacy.render(doc, style='ent')

# Store all Reuters articles with category "coffee".
articles = reuters.raw(categories='coffee')

# Analyze the articles with spaCy
doc = nlp(articles)

# Extract geopolitical "GPE" and organizational entities "ORG" using a list comprehension.
geo_org_entities = [ent.text for ent in doc.ents if ent.label_ in ['GPE', 'ORG']]

# Print the first 20 entities.
geo_org_entities[:20]

# Using a list comprehension convert each entity to lowercase and remove the newline character. 
entities = [i.lower().replace('\n','') for i in geo_org_entities]

# Print the entities
print(entities)

# Create a variable, most_freq_entities, that stores the most frequent entities 
# using the most_common() function from the Counter module.
most_freq_entities = Counter(entities).most_common()

# Print the first 10 most frequent entities
print(most_freq_entities[:10])

# Use list comprehensions to retrieve each entity and the number of occurrences for each entity in separate lists.
entity = [most_freq_entities[i][0] for i, _ in enumerate(most_freq_entities)]
frequency = [most_freq_entities[i][1] for i, _ in enumerate(most_freq_entities)]

# Create a DataFrame that has columns to hold each entity and the number of times each entity appears.
common_entities_df = pd.DataFrame(
    {
        'entity':entity,
        'frequency':frequency
    }
)

# Sort the DataFrame
common_entities_df.sort_values(by=['frequency'], ascending=False).reset_index(drop=True)

# Display the first ten rows. 
common_entities_df.head(10)

# Display the last ten rows. 
common_entities_df.tail(10)



# Import the dependencies
import spacy
from nltk.corpus import reuters
from spacy import displacy
from collections import Counter
import pandas as pd

# Load the small English language model for spacy
nlp = spacy.load("en_core_web_sm")

# Analyze a sentence using spacy
doc = nlp(u"""Patrick Mahomes is a quarterback for the Kansas City Chiefs in the American Conference, 
which is one of two conferences in the National Football League.""")

# Access the tagged entities with .text and .label_


# Get all the categories in the Reuters corpus. 


# Locate and store a single article from the Reuters stories with the category "coffee".


# Analyze the article with spacy


# Render NER visualization with displacy to determine entities for extraction


# Store all reuters articles with category "coffee".


# Set articles to be analyzed with spacy


# Extract geopolitical "GPE" and organizational entities "ORG" using a list comprehension.

# Print the first 20 entities.


# Using a list comprehension convert each entity to lowercase and remove the newline character. 


# Print the entities


# Create a variable, most_freq_entities, that stores the most frequent entities 
# using the most_common() function from the Counter module.


# Print the first 10 most frequent entities


# Use list comprehensions to retrieve each entity and the number of occurrences for each entity in separate lists.


# Create a DataFrame called, adjs_df, that has columns to hold the
# inaugural addresses, the common adjective, and the number of times each adjective appears.


# Sort the DataFrame


# Display the first ten rows. 


# Display the last ten rows. 




# Import dependencies 
import pandas as pd
import numpy as np
import random
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
# Set the column width
pd.set_option('max_colwidth', 200)

# Load the news_articles.csv into a DataFrame.
news_articles_df = pd.read_csv('Resources/news_articles.csv')
# Display the first 20 headlines 
news_articles_df.head(10)

# Get the info on the DataFrame
news_articles_df.info()

# Remove digits and non-alphabetic characters
news_articles_df['headline'] = news_articles_df['headline'].apply(lambda x: re.sub(r'[^a-zA-Z\s ]', '', str(x)))
news_articles_df.head(10)

# Create an instance of the CountVectorizer and set the max_df to 0.95 and min_df to 10, and use the "english" stopwords.
cv = CountVectorizer(max_df=0.95,min_df=10, stop_words='english')
cv

# Get the headlines.
headlines= news_articles_df['headline']
print(headlines)

# Transform each row from the headlines Series to a DTM.
dtm = cv.fit_transform(headlines)
# Get the shape of the DTM.
print(dtm.shape)

# Get the length of the vocabulary 
len(cv.get_feature_names_out())

# Look at 100 random words in the vocabulary
print(cv.get_feature_names_out()[:100])

# Print the first 500 elements (transformed words)from the 1st row, i.e., document. 
print(dtm.toarray()[0][:500])

# Get the feature names (words) from the CountVectorizer
feature_names = cv.get_feature_names_out()

# Get all the non-zero elements from the first row.
non_zero_elements = dtm.toarray()[0]

# Get the indices for each non-zero element.
non_zero_indices = non_zero_elements.nonzero()[0]

# Print out the word and the number of times the word is in the row. 
for idx in non_zero_indices:
    print(f"Word: {feature_names[idx]} | Word index {idx} | Count = {non_zero_elements[idx]}")

# Convert the DTM to a DataFrame
dtm_df = pd.DataFrame(dtm.toarray(), columns=cv.get_feature_names_out())

# Display some random columns and the first 20 rows of the DataFrame.
dtm_df.iloc[:,180:195:].head(10)

# Pick 7 topics to start with `n_components=7`
LDA = LatentDirichletAllocation(n_components=7,random_state=42)
# Fit the model with our DTM data. This may take awhile if you have a large amount of documents.
LDA_data = LDA.fit(dtm)

# Get the values of each topic-word distribution.
topic_word_distributions = LDA.components_
print(topic_word_distributions)

# Get the length of the array of each topic. It should be the same as the vocabulary.
for index,topic in enumerate(LDA.components_):
    print(len(LDA.components_[index]))

# Get the array of the first topic 
first_topic = LDA.components_[0]
# This is the ranking of each word in the array. Lower values have less impact than higher values.
print(first_topic)

# Get the indices for the first topic in descending order.
sorted_first_topic_indices = np.argsort(-first_topic)

# Use the sorted indices to the values from greatest to least.
sorted_first_topic_values = first_topic[sorted_first_topic_indices]
for value in sorted_first_topic_values:
    print(value)

# Define an array of values index 0 = 10, index 1 = 200, index 2 = 1.
arr = np.array([10, 200, 1])
# Print out the indices after sorting the array from least to greatest, i.e., 1, 10, 200:
print(f"The indices the the array, '10, 200, 1' from least to greatest: {np.argsort(arr)}")
# Reverse the sort from greatest to least. 
print(f"The indices the the array, '10, 200, 1' from greatest to least: {np.argsort(-arr)}")

# Sort the array of the first topic
first_topic.argsort()

# Get the value of the word that is least representative of this topic
print(f"The value of the word that is least representative of this topic is: {first_topic[1716]}")
# Get the value of the word that is most representative of this topic
print(f"The value of the word that is most representative of this topic is: {first_topic[1688]}")

# Get the indices of the top ten words for the first topic (e.g., top 10 words for topic 0):
top_word_indices = first_topic.argsort()[-10:][::-1]
print(top_word_indices)

# Get the top ten words from the indices. 
for index in top_word_indices:
    print(cv.get_feature_names_out()[index])

# Get the bottom ten words from the indices.
bottom_word_indices = first_topic.argsort()[:10][::-1]
for index in bottom_word_indices:
    print(cv.get_feature_names_out()[index])

# Print the top 20 words for each topic
for index,topic in enumerate(LDA.components_):
    print(f"The Top 20 Words For Topic #{index+1}")
    print([cv.get_feature_names_out()[i] for i in topic.argsort()[-20:][::-1]])
    print('\n')

# Transform our DTM so we get an array with the (number_of_documents, number_of_topics).
topic_results = LDA.transform(dtm)

# Get the shape of the topic results
topic_results.shape

# Get the first headline's topic probability distribution rounded to 6 decimal places. 
print(topic_results[0].round(6))

# Get the sorted indices for each topic in the first headline.
sorted_indices = np.argsort(-topic_results[0])
# Print the ranking of topics for the headline
print("Ranking of topics for the first headline:")
for rank, topic_index in enumerate(sorted_indices):
    print(f"   Rank {rank+1}: Topic {topic_index+1}, Probability: {topic_results[0, topic_index]:.6f}")

# Get the topic with the highest probability. 
topic_results[0].argmax()+1

# Read in our original news headlines. 
news_articles_df_2 = pd.read_csv('Resources/news_articles.csv')
# Display the first 20 headlines 
news_articles_df_2.head(20)
# Combine the original data with the topic label. 
news_articles_df_2['topic'] = (topic_results.argmax(axis=1)+1)

# Get the first 20 rows. 
news_articles_df_2.head(10)

news_articles_df_2.tail(10)



# Import dependencies 
import pandas as pd
import numpy as np
import random
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
# Set the column width
pd.set_option('max_colwidth', 200)

# Load the news_articles.csv into a DataFrame.
news_articles_df = pd.read_csv('Resources/news_articles.csv')
# Display the first 20 headlines 
news_articles_df.head(10)

# Get the info on the DataFrame
news_articles_df.info()

# Remove digits and non-alphabetic characters


# Create an instance of the CountVectorizer and set the max_df to 0.95 and min_df to 10, and use the "english" stopwords.
cv = CountVectorizer(max_df=0.95,min_df=10, stop_words='english')
cv

# Get the headlines.


# Transform each row from the headlines Series to a DTM.

# Get the shape of the DTM.
print(dtm.shape)

# Get the length of the vocabulary 
len(cv.get_feature_names_out())

# Look at 100 random words in the vocabulary
print(cv.get_feature_names_out()[:100])

# Print the first 500 elements (transformed words)from the 1st row, i.e., document. 
print(dtm.toarray()[0][:500])

# Get the feature names (words) from the CountVectorizer


# Get all the non-zero elements from the first row.


# Get the indices for each non-zero element.


# Print out the word and the number of times the word is in the row. 


# Convert the DTM to a DataFrame


# Display some random columns and the first 20 rows of the DataFrame.


# Create and instance of the LatentDirichletAllocation() class with 5 topics.

# Fit the model with our DTM data. This may take awhile if you have a large amount of documents.
LDA_data = LDA.fit(dtm)

# Get the values of each topic-word distribution.


# Get the length of the array of each topic. It should be the same as the vocabulary.


# Get the array of the first topic 

# This is the ranking of each word in the array. Lower values have less impact than higher values.


# Get the indices for the first topic in descending order.


# Use the sorted indices to the values from greatest to least.


# Define an array of values index 0 = 10, index 1 = 200, index 2 = 1.
arr = np.array([10, 200, 1])
# Print out the indices after sorting the array from least to greatest, i.e., 1, 10, 200:
print(f"The indices the the array, '10, 200, 1' from least to greatest: {np.argsort(arr)}")
# Reverse the sort from greatest to least. 
print(f"The indices the the array, '10, 200, 1' from greatest to least: {np.argsort(-arr)}")

# Sort the array of the first topic


# Get the value of the word that is least representative of this topic
print(f"The value of the word that is least representative of this topic is: {first_topic[1716]}")
# Get the value of the word that is most representative of this topic
print(f"The value of the word that is most representative of this topic is: {first_topic[1688]}")

# Get the indices of the top ten words for the first topic (e.g., top 10 words for topic 0):


# Get the top ten words from the indices. 


# Get the bottom ten words from the indices.


# Print the top 20 words for each topic


# Transform our DTM so we get an array with the (number_of_documents, number_of_topics).
topic_results = LDA.transform(dtm)

# Get the shape of the topic results
topic_results.shape

# Get the first headline's topic probability distribution rounded to 6 decimal places. 


# Get the sorted indices for each topic in the first headline.

# Print the ranking of topics for the headline


# Get the topic with the highest probability. 


# Read in our original news headlines. 

# Combine the original data with the topic label. 
news_articles_df_2['topic'] = (topic_results.argmax(axis=1)+1)

# Get the first 20 rows. 


# Get the last 20 rows.




# Import the dependencies
import re 
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
# Set the maximum column width to 200. 
pd.set_option('max_colwidth', 200)

# Read the bbc_news_articles.csv file into a DataFrame.
news_articles_df = pd.read_csv('Resources/bbc_news_articles.csv')
# Display the first 20 rows. 
news_articles_df.head(20)

# Check for null values.
news_articles_df.info()

# Remove numbers and non-alphabetic characters from the news_summary column.
news_articles_df['news_summary'] = news_articles_df['news_summary'].apply(lambda x: re.sub(r'[^a-zA-Z\s ]', '', str(x)))
news_articles_df.head(10)

# Create an instance of the CountVectorizer and set the max_df to 0.95 and min_df to 5, and use the "english" stopwords.
cv = CountVectorizer(max_df=0.95, min_df=5, stop_words='english')
cv

# Transform each row from the news_summary to a DTM.
dtm = cv.fit_transform(news_articles_df['news_summary'])
# Get the shape of the DTM.
print(dtm.shape)

# Create and instance of the LatentDirichletAllocation() class with 5 topics.
LDA = LatentDirichletAllocation(n_components=5,random_state=42)
# Fit the model with our DTM data.
LDA_data = LDA.fit(dtm)

# Check the length of the vocabulary 
len(cv.get_feature_names_out())

# Print the top 15 words for each topic.
for index,topic in enumerate(LDA.components_):
    print(f'The Top 15 Words For Topic #{index+1}')
    print([cv.get_feature_names_out()[i] for i in topic.argsort()[-15:]])
    print('\n')

# Transform our DTM so we get an array with the (number_of_documents, number_of_topics).
topic_results = LDA.transform(dtm)

# Get the shape of the topic results
topic_results.shape

# Read the bbc_news_articles.csv file into a DataFrame.
news_articles_df = pd.read_csv('Resources/bbc_news_articles.csv')
# Display the DataFrame. 
news_articles_df.head()

# Use the add_topic_labels function to add the topic and topic label to each news summary. 
# Dictionary of  topics and topic label.
topic_labels = {
    1: 'Entertainment',
    2: 'Sports',
    3: 'Business',
    4: 'Politics',
    5: 'Technology'
}

# Define the function and pass in the DataFrame, the topic_results, and topic_labels dictionary.
def add_topic_labels(df, topic_results, topic_labels):
    # Find the dominant topic for each document and add the label to a new column
    df['topic'] = topic_results.argmax(axis=1) + 1
    # Use the map function to add the topic label to the news summary based on the topic number.
    df['topic_label'] = df['topic'].map(topic_labels)


# Call the function to add topic labels to your DataFrame.
add_topic_labels(news_articles_df, topic_results, topic_labels)

# Display the first 20 rows of the updated DataFrame. 
news_articles_df.head(20)

# Display the last 20 rows of the updated DataFrame.
news_articles_df.tail(10)



# Import the dependencies
import re 
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
# Set the maximum column width to 200. 
pd.set_option('max_colwidth', 200)

# Read the bbc_news_articles.csv file into a DataFrame.

# Display the first 20 rows. 


# Check for null values.


# Remove digits and non-alphabetic characters from the news_summary column.


# Create an instance of the CountVectorizer and set the max_df to 0.95 and min_df to 5, and use the "english" stopwords.


# Transform each row from the news_summary to a DTM.

# Get the shape of the DTM.


# Create and instance of the LatentDirichletAllocation() class with 5 topics.

# Fit the model with our DTM data.


# Check the length of the vocabulary 


# Print the top 15 words for each topic.


# Transform our DTM so we get an array with the (number_of_documents, number_of_topics).


# Get the shape of the topic results


# Read the bbc_news_articles.csv file into a DataFrame.

# Display the first 20 rows. 


# Use the add_topic_labels function to add the topic and topic label to each news summary. 
# Dictionary of  topics and topic label.
topic_labels = {
    1: '',
    2: '',
    3: '',
    4: '',
    5: ''
}

# Define the function and pass in the DataFrame, the topic_results, and topic_labels dictionary.
def add_topic_labels(df, topic_results, topic_labels):
    # Find the dominant topic for each document and add the label to a new column
    
    # Use the map function to add the topic label to the news summary based on the topic number.
    


# Call the function to add topic labels to your DataFrame.
add_topic_labels(news_articles_df, topic_results, topic_labels)

# Display the first 20 rows of the updated DataFrame. 


# Display the last 20 rows of the updated DataFrame. 




# Import the dependencies 
import pandas as pd
import re
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF

# Set the column width to 200.
pd.set_option('max_colwidth', 200)

# Load the news_articles.csv into a DataFrame.
news_articles_df = pd.read_csv('Resources/news_articles.csv')
# Display the first 20 headlines 
news_articles_df.head(10)

# Remove digits and non-alphabetic characters
news_articles_df['headline'] = news_articles_df['headline'].apply(lambda x: re.sub(r'[^a-zA-Z\s ]', '', str(x)))
news_articles_df.head(10)

# Create an instance of the TfidfVectorizer and set the max_df to 0.95 and min_df to 10, and use the English stopwords to be ignored.
tfidf = TfidfVectorizer(max_df=0.95, min_df=10, stop_words='english')
tfidf

# Transform each row from the headlines Series to a DTM.
dtm = tfidf.fit_transform(news_articles_df["headline"])
# Get the shape of the DTM.
print(dtm.shape)

# Print the sparse matrix of the transformed data.
# We have 23,377 documents, the first number in the tuple represents the document number.
# The second number in the tuple represents the index of the word in the vocabulary created by fit_transform.
# The last number represents the value of the TF-IDF score for the vocabulary word.
print(dtm)

# Get the feature names (words) from the TfidfVectorizer
feature_names = tfidf.get_feature_names_out()

# Get all the non-zero elements from the first row.
non_zero_elements = dtm.toarray()[0]

# Get the indices for each non-zero element.
non_zero_indices = non_zero_elements.nonzero()[0]

# Print out the word and the number of times the word is in the row. 
for idx in non_zero_indices:
    print(f"Word: {feature_names[idx]} | Word index {idx} | Value = {non_zero_elements[idx]}")

# Initialize the NMF and set the number of topics to 7. 
nmf_model = NMF(n_components=7,random_state=42)
# Fit the model with our DTM data. 
nmf_model.fit(dtm)

# Get the length of the array of each topic. It should be the same as the vocabulary.
for index,topic in enumerate(nmf_model.components_):
    print(len(nmf_model.components_[index]))

# Get the array of the first topic 
first_topic = nmf_model.components_[0]
# This is the ranking of each word in the array. Lower values have less impact than higher values.
print(first_topic)

# Get the indices of the top ten words for the first topic (e.g., top 10 words for topic 0):
top_word_indices = first_topic.argsort()[-10:][::-1]
print(top_word_indices)

# Get the top ten words from the indices. 
for index in top_word_indices:
    print(tfidf.get_feature_names_out()[index])

# Print the top 20 words for each topic
for index,topic in enumerate(nmf_model.components_):
    print(f'The top 30 words for topic #{index+1}')
    print([tfidf.get_feature_names_out()[i] for i in topic.argsort()[-30:]])
    print('\n')

# Transform our DTM so we get an array with the (number_of_documents, number_of_topics).
topic_results = nmf_model.transform(dtm)

# Get the shape of the topic results
topic_results.shape

# Get the sorted indices for each topic in the first headline.
sorted_indices = np.argsort(-topic_results[0])
# Print the ranking of topics for the headline
print("Ranking of topics for the first headline:")
for rank, topic_index in enumerate(sorted_indices):
    print(f"   Rank {rank+1}: Topic {topic_index+1}, Probability: {topic_results[0, topic_index]:.6f}")

# Read in our original news headlines. 
news_articles_df_2 = pd.read_csv('Resources/news_articles.csv')
# Combine the original data with the topic label. 
news_articles_df_2['topic'] = (topic_results.argmax(axis=1)+1)

# Get the first 10 rows. 
news_articles_df_2.head(10)

# Get the last 10 rows. 
news_articles_df_2.tail(10)



# Import the dependencies 
import pandas as pd
import re
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF

# Set the column width to 200.
pd.set_option('max_colwidth', 200)

# Load the news_articles.csv into a DataFrame.
news_articles_df = pd.read_csv('Resources/news_articles.csv')
# Display the first 20 headlines 
news_articles_df.head(10)

# Remove digits and non-alphabetic characters


# Create an instance of the TfidfVectorizer and set the max_df to 0.95 and min_df to 10, and use the English stopwords to be ignored.


# Transform each row from the headlines Series to a DTM.

# Get the shape of the DTM.


# Print the sparse matrix of the transformed data.
# We have 23,377 documents, the first number in the tuple represents the document number.
# The second number in the tuple represents the index of the word in the vocabulary created by fit_transform.
# The last number represents the value of the TF-IDF score for the vocabulary word.


# Get the feature names (words) from the TfidfVectorizer


# Get all the non-zero elements from the first row.


# Get the indices for each non-zero element.


# Print out the word and the number of times the word is in the row. 


# Initialize the NMF and set the number of topics to 7. 

# Fit the model with our DTM data. 


# Get the length of the array of each topic. It should be the same as the vocabulary.


# Get the array of the first topic 

# This is the ranking of each word in the array. Lower values have less impact than higher values.


# Get the indices of the top ten words for the first topic (e.g., top 10 words for topic 0):


# Get the top ten words from the indices. 


# Print the top 30 words for each topic


# Transform our DTM so we get an array with the (number_of_documents, number_of_topics).


# Get the shape of the topic results


# Get the sorted indices for each topic in the first headline.

# Print the ranking of topics for the headline


# Read in our original news headlines. 

# Combine the original data with the topic label. 


# Get the first 10 rows. 


# Get the last 10 rows. 




# Import the dependencies 
import pandas as pd
import re
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF

# Set the column width to 200.
pd.set_option('max_colwidth', 200)

# Read the bbc_news_articles.csv file into a DataFrame.
news_articles_df = pd.read_csv('Resources/bbc_news_articles.csv')
# Display the first 20 rows. 
news_articles_df.head(20)

# Check for null values.
news_articles_df.info()

# Remove numbers and non-alphabetic characters from the news_summary column.
news_articles_df['news_summary'] = news_articles_df['news_summary'].apply(lambda x: re.sub(r'[^a-zA-Z\s ]', '', str(x)))
news_articles_df.head(10)

# Create an instance of the TfidfVectorizer and set the max_df to 0.95 and min_df to 5, and use the English stopwords to be ignored.
tfidf = TfidfVectorizer(max_df=0.95, min_df=5, stop_words='english')
tfidf

# Transform each row from the news summary to a DTM.
dtm = tfidf.fit_transform(news_articles_df['news_summary'])
# Get the shape of the DTM.
print(dtm.shape)

# Initialize the NMF and set the number of topics to 5. 
nmf_model = NMF(n_components=5,random_state=42)
# Fit the model with our DTM data. 
nmf_model.fit(dtm)

# Check the length of the vocabulary 
len(tfidf.get_feature_names_out())

# Print the top 15 words for each topic
for index,topic in enumerate(nmf_model.components_):
    print(f'The top 15 words for topic #{index+1}')
    print([tfidf.get_feature_names_out()[i] for i in topic.argsort()[-15:]])
    print('\n')

# Transform our DTM so we get an array with the (number_of_documents, number_of_topics).
topic_results = nmf_model.transform(dtm)

# Get the shape of the topic results
topic_results.shape

# Read the bbc_news_articles.csv file into a DataFrame.
news_articles_df_2 = pd.read_csv('Resources/bbc_news_articles.csv')
# Display the DataFrame. 
news_articles_df_2.head()

# Use the add_topic_labels function to add the topic and topic label to each news summary. 
# Dictionary of  topics and topic label.
topic_labels = {
    1: 'Business',
    2: 'Entertainment',
    3: 'Politics',
    4: 'Sports',
    5: 'Technology'
}

# Define the function and pass in the DataFrame, the topic_results, and topic_labels dictionary.
def add_topic_labels(df, topic_results, topic_labels):
    # Find the dominant topic for each document and add the label to a new column
    df['topic'] = topic_results.argmax(axis=1) + 1
    # Use the map function to add the topic label to the news summary based on the topic number.
    df['topic_label'] = df['topic'].map(topic_labels)


# Call the function to add topic labels to your DataFrame.
add_topic_labels(news_articles_df_2, topic_results, topic_labels)

# Display the first 10 rows of the updated DataFrame. 
news_articles_df_2.head(10)

# Display the last 10 rows of the updated DataFrame.
news_articles_df_2.tail(10)



# Import the dependencies 
import pandas as pd
import re
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF

# Set the column width to 200.
pd.set_option('max_colwidth', 200)

# Read the bbc_news_articles.csv file into a DataFrame.

# Display the first 20 rows. 


# Check for null values.


# Remove numbers and non-alphabetic characters from the news_summary column.


# Create an instance of the TfidfVectorizer and set the max_df to 0.95 and min_df to 5, and use the English stopwords to be ignored.


# Transform each row from the news summary to a DTM.

# Get the shape of the DTM.


# Initialize the NMF and set the number of topics to 5. 

# Fit the model with our DTM data. 


# Check the length of the vocabulary 


# Print the top 15 words for each topic


# Transform our DTM so we get an array with the (number_of_documents, number_of_topics).


# Get the shape of the topic results


# Read the bbc_news_articles.csv file into a DataFrame.
news_articles_df_2 = pd.read_csv('Resources/bbc_news_articles.csv')
# Display the DataFrame. 
news_articles_df_2.head()

# Use the add_topic_labels function to add the topic and topic label to each news summary. 
# Dictionary of  topics and topic label.
topic_labels = {
    1: '',
    2: '',
    3: '',
    4: '',
    5: ''
}

# Define the function and pass in the DataFrame, the topic_results, and topic_labels dictionary.
def add_topic_labels(df, topic_results, topic_labels):
    # Find the dominant topic for each document and add the label to a new column
    df['topic'] = topic_results.argmax(axis=1) + 1
    # Use the map function to add the topic label to the news summary based on the topic number.
    df['topic_label'] = df['topic'].map(topic_labels)


# Call the function to add topic labels to your DataFrame.
add_topic_labels(news_articles_df_2, topic_results, topic_labels)

# Display the first 10 rows of the updated DataFrame. 


# Display the last 10 rows of the updated DataFrame.




# Import spacy, import the large english model (this is 600 MB), and disable tagger, ner and lemmatizer.
# Download the large English model ~ 600 MB if you have not done so: `python -m spacy download en_core_web_lg`.
import spacy
nlp = spacy.load('en_core_web_lg',disable=["tagger", "ner", "lemmatizer"])

# Read in the text file with the read_file function.
def read_file(filepath):
    """
    Reads in a text file from the directory and saves the file contents to a variable.
    
    Args:
        text (str): The input text to read and save
        
    Returns: 
        A string containing the file contents.
    """
    with open(filepath) as f:
        str_text = f.read()
    return str_text

# Use the separate_punc function to remove the puncutation. 
def separate_punc(md_text):
    """
    Retrieves all the words in the text without punctuation 

    Args:
        text (str): The input text from which words are extracted witout punctuation.

    Returns:
        list: A list of words

    """
    # Create a list comprehension to get only the tokens, i.e., words in the text.
    return [token.text.lower() for token in nlp(md_text) \
            if token.text not in '\n\n \n\n\n!"-#$%&()--.*+,-/:;<=>?@[\\]^_`{|}~\t\n']

# Pass in the first four chapters of Moby Dick to the read_file function.
md_text = read_file('Resources/moby_dick_four_chapters.txt')
# Clean and tokenize the text using the separate_punc function.
tokens = separate_punc(md_text)

# Get the length of the tokens list.
len(tokens)

# Look over the tokems to make sure all theo punctuation has been reomved.  
# Some punctuation wasn't removed, i.e., "?--".
print(tokens[:300])

# How many tokens contain "?--"?
count = 0
for token in tokens:
    if "?--" in token:
        count +=1
print(count)

# Organize into sequences of tokens.
# Use 25 words to predict the 26th word using "+1".
train_len = 26 # = 25 training words plus one target word.

# Create an empty list of sequences
text_sequences = []

for i in range(train_len, len(tokens)):
    # Range is 26 to 11,338
    # Get the train_len amount of characters 
    # First iteration gets the words from 0:26
    # The second iteration getst the words from 1:27, etc.
    seq = tokens[i-train_len:i]
    
    # Add to list of sequences
    text_sequences.append(seq)

# The first 26 words [0:26]
print(text_sequences[0])
# The next 26 words [1:27]
print(text_sequences[1]) 
# The next 26 words [2:28]. 
print(text_sequences[2])

# Join the first 26 words. 
print(' '.join(text_sequences[0]))
# Join the next 26 words. 
print(' '.join(text_sequences[1]))
# And, join the next 26 words. 
print(' '.join(text_sequences[2]))

# The list of text_sequences should be 26 less than the total tokens.
len(text_sequences)

# Import the Keras tokenization to format the data from words into a numerical format.
from keras.preprocessing.text import Tokenizer

# Initialize the Keras Tokenizer class 
tokenizer = Tokenizer()
# Map each word with an index.
tokenizer.fit_on_texts(text_sequences)

# Get the dictionary mapping of words to their indices
print(tokenizer.word_index)

# Get the dictionary of words and the number of times they appear in the text.
print(tokenizer.word_counts) 

# What is the size of the vocabulary
vocabulary_size = len(tokenizer.word_counts)
print(vocabulary_size)

# Encode each word in the text_sequences to the indices. 
sequences = tokenizer.texts_to_sequences(text_sequences)

# Get the encoded indices for the the first 26 words
print(sequences[0])

# Get the word associated with the indices for the first sequence.
for i in sequences[0]:
    print(f'{i} : {tokenizer.index_word[i]}')

# Get the word associated with a specific index.
tokenizer.index_word.get(956)

# Get the number of times the word "call" appears in the text.
count = 0
index_to_word = {index: word for word, index in tokenizer.word_index.items()}
for sequence in sequences:
    words = [index_to_word.get(index, '') for index in sequence]
    count += words.count(tokenizer.index_word.get(956, ''))

print(count)

# Import numpy to convert the list of sequences to arrays.
import numpy as np

# Convert the all 26 word list of lists to arrays.
num_sequences = np.array(sequences)
print(num_sequences)

# Get the length of the array
len(num_sequences)

# Get the first array.
print(num_sequences[0])

# Import the to_categorical function to convert the arrays to binary values.
import keras
from keras.utils import to_categorical

# Get the first 25 numbers from each array. These will be our "X". 
for sequence in num_sequences[:,:-1]:
    print(sequence)

# Get the last number (number 26) from each array. These will be our "y".
for sequence in num_sequences[:,-1]:
    print(sequence)

# Set the input variable, `X`, to the first 25 numbers of each array.
X = num_sequences[:,:-1]
# Set target variable, `y`, to the last number of each array.
y = num_sequences[:,-1]

# Get the shape of X
print(X.shape)
# Get the length of each sequence.
seq_len = X.shape[1]
print(seq_len)

# Get the shape of y
y.shape

# Next,one-hot encode the target variable to get transform each index to a binary value. 
# We increase the vocabulary by 1 so we can predict the next word. 
y = to_categorical(y, num_classes=vocabulary_size+1)

# Get the shape of y again.
y.shape

# Print the first 24 binary values in the first array.
# The index in for the last word in before transforming was "24", this is converted to a "1".
print(y[0,:25])

# Import the dependencies for LSTM model.
from keras.models import Sequential
from keras.layers import Dense,LSTM,Embedding

def create_model(vocabulary_size, seq_len):
    """
    Create and compile an LSTM-based sequential model for text generation.

    Parameters:
    - vocabulary_size (int): The size of the vocabulary, i.e., the number of unique words in the text.
    - seq_len (int): The length of the input sequences, indicating the number of words in each sequence.

    Returns:
    - model (Sequential): A compiled Keras sequential model for text generation.

    Model Architecture:
    - Embedding Layer: Maps word indices to dense vectors.
    - LSTM Layer 1: 150 units, returns full sequence.
    - LSTM Layer 2: 150 units.
    - Dense Layer: 150 units, ReLU activation.
    - Output Layer: Dense layer with softmax activation for multi-class classification.

    Compilation:
    - Loss: Categorical crossentropy.
    - Optimizer: Adam.
    - Metrics: Accuracy.

    Usage Example:
    ```python
    model = create_model(vocabulary_size=10000, seq_len=25)
    ```
    """
    model = Sequential()
    model.add(Embedding(vocabulary_size, 25, input_length=seq_len))
    model.add(LSTM(150, return_sequences=True))
    model.add(LSTM(150))
    model.add(Dense(150, activation='relu'))

    model.add(Dense(vocabulary_size, activation='softmax'))
    
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
   
    model.summary()
    
    return model

# Define the model and pass in the vocabulary (+1) and the seq_len (25 words).
model = create_model(vocabulary_size+1, seq_len)

# Fit model with 300 epochs. (This will take ~60 min to complete.)
model.fit(X, y, batch_size=128, epochs=300,verbose=1) 

from pickle import dump

# Save the model to file
# model.save('four_chapters_moby_dick_model_300.keras')
# Save the tokenizer
# dump(tokenizer, open('four_chapters_moby_dick_tokenizer_300', 'wb'))

# Import the dependencies needed for the LSTM.
from random import randint
from pickle import load
from keras.models import load_model
# pip install Keras-Preprocessing
from keras_preprocessing.sequence import pad_sequences

def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):
    '''
    Generate text using a trained language model.
    
    INPUTS:
     - model: Trained language model (e.g., LSTM) capable of text generation.
     - tokenizer: Tokenizer that was fit on text data.
     - seq_len: Length of the training sequences used to train the model.
     - seed_text: A raw string text serving as the seed for text generation.
     - num_gen_words: The number of words to be generated by model.
    '''
    
    # Final Output
    output_text = []
    
    # Intial Seed Sequence
    input_text = seed_text
    
    # Create num_gen_words
    for i in range(num_gen_words):
        
        # Take the input text string and encode it to a sequence
        encoded_text = tokenizer.texts_to_sequences([input_text])[0]
        # Pad sequences to our trained rate of 25 words.
        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')
        # Predict Class Probabilities for each word
        pred_w = model.predict(pad_encoded, verbose=0)[0]
        
        pred_word_ind = np.argmax(pred_w, axis= -1)
        
        # Grab word
        pred_word = tokenizer.index_word[pred_word_ind] 
        
        # Update the sequence of input text (shifting one over with the new word)
        input_text += ' ' + pred_word
        
        output_text.append(pred_word)
        
    # Make it look like a sentence.
    return ' '.join(output_text)

# Import the random module. 
import random

# Pick a random sequence of 26 words. 
random_pick = random.randint(10,len(text_sequences))
random_seed_text = text_sequences[random_pick]
# Join the words
seed_text = ' '.join(random_seed_text)
print(seed_text)

# Import the load_model method.
from keras.models import load_model

# Set the model to the saved trained 300 epoch model. 
model = load_model('four_chapters_moby_dick_model_300.keras')
# Set the tokenizer to the trained tokenizer from the model. 
tokenizer = load(open('four_chapters_moby_dick_tokenizer_300', 'rb'))

# Call the generate_text function and pass in the required parameters. We set the num_gen_words = 25. 
generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=25)

# Choose a 26 word text string from the first four chapters of Moby Dick.
# If less than 26 the accuracy is off. 
text = """Seeing, now, that there were no curtains to the window, and that the street being very narrow,
the house opposite commanded a plain view into the room, and"""

# Create tokens by using the separate_punc function.
text_tokens = separate_punc(text)
print(text_tokens)
# Join the tokens and set them to the "seed_text" variable. 
seed_text = ' '.join(text_tokens)
print(seed_text)

# Call the generate_text function and pass in the required parameters. We set the num_gen_words =50. 
generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=25)

# Import spacy, import the large english model (this is 600 MB), and disable tagger, ner and lemmatizer.
# Download the large English model ~ 600 MB if you have not done so: `python -m spacy download en_core_web_lg`.
import spacy
nlp = spacy.load('en_core_web_lg',disable=["tagger", "ner", "lemmatizer"])

# Read in the text file with the read_file function.
def read_file(filepath):
    """
    Reads in a text file from the directory and saves the file contents to a variable.
    
    Args:
        text (str): The input text to read and save
        
    Returns: 
        A string containing the file contents.
    """
    with open(filepath) as f:
        str_text = f.read()
    return str_text

# Use the separate_punc function to remove the puncutation. 
def separate_punc(md_text):
    """
    Retrieves all the words in the text without punctuation 

    Args:
        text (str): The input text from which words are extracted witout punctuation.

    Returns:
        list: A list of words

    """
    # Create a list comprehension to get only the tokens, i.e., words in the text.
    return [token.text.lower() for token in nlp(md_text) \
            if token.text not in '\n\n \n\n\n!"-#$%&()--.*+,-/:;<=>?@[\\]^_`{|}~\t\n']

# Pass in the first four chapters of Moby Dick to the read_file function.

# Clean and tokenize the text using the separate_punc function.


# Get the length of the tokens list.


# Look over the tokems to make sure all theo punctuation has been reomved.  
# Some punctuation wasn't removed, i.e., "?--".
print(tokens[:300])

# How many tokens contain "?--"?


# Organize into sequences of tokens.
# Use 25 words to predict the 26th word using "+1".
train_len = 26 # = 25 training words plus one target word.

# Create an empty list of sequences


# Use a for loop to create lists of 26 tokens for the whole text. 

    # Range is 26 to 11,338
    # Get the train_len amount of characters 
    # First iteration gets the words from 0:26
    # The second iteration getst the words from 1:27, etc.

    
    # Add to list of sequences


# The first 26 words [0:26]
print(text_sequences[0])
# The next 26 words [1:27]
print(text_sequences[1]) 
# The next 26 words [2:28]. 
print(text_sequences[2])

# Join the first 26 words. 
print(' '.join(text_sequences[0]))
# Join the next 26 words. 
print(' '.join(text_sequences[1]))
# And, join the next 26 words. 
print(' '.join(text_sequences[2]))

# The list of text_sequences should be 26 less than the total tokens.
len(text_sequences)

# Import the Keras tokenization to format the data from words into a numerical format.
from keras.preprocessing.text import Tokenizer

# Initialize the Keras Tokenizer class 
tokenizer = Tokenizer()
# Map each word with an index.
tokenizer.fit_on_texts(text_sequences)

# Get the dictionary mapping of words to their indices


# Get the dictionary of words and the number of times they appear in the text.


# What is the size of the vocabulary


# Encode each word in the text_sequences to the indices. 


# Get the encoded indices for the the first 26 words


# Get the word associated with the indices for the first sequence.


# Get the word associated with a specific index.


# Get the number of times the word "call" appears in the text.


# Import numpy to convert the list of sequences to arrays.
import numpy as np

# Convert the all 26 word list of lists to arrays.


# Get the length of the array


# Get the first array.


# Import the to_categorical function to convert the arrays to binary values.
import keras
from keras.utils import to_categorical

# Get the first 25 numbers from each array. These will be our "X". 


# Get the last number (number 26) from each array. These will be our "y".


# Set the input variable, `X`, to the first 25 numbers of each array.

# Set target variable, `y`, to the last number of each array.


# Get the shape of X

# Get the length of each sequence.


# Get the shape of y


# Next,one-hot encode the target variable to get transform each index to a binary value. 
# We increase the vocabulary by 1 so we can predict the next word. 


# Get the shape of y again.


# Print the first 24 binary values in the first array.
# The index in for the last word in before transforming was "24", this is converted to a "1".


# Import the dependencies for LSTM model.
from keras.models import Sequential
from keras.layers import Dense,LSTM,Embedding

def create_model(vocabulary_size, seq_len):
    """
    Create and compile an LSTM-based sequential model for text generation.

    Parameters:
    - vocabulary_size (int): The size of the vocabulary, i.e., the number of unique words in the text.
    - seq_len (int): The length of the input sequences, indicating the number of words in each sequence.

    Returns:
    - model (Sequential): A compiled Keras sequential model for text generation.

    Model Architecture:
    - Embedding Layer: Maps word indices to dense vectors.
    - LSTM Layer 1: 150 units, returns full sequence.
    - LSTM Layer 2: 150 units.
    - Dense Layer: 150 units, ReLU activation.
    - Output Layer: Dense layer with softmax activation for multi-class classification.

    Compilation:
    - Loss: Categorical crossentropy.
    - Optimizer: Adam.
    - Metrics: Accuracy.

    Usage Example:
    ```python
    model = create_model(vocabulary_size=10000, seq_len=25)
    ```
    """
    model = Sequential()
    model.add(Embedding(vocabulary_size, 25, input_length=seq_len))
    model.add(LSTM(150, return_sequences=True))
    model.add(LSTM(150))
    model.add(Dense(150, activation='relu'))

    model.add(Dense(vocabulary_size, activation='softmax'))
    
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
   
    model.summary()
    
    return model

# Define the model and pass in the vocabulary (+1) and the seq_len (25 words).
model = create_model(vocabulary_size+1, seq_len)

# Fit model with 300 epochs. (This will take ~60 min to complete.)
model.fit(X, y, batch_size=128, epochs=300,verbose=1) 

from pickle import dump

# Save the model to file
# model.save('four_chapters_moby_dick_model_300.keras')
# Save the tokenizer
# dump(tokenizer, open('four_chapters_moby_dick_tokenizer_300', 'wb'))

# Import the dependencies needed for the LSTM.
from random import randint
from pickle import load
from keras.models import load_model
# May needt to use `pip install Keras-Preprocessing`
from keras_preprocessing.sequence import pad_sequences

def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):
    '''
    Generate text using a trained language model.
    
    INPUTS:
     - model: Trained language model (e.g., LSTM) capable of text generation.
     - tokenizer: Tokenizer that was fit on text data.
     - seq_len: Length of the training sequences used to train the model.
     - seed_text: A raw string text serving as the seed for text generation.
     - num_gen_words: The number of words to be generated by model.
    '''
    
    # Final Output
    output_text = []
    
    # Intial Seed Sequence
    input_text = seed_text
    
    # Create num_gen_words
    for i in range(num_gen_words):
        
        # Take the input text string and encode it to a sequence
        encoded_text = tokenizer.texts_to_sequences([input_text])[0]
        # Pad sequences to our trained rate of 25 words.
        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')
        # Predict Class Probabilities for each word
        pred_w = model.predict(pad_encoded, verbose=0)[0]
        
        pred_word_ind = np.argmax(pred_w, axis= -1)
        
        # Grab word
        pred_word = tokenizer.index_word[pred_word_ind] 
        
        # Update the sequence of input text (shifting one over with the new word)
        input_text += ' ' + pred_word
        
        output_text.append(pred_word)
        
    # Make it look like a sentence.
    return ' '.join(output_text)

# Import the random module. 
import random

# Pick a random sequence of 26 words. 

# Join the words


# Import the load_model method.
from keras.models import load_model

# Set the model to the saved trained 300 epoch model. 

# Set the tokenizer to the trained tokenizer from the model. 


# Call the generate_text function and pass in the required parameters. We set the num_gen_words = 25. 


# Choose a 26 word text string from the first four chapters of Moby Dick.
# If less than 26 the accuracy is off. 
text = """provide 25 words from the first four chapters of Mody Dick"""

# Create tokens by using the separate_punc function.

# Join the tokens and set them to the "seed_text" variable. 


# Call the generate_text function and pass in the required parameters. Set the `num_gen_words` to 25. 
generate_text(m)

# Import spacy, import the large english model (this is 600 MB), and disable tagger, ner and lemmatizer.
# Download the large English model ~ 600 MB if you have not done so: `python -m spacy download en_core_web_lg`.
import spacy
nlp = spacy.load('en_core_web_lg',disable=["tagger", "ner", "lemmatizer"])

# Read in the text file with the read_file function.
def read_file(filepath):
    """
    Reads in a text file from the directory and saves the file contents to a variable.
    
    Args:
        text (str): The input text to read and save
        
    Returns: 
        A string containing the file contents.
    """
    with open(filepath) as f:
        str_text = f.read()
    return str_text

# Use the separate_punc function to remove the puncutation. 
def separate_punc(holmes_text):
    """
    Retrieves all the words in the text without punctuation 

    Args:
        text (str): The input text from which words are extracted witout punctuation.

    Returns:
        list: A list of words

    """
    # Create a list comprehension to get only the tokens, i.e., words in the text.
    return [token.text.lower() for token in nlp(holmes_text) \
            if token.text not in '\n\n \n\n\n!"â€œâ€-#$%&()--.*+,-/:;<=>?@[\\]^_`{|}~\t\n']

# Pass in the first four chapters of Moby Dick to the read_file function.
holmes_text = read_file('Resources/A_Case_Of_Identity.txt')
print(holmes_text)

# Clean and tokenize the text using the separate_punc function.
tokens = separate_punc(holmes_text)

# Get the length of the tokens list.
len(tokens)

# Look over the tokems to make sure all theo punctuation has been reomved.  
# Some punctuation wasn't removed
print(tokens[:300])

print(tokens[500:800])

# Organize into sequences of tokens.
# Use 25 words to predict the 26th word using "+1".
train_len = 26 # = 25 training words plus one target word.

# Create an empty list of sequences
text_sequences = []

# Use a for loop to create lists of 26 tokens for the whole text. 
for i in range(train_len, len(tokens)):
    seq = tokens[i-train_len:i]
    
    # Add to list of sequences
    text_sequences.append(seq)

# The list of text_sequences should be 26 less than the total tokens.
len(text_sequences)

# Import the Keras tokenization to format the data from words into a numerical format.
from keras.preprocessing.text import Tokenizer

# Initialize the Keras Tokenizer class 
tokenizer = Tokenizer()
# Map each word with an index.
tokenizer.fit_on_texts(text_sequences)

# What is the size of the vocabulary
vocabulary_size = len(tokenizer.word_counts)
print(vocabulary_size)

# Encode each word in the text_sequences to the indices. 
sequences = tokenizer.texts_to_sequences(text_sequences)

# Import numpy to convert the list of sequences to arrays.
import numpy as np

# Convert the all 26 word list of lists to arrays.
num_sequences = np.array(sequences)
# Get the length of the array
len(num_sequences)

# Import the to_categorical function to convert the arrays to binary values.
import keras
from keras.utils import to_categorical

# Set the input variable, `X`, to the first 25 numbers of each array.
X = num_sequences[:,:-1]
# Set target variable, `y`, to the last number of each array.
y = num_sequences[:,-1]

# Get the shape of X
print(X.shape)
# Get the length of each sequence.
seq_len = X.shape[1]
print(seq_len)

# Get the shape of y
y.shape

# Next,one-hot encode the target variable to get transform each index to a binary value. 
# We increase the vocabulary by 1 so we can predict the next word. 
y = to_categorical(y, num_classes=vocabulary_size+1)

# Get the shape of y again.
y.shape

# Import the dependencies for LSTM model.
from keras.models import Sequential
from keras.layers import Dense,LSTM,Embedding

def create_model(vocabulary_size, seq_len):
    """
    Create and compile an LSTM-based sequential model for text generation.

    Parameters:
    - vocabulary_size (int): The size of the vocabulary, i.e., the number of unique words in the text.
    - seq_len (int): The length of the input sequences, indicating the number of words in each sequence.

    Returns:
    - model (Sequential): A compiled Keras sequential model for text generation.

    Model Architecture:
    - Embedding Layer: Maps word indices to dense vectors.
    - LSTM Layer 1: 150 units, returns full sequence.
    - LSTM Layer 2: 150 units.
    - Dense Layer: 150 units, ReLU activation.
    - Output Layer: Dense layer with softmax activation for multi-class classification.

    Compilation:
    - Loss: Categorical crossentropy.
    - Optimizer: Adam.
    - Metrics: Accuracy.

    Usage Example:
    ```python
    model = create_model(vocabulary_size=10000, seq_len=25)
    ```
    """
    model = Sequential()
    model.add(Embedding(vocabulary_size, 25, input_length=seq_len))
    model.add(LSTM(150, return_sequences=True))
    model.add(LSTM(150))
    model.add(Dense(150, activation='relu'))

    model.add(Dense(vocabulary_size, activation='softmax'))
    
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
   
    model.summary()
    
    return model

# Define the model and pass in the vocabulary (+1) and the seq_len (25 words).
model = create_model(vocabulary_size+1, seq_len)

# Fit model with 290-300 epochs and a batch size of 128.
model.fit(X, y, epochs=300, batch_size=128,verbose=1) 

# Import the dump function from the pickle module.
from pickle import dump

# Save the model to file
model.save('sherlock_model_300.keras')
# Save the tokenizer
dump(tokenizer, open('sherlock_tokenizer_300', 'wb'))

# Import the load function from the pickle module.
from pickle import load
# Import the dependencies needed to load the model and tokenizers, and process the text.
from keras.models import load_model
from keras_preprocessing.sequence import pad_sequences

def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):
    '''
    Generate text using a trained language model.
    
    INPUTS:
     - model: Trained language model (e.g., LSTM) capable of text generation.
     - tokenizer: Tokenizer that was fit on text data.
     - seq_len: Length of the training sequences used to train the model.
     - seed_text: A raw string text serving as the seed for text generation.
     - num_gen_words: The number of words to be generated by model.
    '''
    
    # Final Output
    output_text = []
    
    # Intial Seed Sequence
    input_text = seed_text
    
    # Create num_gen_words
    for i in range(num_gen_words):
        
        # Take the input text string and encode it to a sequence
        encoded_text = tokenizer.texts_to_sequences([input_text])[0]
        # Pad sequences to our trained rate of 25 words.
        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')
        # Predict Class Probabilities for each word
        pred_w = model.predict(pad_encoded, verbose=0)[0]
        
        pred_word_ind = np.argmax(pred_w, axis= -1)
        
        # Grab word
        pred_word = tokenizer.index_word[pred_word_ind] 
        
        # Update the sequence of input text (shifting one over with the new word)
        input_text += ' ' + pred_word
        
        output_text.append(pred_word)
        
    # Make it look like a sentence.
    return ' '.join(output_text)

# Choose a 26 word text string from the first four chapters of Moby Dick.
# If less than 26 the accuracy is off. 
text = """Her jacket was black, with black beads sewn
upon it, and a fringe of little black jet ornaments. Her dress was
brown, rather darker than"""

# Create tokens by using the separate_punc function.
text_tokens = separate_punc(text)
print(text_tokens)
# Join the tokens and set them to the "seed_text" variable. 
seed_text = ' '.join(text_tokens)
print(seed_text)

# Set the model to the saved trained 300 epoch model. 
model = load_model('Resources/sherlock_model_300.keras')
# Set the tokenizer to the trained tokenizer from the model. 
tokenizer = load(open('Resources/sherlock_tokenizer_300', 'rb'))

# Call the generate_text function and pass in the required parameters. We set the num_gen_words =50. 
generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=25)



# Import spacy, import the large english model (this is 600 MB), and disable tagger, ner and lemmatizer.
# Download the large English model ~ 600 MB if you have not done so: `python -m spacy download en_core_web_lg`.
import spacy
nlp = spacy.load('en_core_web_lg',disable=["tagger", "ner", "lemmatizer"])

# Read in the text file with the read_file function.
def read_file(filepath):
    """
    Reads in a text file from the directory and saves the file contents to a variable.
    
    Args:
        text (str): The input text to read and save
        
    Returns: 
        A string containing the file contents.
    """
    with open(filepath) as f:
        str_text = f.read()
    return str_text

# Use the separate_punc function to remove the puncutation. 
def separate_punc():
    """
    Retrieves all the words in the text without punctuation 

    Args:
        text (str): The input text from which words are extracted witout punctuation.

    Returns:
        list: A list of words

    """
    # Create a list comprehension to get only the tokens, i.e., words in the text.
    return [token.text.lower() for token in nlp(holmes_text) \
            if token.text not in '\n\n \n\n\n!"â€œâ€-#$%&()--.*+,-/:;<=>?@[\\]^_`{|}~\t\n']

# Pass in the first four chapters of Moby Dick to the read_file function.
holmes_text = read_file('Resources/A_Case_Of_Identity.txt')
print(holmes_text)

# Clean and tokenize the text using the separate_punc function.


# Get the length of the tokens list.


# Look over the tokems to make sure all theo punctuation has been reomved.  
# Some punctuation wasn't removed




# Organize into sequences of tokens.
# Use 25 words to predict the 26th word using "+1".


# Create an empty list of sequences

# Use a for loop to create lists of 26 tokens for the whole text. 

    
    # Add to list of sequences
    

# The list of text_sequences should be 26 less than the total tokens.


# Import the Keras tokenization to format the data from words into a numerical format.
from keras.preprocessing.text import Tokenizer

# Initialize the Keras Tokenizer class 

# Map each word with an index.


# What is the size of the vocabulary


# Encode each word in the text_sequences to the indices. 


# Import numpy to convert the list of sequences to arrays.
import numpy as np

# Convert the all 26 word list of lists to arrays.

# Get the length of the array


# Import the to_categorical function to convert the arrays to binary values.
import keras
from keras.utils import to_categorical

# Set the input variable, `X`, to the first 25 numbers of each array.

# Set target variable, `y`, to the last number of each array.


# Get the shape of X

# Get the length of each sequence.



# Get the shape of y


# Next,one-hot encode the target variable to get transform each index to a binary value. 
# We increase the vocabulary by 1 so we can predict the next word. 


# Get the shape of y again.


# Import the dependencies for LSTM model.
from keras.models import Sequential
from keras.layers import Dense,LSTM,Embedding

def create_model(vocabulary_size, seq_len):
    """
    Create and compile an LSTM-based sequential model for text generation.

    Parameters:
    - vocabulary_size (int): The size of the vocabulary, i.e., the number of unique words in the text.
    - seq_len (int): The length of the input sequences, indicating the number of words in each sequence.

    Returns:
    - model (Sequential): A compiled Keras sequential model for text generation.

    Model Architecture:
    - Embedding Layer: Maps word indices to dense vectors.
    - LSTM Layer 1: 150 units, returns full sequence.
    - LSTM Layer 2: 150 units.
    - Dense Layer: 150 units, ReLU activation.
    - Output Layer: Dense layer with softmax activation for multi-class classification.

    Compilation:
    - Loss: Categorical crossentropy.
    - Optimizer: Adam.
    - Metrics: Accuracy.

    Usage Example:
    ```python
    model = create_model(vocabulary_size=10000, seq_len=25)
    ```
    """
    model = Sequential()
    model.add(Embedding(vocabulary_size, 25, input_length=seq_len))
    model.add(LSTM(150, return_sequences=True))
    model.add(LSTM(150))
    model.add(Dense(150, activation='relu'))

    model.add(Dense(vocabulary_size, activation='softmax'))
    
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
   
    model.summary()
    
    return model

# Define the model and pass in the vocabulary (+1) and the seq_len (25 words).


# Fit model with 290-300 epochs and a batch size of 128.
model.fit() 

# Import the dump function from the pickle module.
from pickle import dump

# Save the model to file
model.save('<name>.keras')
# Save the tokenizer
dump(tokenizer, open('<name>_tokenizer', 'wb'))

# Import the load function from the pickle module.
from pickle import load
# Import the dependencies needed to load the model and tokenizers, and process the text.
from keras.models import load_model
from keras_preprocessing.sequence import pad_sequences

def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):
    '''
    Generate text using a trained language model.
    
    INPUTS:
     - model: Trained language model (e.g., LSTM) capable of text generation.
     - tokenizer: Tokenizer that was fit on text data.
     - seq_len: Length of the training sequences used to train the model.
     - seed_text: A raw string text serving as the seed for text generation.
     - num_gen_words: The number of words to be generated by model.
    '''
    
    # Final Output
    output_text = []
    
    # Intial Seed Sequence
    input_text = seed_text
    
    # Create num_gen_words
    for i in range(num_gen_words):
        
        # Take the input text string and encode it to a sequence
        encoded_text = tokenizer.texts_to_sequences([input_text])[0]
        # Pad sequences to our trained rate of 25 words.
        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')
        # Predict Class Probabilities for each word
        pred_w = model.predict(pad_encoded, verbose=0)[0]
        
        pred_word_ind = np.argmax(pred_w, axis= -1)
        
        # Grab word
        pred_word = tokenizer.index_word[pred_word_ind] 
        
        # Update the sequence of input text (shifting one over with the new word)
        input_text += ' ' + pred_word
        
        output_text.append(pred_word)
        
    # Make it look like a sentence.
    return ' '.join(output_text)

# Choose a 26 word text string from the first four chapters of Moby Dick.
# If less than 26 the accuracy is off. 
text = """"""

# Create tokens by using the separate_punc function.

# Join the tokens and set them to the "seed_text" variable. 


# Set the model to the saved trained 300 epoch model. 
model = load_model('<name>.keras')
# Set the tokenizer to the trained tokenizer from the model. 
tokenizer = load(open('<name>_tokenizer', 'rb'))

# Call the generate_text function and pass in the required parameters. Set the `num_gen_words` to 25. 
generate_text()



# Import the dependencies
import nltk
from nltk.corpus import stopwords
nltk.download('punkt')
# Initialize the stopwords
stop_words = set(stopwords.words('english'))

# Define the sentences.
sentence_1 = "I want to invest for retirement."
sentence_2 = "Should I invest in mutual funds, or should I invest in stocks?"
sentence_3 = "I should schedule an appointment with a financial planner."

# Import regex
import re
# Create a regex pattern to remove punctuation. 
pattern = r'[^a-zA-Z\s ]'

# Create an empty list to hold the tokens.
tokens = []

# Remove punctuation, tokenize sentence 1, and add the tokens to the tokens list.
sentence_1_cleaned = re.sub(pattern, '', sentence_1)
sentence_1_tokens = nltk.word_tokenize(sentence_1_cleaned.lower())
tokens.append(sentence_1_tokens)

# Remove punctuation, tokenize sentence 2, and add the tokens to the tokens list.
sentence_2_cleaned = re.sub(pattern, '', sentence_2)
sentence_2_tokens = nltk.word_tokenize(sentence_2_cleaned.lower())
tokens.append(sentence_2_tokens)

# Remove punctuation, tokenize sentence 3, and add the tokens to the tokens list.
sentence_3_cleaned = re.sub(pattern, '', sentence_3)
sentence_3_tokens = nltk.word_tokenize(sentence_3_cleaned.lower())
tokens.append(sentence_3_tokens)

# Display the tokens.
tokens

# Remove stopwords
filtered_tokens = []
for token in tokens:
    filtered_token = [word for word in token if not word in stop_words]
    filtered_tokens.append(filtered_token)
    
# Diplay the filtered_tokens
filtered_tokens

# Create the bag-of-words
bag_of_words = {}
for i in range(len(filtered_tokens)):
    for word in filtered_tokens[i]:
        if word not in bag_of_words:
            bag_of_words[word] = 0
        bag_of_words[word] += 1

# Print the bag_of_words
print(bag_of_words)

# Import the dependencies
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

# Create a CountVectorizer object
vectorizer = CountVectorizer(stop_words='english')

# Fit the vectorizer to the input sentences and transform them into a bag of words
bag_of_words = vectorizer.fit_transform([sentence_1,sentence_2, sentence_3])

# Print the resulting bag of words
print(bag_of_words.toarray())

# Create a DataFrame of the bag of words. 
bow_df = pd.DataFrame(bag_of_words.toarray(),columns=vectorizer.get_feature_names_out())
bow_df

# Print the vocabulary. 
print(bow_df.columns.to_list())

# Get the number of times each word appears in the vocabulary.
occurrence = bow_df.sum(axis=0)
print(occurrence)



# Import the dependencies
import nltk
from nltk.corpus import stopwords
nltk.download('punkt')
# Initialize the stopwords
stop_words = set(stopwords.words('english'))

# Define the sentences.
sentence_1 = "I want to invest for retirement."
sentence_2 = "Should I invest in mutual funds, or should I invest in stocks?"
sentence_3 = "I should schedule an appointment with a financial planner."

# Import regex
import re

# Create a regex pattern to remove punctuation. 
pattern = r'[^a-zA-Z\s ]'

# Create an empty list to hold the tokens.


# Remove punctuation, tokenize sentence 1, and add the tokens to the tokens list.


# Remove punctuation, tokenize sentence 2, and add the tokens to the tokens list.


# Remove punctuation, tokenize sentence 3, and add the tokens to the tokens list.


# Display the tokens.
tokens

# Remove stopwords



# Display the filtered tokens.


# Create a dictionary that will be our bag-of-words.


# Display the bag-of-words.


# Import the dependencies
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

# Create a CountVectorizer object
vectorizer = CountVectorizer(stop_words='english')

# Fit the vectorizer to the input sentences and transform them into a bag of words


# Print the resulting bag of words


# Create a DataFrame of the bag of words. 


# Print the vocabulary. 


# Get the number of times each word appears in the vocabulary.




# Uncomment the next line if you are using Google Colab
# !pip install transformers 

# Import the BertTokenizer from the transformers package.
from transformers import BertTokenizer

# Instantiate the BertTokenizer on the pre-trained data.
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Define an input text.
text = "I am learning about subword tokenization."

# Tokenize the text into subwords.
subwords = tokenizer.tokenize(text)
subwords

# Uncomment the next line if you are using Google Colab
# !pip install nltk

# Import Reuters database from the nltk corpus
from nltk.corpus import reuters
# Import tokenizers
from nltk.tokenize import sent_tokenize, word_tokenize

# Download the "punkt" sentence tokenizer.
import nltk
nltk.download("reuters")
nltk.download('punkt')

# We'll find the first article about cocoa.
print(reuters.categories())

# We'll find the first article about cocoa.
reuters.fileids(categories = 'cocoa')[0]

article = reuters.raw('test/15095')
print(article)

# NLTK tokenizes in similar way by using the `sent_tokenize` function
sent_tokenize(article)

# Print the first  sentence.
sent = sent_tokenize(article)[0]
print(sent)

# Tokenize the first sentence with the `word_tokenize` function.
word_tokenize(sent)

# Import the spaCy library
import spacy
# Load the small English language model for spaCy
nlp = spacy.load("en_core_web_sm")

# Tokenize the first sentence using token.text
spacy_sent = nlp(sent)
[token.text for token in spacy_sent]

# Tokenize the first sentence into subwords.
sentence_subwords = tokenizer.tokenize(sent)
sentence_subwords



# Uncomment the next line if you are using Google Colab
# !pip install transformers 

# Import the BertTokenizer from the transformers package.
from transformers import BertTokenizer

# Instantiate the BertTokenizer on the pre-trained data.
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Define an input text.
text = "I am learning about subword tokenization."

# Tokenize the text into subwords.


# Uncomment the next line if you are using Google Colab
# !pip install nltk

# Import Reuters database from the nltk corpus
from nltk.corpus import reuters
# Import tokenizers
from nltk.tokenize import sent_tokenize, word_tokenize

# Download the "punkt" sentence tokenizer.
import nltk
nltk.download("reuters")
nltk.download('punkt')

# We'll find the first article about cocoa.
print(reuters.categories())

# We'll find the first article about cocoa.
reuters.fileids(categories = 'cocoa')[0]

article = reuters.raw('test/15095')
print(article)

# NLTK tokenizes in similar way by using the `sent_tokenize()` function


# Print the first  sentence.


# Tokenize the first sentence with the `word_tokenize()` function.


# Import the spaCy library
import spacy
# Load the small English language model for spaCy
nlp = spacy.load("en_core_web_sm")

# Tokenize the first sentence using token.text


# Tokenize the first sentence into subwords.




# Import the dependencies
from keras.preprocessing.text import Tokenizer

# Define a list of sentences to tokenize. 
sentences = ["I love my dog.", "I love my family.", "My dog is a lab"]

# Create an instance of the Tokenizer
tokenizer = Tokenizer()
# Fit the tokenizer on the documents
tokenizer.fit_on_texts(sentences)

# Create a dictionary mapping of words to their indices
print(tokenizer.word_index)

# Encode each word in the text_sequences to the indices. 
sequences = tokenizer.texts_to_sequences(sentences)
sequences

# Regenerate the sentences from the indcies.
[sequence for sequence in tokenizer.sequences_to_texts_generator(sequences)]



# Import the dependencies
from keras.preprocessing.text import Tokenizer

# Define a list of sentences to tokenize. 
sentences = ["I love my dog.", "I love my family.", "My dog is a lab"]

# Create an instance of the Tokenizer
tokenizer = Tokenizer()
# Fit the tokenizer on the documents
tokenizer.fit_on_texts(sentences)

# Create a dictionary mapping of words to their indices


# Encode each word in the text_sequences to the indices. 


# Regenerate the sentences from the indcies.




# Uncomment the next line if you are using Google Colab
# ! pip install sentence-transformers

# Import the SentenceTransformer class from the sentence_transformers library. 
from sentence_transformers import SentenceTransformer
# Use the all-MiniLM-L6-v2 model.
model = SentenceTransformer('all-MiniLM-L6-v2')

# Define a sentence to be tokenized and pass the sentence into the tokenize() method
sentence = "I am learning a lot about transformers."

# Tokenize the sentence with model. 
tokens = model.tokenizer.tokenize(sentence)
tokens

# Convert the tokens to IDs.
ids = model.tokenizer.convert_tokens_to_ids(tokens)
print(ids)

# Get the first 20 numerical embedding for the sentence.
embeddings = model.encode(sentence)
embeddings[0:20]

len(embeddings)

# The model can decode the ids and embeddings back to the original sentence.
decoded_tokens = model.tokenizer.decode(ids)
print(decoded_tokens)



# Uncomment the next line if you are using Google Colab
# !pip install sentence-transformers

# Import the SentenceTransformer class and the utility function from the sentence_transformers library.
from sentence_transformers import SentenceTransformer, util
# Use the all-MiniLM-L6-v2 model.
model = SentenceTransformer('all-MiniLM-L6-v2')

# Define a list of sentences to tokenize.
sentences = ["I love my dog.", "I love my family.", "My dog is a lab."]

# Tokenize the sentences with the model.
tokenized_documents = [model.tokenizer.tokenize(sentence) for sentence in sentences]

# Get the numerical embeddings for all sentences.
embeddings = model.encode(sentences)

# Print the embeddings
for i, sentence in enumerate(sentences):
    print(f"Sentence {i+1}: {sentence}")
    print(f"Embedding {i+1}: {embeddings[i][0:10]}")
    print()

# Generate the cosine similarity scores using the embeddings.
cosine_scores = util.cos_sim(embeddings, embeddings)
cosine_scores

#Find the pairs with the highest cosine similarity scores
pairs = []
for i in range(len(cosine_scores)-1):
    for j in range(i+1, len(cosine_scores)):
        pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})

#Sort scores in decreasing order.
pairs = sorted(pairs, key=lambda x: x['score'], reverse=True)
print(pairs)

# Print out the pairs of sentences and their cosine similarity score.
for pair in pairs:
    i, j = pair['index']
    print(f" {sentences[i]} \t{sentences[j]} \t Score: {pair['score']:.4f}")

# Create a DataFrame for the cosine similarity scores.
import pandas as pd

# Convert the cosine similarity matrix to a Pandas DataFrame.
similarity_df = pd.DataFrame(cosine_scores, columns=['Sentence 1', 'Sentence 2', 'Sentence 3'], index=['Sentence 1', 'Sentence 2', 'Sentence 3'])
similarity_df



# Uncomment the next line if you are using Google Colab
# ! pip install sentence-transformers

# Import the SentenceTransformer class from the sentence_transformers library. 
from sentence_transformers import SentenceTransformer
# Use the all-MiniLM-L6-v2 model.
model = SentenceTransformer('all-MiniLM-L6-v2')

# Define a sentence to be tokenized and pass the sentence into the tokenize() method
sentence = "I am learning a lot about transformers."

# Tokenize the sentence with model. 


# Convert the tokens to IDs.


# Get the first 20 numerical embedding for the sentence.


# What is the lenght of the embeddings?


# The model can decode the ids and embeddings back to the original sentence.




# Uncomment the next line if you are using Google Colab
# !pip install sentence-transformers

# Import the SentenceTransformer class and the utility function from the sentence_transformers library.
from sentence_transformers import SentenceTransformer, util
# Use the all-MiniLM-L6-v2 model.
model = SentenceTransformer('all-MiniLM-L6-v2')

# Define a list of sentences to tokenize.
sentences = ["I love my dog.", "I love my family.", "My dog is a lab."]

# Tokenize the sentences with the model.
tokenized_documents = [model.tokenizer.tokenize(sentence) for sentence in sentences]

# Get the numerical embeddings for all sentences.
embeddings = model.encode(sentences)

# Print the embeddings


# Generate the cosine similarity scores using the embeddings.


#Find the pairs with the highest cosine similarity scores


#Sort scores in decreasing order.


# Print out the pairs of sentences and their cosine similarity score.


# Create a DataFrame for the cosine similarity scores.
import pandas as pd

# Convert the cosine similarity matrix to a Pandas DataFrame.




#  Uncomment the line below if you are using Google Colab.
# !pip install -U sentence-transformers

# Import the SentenceTransformer class and utility function class from the sentence_transformers module 
from sentence_transformers import SentenceTransformer, util
# Use the `all-MiniLM-L6-v2` model.
model = SentenceTransformer('all-MiniLM-L6-v2')
# Import pandas
import pandas as pd
pd.set_option('max_colwidth', 200)

# Create a DataFrame for the "news_headlines.csv" 
news_headlines_df = pd.read_csv("Resources/news_headlines.csv")
news_headlines_df

# Convert the "headline" column to a list 
news_headlines = news_headlines_df["headline"].tolist()
news_headlines

# Get the vector embeddings for the headlines.
news_headlines_embeddings = model.encode(news_headlines)

# Get the vector embeddings from the following news headline. 
new_headline = "Top 10 Hacks for Traveling Like a Pro."
new_headline_embedding = model.encode([new_headline])

# Create a list to store tuples of (news headline, similarity score)
similarities = []

# Loop through the headline embeddings.
for i, headline_embedding in enumerate(news_headlines_embeddings):
    # Calculate the cosine similarity score between each headline embedding and the ew headline embedding. 
    cosine_similarity_score = util.cos_sim(headline_embedding, new_headline_embedding)

    # Store the news headline and similarity score as a tuple in the list.
    similarities.append((news_headlines[i], cosine_similarity_score))

# Sort the list of tuples based on similarity score in descending order
similarities.sort(key=lambda x: x[1], reverse=True)

# Print the sorted results
print(f"News headline to categorize: {new_headline}")
print()

# Loop through the similarities list and get the headline and similarity score.
for i, (headline, similarity_score) in enumerate(similarities):
    # Get the category from the DataFrame for each headline.
    category = news_headlines_df.loc[news_headlines_df['headline'] == headline, 'category'].values[0]
    # Print the rank, category, and the news headline.
    print(f"Rank {i+1}: Category: {category}, Headline: {headline}")
    # Print the similarity score of the news headline.
    print(f"Similarity score: {similarity_score[0][0]}")
    print()



#  Uncomment the line below if you are using Google Colab.
# !pip install -U sentence-transformers

# Import the SentenceTransformer class and utility function class from the sentence_transformers module 
from sentence_transformers import SentenceTransformer, util
# Use the `all-MiniLM-L6-v2` model.
model = SentenceTransformer('all-MiniLM-L6-v2')
# Import pandas
import pandas as pd
pd.set_option('max_colwidth', 200)

# Create a DataFrame for the "news_headlines.csv" 


# Convert the "headline" column to a list 


# Get the vector embeddings for the headlines.


# Get the vector embeddings from the following news headline. 
new_headline = "Top 10 Hacks for Traveling Like a Pro."


# Create a list to store tuples of (news headline, similarity score)
similarities = []

# # Loop through the headline embeddings.

    # Calculate the cosine similarity score between each headline embedding and the new headline embedding. 
    # cosine_similarity_score = util.cos_sim(headline_embedding, new_headline_embedding)

    # Store the news headline and similarity score as a tuple
    

# Sort the list of tuples based on similarity score in descending order
#similarities.sort(key=lambda x: x[1], reverse=True)

# Print our news headline 
print(f"News headline to categorize: {new_headline}")
print()

# Loop through the the similarities list and get the headline and similarity score.

    # Get the category from the DataFrame for each headline..
    # category = news_headlines_df.loc[news_headlines_df['headline'] == headline, 'category'].values[0]
    # Print the rank, category, and the news headline.
    # print(f"Rank {i+1}: Category: {category}, Headline: {headline}")
    # Print the similarity score of the news headline.
    # print(f"Similarity score: {similarity_score[0][0]}")
    print()




#  Uncomment the line below if you are using Google Colab.
# !pip install -U sentence-transformers

# Import the SentenceTransformer class and utility function class from the sentence_transformers module 
from sentence_transformers import SentenceTransformer, util
# Use the `all-MiniLM-L6-v2` model.
model = SentenceTransformer('all-MiniLM-L6-v2')
# Import pandas
import pandas as pd
pd.set_option('max_colwidth', 200)

# Create a DataFrame for the "SMS_Ham_Spam.csv" 
sms_text_df = pd.read_csv("Resources/SMS_Ham_Spam.csv")
sms_text_df

#  Get the number of "ham" and "spam" from the "label" column:
sms_text_df['label'].value_counts()

# Create a DataFrame for the unclassified_text_messages.csv file of text messages. 
unclassified_texts_df = pd.read_csv("Resources/unclassified_text_messages.csv")
unclassified_texts_df

#  Get the number of "ham" and "spam" from the "label" column:
unclassified_texts_df['label'].value_counts()

# Convert the "text_message" column from the classified text messages DataFrame to a list.
classified_text_messages = sms_text_df["text_message"].tolist()
classified_text_messages

# Get the vector embeddings for the classified text messages.
classified_message_embeddings = model.encode(classified_text_messages)

# Convert the  "text_message" column from the unclassified text messages DataFrame to a list.
unclassified_text_messages = unclassified_texts_df["text_message"].tolist()
unclassified_text_messages

# Create embeddings for the unclassified text messages. 
unclassified_message_embeddings = model.encode(unclassified_text_messages)

# Create a list to store tuples of (unclassified message, top 5 similarity scores)
unclassified_similarities = []

# Create a list to store tuples of (classified message, similarity score)
classified_similarities = []

# Loop through each unclassified message.
# Use the `zip() function to pack the unclassified texts and their embeddings lists.
for unclassified_message, unclassified_message_embedding in zip(unclassified_text_messages, unclassified_message_embeddings):

    # Loop through the classified messages and compute cosine similarity.
    # Use the `zip() function to pack the classified messages and their embeddings lists.
    for classified_message, classified_message_embedding in zip(classified_text_messages, classified_message_embeddings):
        # Compute cosine similarity between the unclassified and classified embeddings
        cosine_similarity_score = util.cos_sim(unclassified_message_embedding.reshape(1, -1),
                                               classified_message_embedding.reshape(1, -1))[0, 0]

        # Store the classified message and similarity score as a tuple to the s a tuple `classified_similarities` list.
        classified_similarities.append((classified_message, cosine_similarity_score))

    # Sort the list of tuples on the similarity score in descending order
    classified_similarities.sort(key=lambda x: x[1], reverse=True)

    # Get the top 5 similarity scores
    top_5_similarities = classified_similarities[:5]

    # Store the unclassified message and top 5 similarity scores as a tuple to the `unclassified_similarities` list.
    unclassified_similarities.append((unclassified_message, top_5_similarities))

# Loop through the unclassified_similarities list and print the unclassified message.
for unclassified_message, top_5_similarities in unclassified_similarities:
    print(f"Unclassified Message: {unclassified_message}")
    print("Top 5 Similarities:")
    # Loop through the top five similarities and get the classified text message and its similarity score.
    for i, (classified_message, similarity_score) in enumerate(top_5_similarities):
        # Get the classification of the classified text message 
        label = sms_text_df.loc[sms_text_df['text_message'] == classified_message, 'label'].values[0]
        # Print the rank, label, and classified text message.
        print(f"Rank {i + 1}: Label: {label}, Message: {classified_message}")
        # Print the similarity score between the unclassified message and the classified message. 
        print(f"Similarity score: {similarity_score}")
        print()
    print()



#  Uncomment the line below if you are using Google Colab.
# !pip install -U sentence-transformers

# Import the SentenceTransformer class and utility function class from the sentence_transformers module 
from sentence_transformers import SentenceTransformer, util
# Use the `all-MiniLM-L6-v2` model.
model = SentenceTransformer('all-MiniLM-L6-v2')
# Import pandas
import pandas as pd
pd.set_option('max_colwidth', 200)

# Create a DataFrame for the "SMS_Ham_Spam.csv" 


#  Get the number of "ham" and "spam" from the "label" column:


# Create a DataFrame for the unclassified_text_messages.csv file of text messages. 


#  Get the number of "ham" and "spam" from the "label" column:


# Convert the "text_message" column from the classified text messages DataFrame to a list.


# Get the vector embeddings for the classified text messages.


# Convert the  "text_message" column from the unclassified text messages DataFrame to a list.


# Create embeddings for the unclassified text messages. 


# Create a list to store tuples of (unclassified message, top 5 similarity scores)
unclassified_similarities = []

# Create a list to store tuples of (classified message, similarity score)
classified_similarities = []

# Loop through each unclassified message.
# Use the `zip() function to pack the unclassified texts and their embeddings lists.


    # Loop through the unclassified messages and compute cosine similarity.
    # Use the `zip() function to pack the classified messages and their embeddings lists.
    
        # Compute cosine similarity between the unclassified and classified embeddings
        # cosine_similarity_score = util.cos_sim(unclassified_message_embedding.reshape(1, -1),
        #                                       classified_message_embedding.reshape(1, -1))[0, 0]

        # Store the classified message and similarity score as a tuple to the s a tuple `classified_similarities` list.
        

    # Sort the list of tuples on the similarity score in descending order
    # classified_similarities.sort(key=lambda x: x[1], reverse=True)

    # Get the top 5 similarity scores


    # Store the unclassified message and top 5 similarity scores as a tuple to the `unclassified_similarities` list.
    

# Loop through the unclassified_similarities list and print the unclassified message.



    # Loop through the top five similarities and get the classified text message and its similarity score.
    
        # Get the classification of the classified text message 
        
        # Print the rank, label, and classified text message.
        
        # Print the similarity score between the unclassified message and the classified message. 
        
        print()
    print()



#  Uncomment the line below if you are using Google Colab.
# !pip install transformers
# Import the pipeline class from the transformers module. 
from transformers import pipeline

# Initialize the pipeline to translate using the t5-base model. 
translator = pipeline("translation", model="t5-base")

# Define a English text and translate it to German. 
english_text = "I am celebrating my birthday."
text = f"translate English to German: {english_text}"
results = translator(text)
# Display the translation JSON data. 
print(results)
# Get the translated text.
results[0]['translation_text']

# Define a English text and translate it to French
english_text = "I am celebrating my birthday."
text = f"translate English to French: {english_text}"
results = translator(text)
results[0]['translation_text']



#  Uncomment the line below if you are using Google Colab.
# !pip install transformers

# Import the Autotokenizer class from the transformers module. 
from transformers import AutoTokenizer
# Create an instance of the Autotokenizer class using the t5-base model.
tokenizer = AutoTokenizer.from_pretrained("t5-base", max_length=50)

# Define text we want to translate.
english_text = "Hello, how are you today?"

# Retrieve the input IDs from the translation.
input_ids = tokenizer(f"translate English to French: {english_text}", return_tensors="tf").input_ids
input_ids

# Import the TFAutoModelForSeq2SeqLM class from the transformers module. 
from transformers import TFAutoModelForSeq2SeqLM

# Generate the numerical outputs from the model. 
translation_model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base", max_length=100)
output_ids = translation_model.generate(input_ids, max_new_tokens=100)
output_ids

# Decode the numerical outputs 
tokenizer.decode(output_ids[0])

# Retrieve the text from the special characters.
tokenizer.decode(output_ids[0], skip_special_tokens=True)



#  Uncomment the line below if you are using Google Colab.
# !pip install transformers

# Import the Autotokenizer class from the transformers module. 
from transformers import AutoTokenizer
# Create an instance of the Autotokenizer class using the t5-base model.
tokenizer = AutoTokenizer.from_pretrained("t5-base")

# Define text we want to translate.


# Retrieve the input IDs from the translation.
inputs = tokenizer(f"translate English to French: {english_text}", return_tensors="tf").input_ids
inputs

# Import the TFAutoModelForSeq2SeqLM class from the transformers module. 
from transformers import TFAutoModelForSeq2SeqLM

# Generate the numerical outputs from the model. 
translation_model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base")
outputs = translation_model.generate(inputs, max_new_tokens=100)
outputs

# Decode the numerical outputs 
tokenizer.decode(outputs[0])

# Retrieve the text from the special characters.
tokenizer.decode(outputs[0], skip_special_tokens=True)



#  Uncomment the line below if you are using Google Colab.
# !pip install transformers

# Import the pipeline class from the transformers module. 
from transformers import pipeline

# Initialize the pipeline to translate using the t5-base model. 
translator = pipeline("translation", model="t5-base")

# Define a English text and translate it to German. 


# Define a English text and translate it to French.  

#  Uncomment the line below if you are using Google Colab.
# !pip install transformers

# Import the Autotokenizer and TFAutoModelForSeq2SeqLM classes from the transformers module.
from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM

# Create an instance of the Autotokenizer class using the t5-base model.
tokenizer = AutoTokenizer.from_pretrained("t5-base", model_max_length=256)

# The news headlines to translate.
headlines = [
    'How To Spend More Time With Your Family While Working Remotely',
    'NCAA Football Playoffs Should be Like the NFL',
    'Hacker Pleads Guilty To Stealing Over 100,000 Passwords for Reddit',
    'Lawmakers Want To Boost School Funding To Address Teacher Walkouts',
    'The Best Sub Shops in the Caribbean You Should Visit This Summer',
    'The Dark Side Of The Bitcoin Mining',
    'Treasury Secretary is Confirmed Today',
    'The 5 Best Restaurants In The World',
    'How to Build a Brand for Your Small Business',
    'NY Giants Quarterback Injured After Being Punched By Teammate']

# Create a list to hold the input ids. 
headline_input_ids =[]
# Retrieve the input ids from each headline translation using the translate function.
def create_input_ids(headline):
    # Get input ids using the translate prompt for each headline.
    input_ids = tokenizer(f"translate English to French: {headline}", return_tensors="tf").input_ids
    # Append each input id to the list and return the list.
    return headline_input_ids.append(input_ids)

# Use a for loop to pass each headline to the `create_input_ids` function to create the input ids.
for headline in headlines:
    create_input_ids(headline)

# Print the headline input ids array
print(headline_input_ids)

# Create an instance of the TFAutoModelForSeq2SeqLM class using the t5-base model.
translation_model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Create a list to hold the translated headlines.
translated_headlines = []
# Use the decode function to generate the numerical outputs from the model.
def decode(input_id):
    # Create the output id from the input id
    output_id = translation_model.generate(input_id, max_new_tokens=100)
    # Append each decoded output_id, i.e., translation to the list. 
    translated_headlines.append(tokenizer.decode(output_id[0], skip_special_tokens=True))
    # Return the list. 
    return translated_headlines

# Use a for loop to pass each input id to the decode function. 
for input_id in headline_input_ids:
    decode(input_id)

# Print out each translated headline.
for translation in translated_headlines:
    print(translation)

# Import the pipeline class from the transformers module. 
from transformers import pipeline
# Initialize the pipeline to translate using the t5-base model. 
translator = pipeline("translation", model="t5-base")

# Create a list to hold the translated headlines 
translated_headlines = []
# Use the translate function to translate each English headline into French.
def translate(headline):
    # Set the translate prompt to a variable
    text = f"translate English to French: {headline}"
    # Pass the translated variable to the translator method.
    results = translator(text)
    # Return the list with the translated text.
    return translated_headlines.append(results[0]['translation_text'])

# Use a for loop to pass each headline to the translate function.
for headline in headlines:
    translate(headline)

# Print out each translated headline.
for translation in translated_headlines:
    print(translation)



#  Uncomment the line below if you are using Google Colab.
# !pip install transformers

# Import the Autotokenizer and TFAutoModelForSeq2SeqLM classes from the transformers module.
from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM

# Create an instance of the Autotokenizer class using the t5-base model.
tokenizer = AutoTokenizer.from_pretrained("t5-base", model_max_length=256)

# The news headlines to translate.
headlines = [
    'How To Spend More Time With Your Family While Working Remotely',
    'NCAA Football Playoffs Should be Like the NFL',
    'Hacker Pleads Guilty To Stealing Over 100,000 Passwords for Reddit',
    'Lawmakers Want To Boost School Funding To Address Teacher Walkouts',
    'The Best Sub Shops in the Caribbean You Should Visit This Summer',
    'The Dark Side Of The Bitcoin Mining',
    'Treasury Secretary is Confirmed Today',
    'The 5 Best Restaurants In The World',
    'How to Build a Brand for Your Small Business',
    'NY Giants Quarterback Injured After Being Punched By Teammate']

# Create a list to hold the input ids. 

# Retrieve the input IDs from each headline translation using the translate function.
def create_input_ids():
    # Get input ids using the translate prompt for each headline.
    
    # Append each input id to the list and return the list.
    

# Use a for loop to pass each headline to the `create_input_ids` function to create the input ids.


# Print the headline input ids array


# Create an instance of the TFAutoModelForSeq2SeqLM class using the t5-base model.
translation_model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Create a list to hold the translated headlines.

# Use the decode function to generate the numerical outputs from the model.
def decode(input_id):
    # Create the output id from the input id.
    
    # Append each decoded output_id, i.e., translation to the list. 
    
    # Return the list. 


# Use a for loop to pass each input id to the decode function. 


# Print out each translated headline.


# Import the pipeline class from the transformers module. 
from transformers import pipeline
# Initialize the pipeline to translate using the t5-base model. 
translator = pipeline("translation", model="t5-base")

# Create a list to hold the translated headlines 

# Use the translate function to translate each English headline into French.
def translate(headline):
    # Set the translate prompt to a variable
   
    # Pass the translated variable to the translator method.
    
    # Return the list with the translated text.
    

# Use a for loop to pass each headline to the translate function.


# Print out each translated headline.




#  Uncomment the line below if you are using Google Colab.
# !pip install transformers

# Import the pipeline class from the transformers module. 
from transformers import pipeline

# Use the text-generation parameter for the pipeline and EleutherAI/gpt-neo-1.3B model. 
generator = pipeline('text-generation', model='EleutherAI/gpt-neo-1.3B')

# Give the model a prompt. 
prompt = "I like gardening because"
# Pass the prompt to the generator
results = generator(prompt, max_length=125, pad_token_id=50256)
# Get the text based on the prompt. 
generated_text = results[0]['generated_text']
# Print the generated text.
print(generated_text)

# Use the text-generation parameter for the pipeline and EleutherAI/gpt-neo-125m model. 
small_generator = pipeline('text-generation', model='EleutherAI/gpt-neo-125m')

# Give the model a prompt. 
prompt = "My favorite animal is the cat because "
# Pass the prompt to the generator. Use `max_length=25`.
new_results = small_generator(prompt, max_length=25, pad_token_id=50256)
# Get the text based on the prompt. 
generated_text = new_results[0]['generated_text']
# Print the generated text.
print(generated_text)



#  Uncomment the line below if you are using Google Colab.
# !pip install transformers

# Import the pipeline class from the transformers module. 
from transformers import pipeline

# Use the text-generation parameter for the pipeline and EleutherAI/gpt-neo-1.3B model. 
generator = pipeline('text-generation', model='EleutherAI/gpt-neo-1.3B')

# Give the model a prompt. 
prompt = "I like gardening because"
# Pass the prompt to the generator

# Get the text based on the prompt. 

# Print the text that is generated. 

# Use the text-generation parameter for the pipeline and EleutherAI/gpt-neo-125m model. 
small_generator = pipeline('text-generation', model='EleutherAI/gpt-neo-125m')

# Give the model a prompt. 
prompt = "My favorite animal is the cat because"
# Pass the prompt to the generator. Use a `max_length=25`. 

# Get the text based on the prompt. 

# Print the text that is generated. 




#  Uncomment the line below if you are using Google Colab.
# !pip install transformers

# Import the pipeline class from the transformers module. 
from transformers import pipeline

# Give the model a prompt. 
prompt = """All the world's a stage and we are"""

# Use the text_generator function to generate text
def text_generator(model, prompt):
    # Initialize the pipeline with the task and model
    generator = pipeline('text-generation', model=model)
    # Pass the prompt, set the max_length and pad_token_id parameters for generator.
    results = generator(prompt,max_length=125, pad_token_id=50256)
    # Return the generated text. 
    return results[0]['generated_text']

# Call the text_generator function with your first model and the prompt.
small_generator = text_generator('EleutherAI/gpt-neo-125m', prompt)
# Display the generated text.
small_generator

# Call the text_generator function with the second model and the prompt.
medium_generator = text_generator('EleutherAI/gpt-neo-1.3B', prompt)
# Display the generated text.
medium_generator

# Call the text_generator function with the third model and the prompt.
large_generator = text_generator('EleutherAI/gpt-neo-2.7B', prompt)
# Display the generated text.
large_generator



#  Uncomment the line below if you are using Google Colab.
# !pip install transformers

# Import the pipeline class from the transformers module. 
from transformers import pipeline

# Give the model a prompt. 
prompt = """<>"""

# Use the text_generator function to generate text
def text_generator(model, prompt):
    # Initialize the pipeline with the task and model
    
    # Pass the prompt, set the max_length and pad_token_id parameters for generator.
    
    # Return the generated text. 
   

# Call the text_generator function with your first model and the prompt.

# Display the generated text. 


# Call the text_generator function with a second model and the prompt.

# Display the generated text. 


# Call the text_generator function with a third model and the prompt.

# Display the generated text. 




#  Uncomment the line below if you are using Google Colab.
# !pip install transformers

# Import the pipeline class from the transformers module. 
from transformers import pipeline
# Initialize the pipeline to generate questions and answers using the distilbert-base-cased-distilled-squad model. 
question_answerer = pipeline("question-answering", model='distilbert-base-cased-distilled-squad')

# Source: https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)
text = """
A transformer is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data. It is used primarily in the fields of natural language processing (NLP)[1] and computer vision (CV).[2]

Like recurrent neural networks (RNNs), transformers are designed to process sequential input data, such as natural language, with applications towards tasks such as translation and text summarization. However, unlike RNNs, transformers process the entire input all at once. The attention mechanism provides context for any position in the input sequence. For example, if the input data is a natural language sentence, the transformer does not have to process one word at a time. This allows for more parallelization than RNNs and therefore reduces training times.[1]

Transformers were introduced in 2017 by a team at Google Brain[1] and are increasingly becoming the model of choice for NLP problems,[3] replacing RNN models such as long short-term memory (LSTM). The additional training parallelization allows training on larger datasets. This led to the development of pretrained systems such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), which were trained with large language datasets, such as the Wikipedia Corpus and Common Crawl, and can be fine-tuned for specific tasks.[4][5]
"""


# Generate a list of questions.
questions = ["When were transformers first introduced?",
             "What are transformers better than?",
             "What are applications of transformers?"]

# Check the output from one question.
question = "When were transformers first introduced?"
# Pass the first question and text to the question_answerer.
result = question_answerer(question=question, context=text)
# Show the results
result

# Import Pandas 
import pandas as pd

# Create a function to generate the answers based on an input text.
def question_answer(questions, text):
    # Create a list to hold the data that will be added to the DataFrame.
    data = []
    # Use a for loop to iterate through the questions.
    for question in questions:
        # Pass the question and text to the initialized question_answerer. 
        result = question_answerer(question=question, context=text)
        # Retrieve the question, answer, the score, the starting 
        # and ending of where the answer is located in the text.
        data.append([question, result['answer'], result['score'], result['start'], result['end']])
    # Create a DataFrame from the data with appropriate columns. 
    df = pd.DataFrame(data, columns=["Question", "Answer", "Score", "Starting Position", "Ending Position"])
    # Return the DataFrame
    return df

# Call the question_answer function with the questions and text.
question_answer(questions, text)



#  Uncomment the line below if you are using Google Colab.
# !pip install transformers

# Import the pipeline class from the transformers module. 
from transformers import pipeline
# Initialize the pipeline to generate questions and answers using the distilbert-base-cased-distilled-squad model. 
question_answerer = pipeline("question-answering", model='distilbert-base-cased-distilled-squad')

# Source: https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)
text = r"""
A transformer is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data. It is used primarily in the fields of natural language processing (NLP)[1] and computer vision (CV).[2]

Like recurrent neural networks (RNNs), transformers are designed to process sequential input data, such as natural language, with applications towards tasks such as translation and text summarization. However, unlike RNNs, transformers process the entire input all at once. The attention mechanism provides context for any position in the input sequence. For example, if the input data is a natural language sentence, the transformer does not have to process one word at a time. This allows for more parallelization than RNNs and therefore reduces training times.[1]

Transformers were introduced in 2017 by a team at Google Brain[1] and are increasingly becoming the model of choice for NLP problems,[3] replacing RNN models such as long short-term memory (LSTM). The additional training parallelization allows training on larger datasets. This led to the development of pretrained systems such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), which were trained with large language datasets, such as the Wikipedia Corpus and Common Crawl, and can be fine-tuned for specific tasks.[4][5]
"""


# Generate a list of questions.
questions = ["When were transformers first introduced?",
             "What are transformers better than?",
             "What are applications of transformers?"]

# Check the output from one question.

# Pass the first question and text to the question_answerer.

# Show the results


# Import Pandas 
import pandas as pd

# Create a function to generate the answers based on an input text.
def question_answer(questions, text):
    # Create a list to hold the data that will be added to the DataFrame.
    
    # Use a for loop to iterate through the questions.
    
        # Pass the question and text to the initialized question_answerer. 
        
        # Retrieve the question, answer, the score, the starting 
        # and ending of where the answer is located in the text.
        
    # Create a DataFrame from the data with appropriate columns. 
    
    # Return the DataFrame
    

# Call the question_answer function with the questions and text.




#  Uncomment the line below if you are using Google Colab.
# !pip install transformers

# Import the pipeline class from the transformers module. 
from transformers import pipeline
# Initialize the pipeline to generate questions and answers using the distilbert-base-cased-distilled-squad model. 
question_answerer = pipeline("question-answering", model='distilbert-base-cased-distilled-squad')

# Read in the text file with the read_file function.
filepath = "Resources/video_game_history.txt"
with open(filepath) as f:
    video_game_history = f.read().replace("\n"," ")

# Source: https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)
video_game_history

# Generate a list of questions.
questions = ["When did Nintendo released its Nintendo Entertainment System in the United States?",
             "What was the first home video game?",
             "When did internet gaming start?"]

# Check the output from one question.
question = "When did Nintendo released its Nintendo Entertainment System in the United States?"
# Pass the first question and text to the question_answerer.
result = question_answerer(question=question, context=video_game_history)
# Show the results
result

# Import Pandas 
import pandas as pd

# Create a function to generate the answers based on an input text.
def question_answer(questions, text):
    # Create a list to hold the data that will be added to the DataFrame.
    data = []
    # Use a for loop to iterate through the questions.
    for question in questions:
        # Pass the question and text to the initialized question_answerer. 
        result = question_answerer(question=question, context=text)
        # Retrieve the question, answer, the score, the starting 
        # and ending of where the answer is located in the text.
        data.append([question, result['answer'], result['score'], result['start'], result['end']])
    # Create a DataFrame from the data with appropriate columns. 
    df = pd.DataFrame(data, columns=["Question", "Answer", "Score", "Starting Position", "Ending Position"])
    # Return the DataFrame
    return df

# Call the question_answer function with the questions and text.
question_answer(questions, video_game_history)



#  Uncomment the line below if you are using Google Colab.
# !pip install transformers

# Import the pipeline class from the transformers module. 
from transformers import pipeline
# Initialize the pipeline to generate questions and answers using the distilbert-base-cased-distilled-squad model. 
question_answerer = pipeline("question-answering", model='distilbert-base-cased-distilled-squad')

# Provide text for the question and answering system.
text = """<>"""


# Generate a list of questions.
questions = []

# Check the output from one question.

# Pass the first question and text to the question_answerer.

# Show the results


# Import Pandas 
import pandas as pd

# Create a function to generate the answers based on an input text.
def question_answer(questions, text):
    # Create a list to hold the data that will be added to the DataFrame.
    
    # Use a for loop to iterate through the questions.
    
        # Pass the question and text to the initialized question_answerer. 
        
        # Retrieve the question, answer, the score, the starting 
        # and ending of where the answer is located in the text.
        
    # Create a DataFrame from the data with appropriate columns. 
    
    # Return the DataFrame
    

# Call the question_answer function with the questions and text.




#  Uncomment the line below if you are using Google Colab.
# !pip install transformers

# Import the pipeline class from the transformers module. 
from transformers import pipeline
# Instantiate the pipeline class for summarization using the facebook/bart-large-cnn model.
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# Create a variable to contain the text from (https://en.wikipedia.org/wiki/Deep_learning) to summarize.
article ="""Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.[2] 

Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.[3][4][5]

Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog.[6][7]

The adjective "deep" in deep learning refers to the use of multiple layers in the network. Early work showed that a linear perceptron cannot be a universal classifier, but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can. Deep learning is a modern variation that is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability."""

# Get the most likely summary of the article using "False" for the `do_sample` parameter.
most_likely_summary = summarizer(article, 
                     min_length=30, 
                     max_length=130, 
                     do_sample=False)

# Display the summary
most_likely_summary

# Get the summary text from the JSON output
most_likely_summary[0]["summary_text"]

# Get a more diverse summary of the article using "True" for the `do_sample` parameter.
diverse_summary = summarizer(article, 
                     min_length=30, 
                     max_length=130, 
                     do_sample=True)[0]["summary_text"]

# Display the summary
diverse_summary



#  Uncomment the line below if you are using Google Colab.
# !pip install transformers

# Import the pipeline class from the transformers module. 
from transformers import pipeline
# Instantiate the pipeline class for summarization using the facebook/bart-large-cnn model.
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# Create a variable to contain the text from (https://en.wikipedia.org/wiki/Deep_learning) to summarize.
article ="""Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.[2] 

Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.[3][4][5]

Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog.[6][7]

The adjective "deep" in deep learning refers to the use of multiple layers in the network. Early work showed that a linear perceptron cannot be a universal classifier, but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can. Deep learning is a modern variation that is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability."""

# Get the most likely summary of the article using "False" for the `do_sample` parameter.


# Display the summary


# Get the summary text from the JSON output


# Get a more diverse summary of the article using "True" for the `do_sample` parameter.


# Display the summary




#  Uncomment the line below if you are using Google Colab.
# !pip install transformers

# Import the pipeline class from the transformers module. 
from transformers import pipeline
# Instantiate the pipeline class for summarization using the facebook/bart-large-cnn model.
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# Read in the text file with the read_file function.
# https://en.wikipedia.org/wiki/History_of_artificial_intelligence
filepath = "Resources/AI_history.txt"
with open(filepath) as f:
    ai_text = f.read().replace('\n',' ')

# Print the first 1000 items in the AI history text 
ai_text[0:1000]

len(ai_text)

# Get the most likely summary of the article using "False" for the `do_sample` parameter.
most_likely_summary = summarizer(ai_text, 
                     min_length=50, 
                     max_length=150, 
                     do_sample=False)

# Display the summary
most_likely_summary[0]["summary_text"]

# Get a more diverse summary of the article using "True" for the `do_sample` parameter.
diverse_summary = summarizer(ai_text, 
                     min_length=50, 
                     max_length=150, 
                     do_sample=True)[0]["summary_text"]

# Display the summary
diverse_summary



#  Uncomment the line below if you are using Google Colab.
# !pip install transformers

# Import the pipeline class from the transformers module. 
from transformers import pipeline
# Instantiate the pipeline class for summarization using the facebook/bart-large-cnn model.
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# Create a variable to contain the text from you want to summarize.

# Get the most likely summary of the article using "False" for the `do_sample` parameter.
# Set the minimum output length to 50, and maximum output length to 150.

# Display the summary


# Get a more diverse summary of the article using "True" for the `do_sample` parameter.
# Set the minimum output length to 50, and maximum output length to 150.


# Display the summary




# Uncomment the following code if you are using Colab.
# !pip install gradio

# Create a function that takes in a message. 
def run(msg):
    return f'Returning this message: {msg}'

# Define an input for the message you want to send.
message = input("What is the message you want to send? ")

# Call the function.
new_message = run(message)
print(new_message)

# Import gradio 
import gradio as gr

# Create a function that takes in a message. 
def run(msg):
    return f'Returning this message: {msg}'

# Create an instance of the Gradio Interface application function with the following parameters. 
app = gr.Interface(fn=run, inputs="text", outputs="text")

# Launch the app
app.launch()

# Create a function that takes in a message. 
def run(msg):
    return f'Returning this message: {msg}'

# Create an instance of the Gradio Interface application function with the following parameters. 
app = gr.Interface(fn=run, inputs="text", outputs="text")

# Launch the app
app.launch(share=True)

# Create a savings interest calculator function that takes 
# the balance, apr, and number of days, and returns the interest for the number of days. 
def calculate_interest(balance, apr, days):
    """
    Calculate the interest earned on a balance based on the annual percentage rate (APR) over a specified number of days.

    Parameters:
    - balance (float): The initial balance or principal amount.
    - apr (float): The Annual Percentage Rate (APR) as a percentage.
    - days (int): The number of days for which interest is calculated.

    Returns:
    float: The interest earned rounded to 2 decimal places.
    """
    apr_decimal = apr/100
    interest_rate = apr_decimal * (days/365)
    interest_earned = balance * interest_rate
    return round(interest_earned,2)

# Create an instance of the Gradio Interface application function with the following parameters. 
app = gr.Interface(fn=calculate_interest, 
                   inputs=["number","number", 'number'], outputs="number")
# Launch the app
app.launch()



# Uncomment the following code if you are using Colab.
# !pip install gradio

# Create a function that takes in a message. 
def run(msg):
    return f'Returning this message: {msg}'

# Define an input for the message you want to send.


# Call the function.


# Import gradio 
import gradio as gr

# Create a function that takes in a message. 
def run(msg):
    

# Create an instance of the Gradio Interface application function with the following parameters. 


# Launch the app
app.launch()

# Create a function that takes in a message. 
def run(msg):


# Create an instance of the Gradio Interface application function with the following parameters. 


# Launch the app
app.launch(share=True)

# Create a savings interest calculator function that takes 
# the balance, apr, and number of days, and returns the interest for the number of days. 
def calculate_interest(balance, apr, days):
    """
    Calculate the interest earned on a balance based on the annual percentage rate (APR) over a specified number of days.

    Parameters:
    - balance (float): The initial balance or principal amount.
    - apr (float): The Annual Percentage Rate (APR) as a percentage.
    - days (int): The number of days for which interest is calculated.

    Returns:
    float: The interest earned rounded to 2 decimal places.
    """
    

# Create an instance of the Gradio Interface application function with the following parameters. 

# Launch the app
app.launch()



# Uncomment the following code if you are using Colab.
# !pip install gradio

# Import gradio 
import gradio as gr

# Create a pizza order function that takes 
# the size of the pizza and up to three toppings, and returns the price of the pizza with tax. 
def pizza_order(size, topping_1, topping_2, topping_3): 
     """
    Calculate the total cost of a pizza order based on size and includes up to three toppings.

    Parameters:
    - size (str): Size of the pizza (options: "large", "medium", "small").
    - topping_1 (bool): True if topping 1 field has been filled in, False otherwise.
    - topping_2 (bool): True if topping 2 field has been filled in, False otherwise.
    - topping_3 (bool): True if topping 3 field has been filled in, False otherwise.

    Returns:
    str: A formatted string indicating the total cost of the pizza order, including tax.
    """
    # Get the price for the size of pizza. 
    
        
    # Set the price of the toppings to zero. 
    
    
    # Use conditionals to add up the price for each topping if a topping has been added.
    
       
    
    # Add the size_price and topping_price and calculate the tax (7%) of the order.

    
    # Return the pizza price to two decimal places
    
        
# Create an instance of the gradio Interface application function with parameters. 
app = gr.Interface()

# Launch the app
app.launch()



# Uncomment the following code if you are using Google Colab.
# ! pip install transformers
# ! pip install gradio

# Import transformers pipeline
from transformers import pipeline
# Import Gradio
import gradio as gr

# Instantiate the pipeline with the summarization parameter and `facebook/bart-large-cnn` model. 
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# Create the summarize function passing in the desired parameter. 
def summarize(article):
    return f'{summarizer(article, min_length=30, max_length=200, do_sample=False)[0]["summary_text"]}'

# Create an instance of the Gradio Interface application function with the appropriate parameters. 
app = gr.Interface(fn=summarize, inputs="text", outputs="text")
# Launch the app
app.launch()



# Uncomment the following code if you are using Google Colab.
# ! pip install transformers
# ! pip install gradio

# Import transformers pipeline
from transformers import pipeline
# Import Gradio
import gradio as gr

# Instantiate the pipeline with the summarization parameter and `facebook/bart-large-cnn` model. 
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# Create the summarize function passing in the desired parameter. 
def summarize(article):
    

# Create an instance of the Gradio Interface application function with the appropriate parameters. 

# Launch the app
app.launch()



# Uncomment the following code if you are using Google Colab.
# ! pip install transformers
# ! pip install gradio

# Import transformers pipeline
from transformers import pipeline
# Import Gradio
import gradio as gr

# Instantiate the pipeline class for summarization using the facebook/bart-large-cnn model.
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# Create the summarize function passing in the desired parameter for the most likely summary of the article. 
def summarize(article):
    

# Create an instance of the Gradio Interface application function with the appropriate parameters. 

# Launch the app


# Create the summarize function passing in the desired parameter for a more diverse summary of the article. 
def summarize(article):
    

# Create an instance of the Gradio Interface application function with the appropriate parameters. 

# Launch the app




# Uncomment the following code if you are using Google Colab.
# ! pip install transformers
# ! pip install gradio

# Import transformers pipeline
from transformers import pipeline
# Import Gradio
import gradio as gr

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# Create a summary function passing in the desired parameters. 
def summarize(article, max_output):
    return f'{summarizer(article, max_length=max_output, min_length=30, do_sample=False)[0]["summary_text"]}'

# Create an instance of the Gradio Interface application function with parameters. 
app = gr.Interface(fn=summarize, 
                   title="Text Summarizer using Transformers",
                   inputs=["text", "number"], 
                   outputs=gr.Textbox(lines=20, label="Summarized Text Output", show_copy_button=True))
# Launch the app
app.launch()

# Set the default maximum output length to 150 words.
max_output = gr.Number(value=150)
app = gr.Interface(fn=summarize, 
                   title="Text Summarizer using Transformers",
                   inputs=["text", max_output], 
                   outputs=gr.Textbox(lines=20, label="Summarized Text Output", show_copy_button=True))

# Launch the app
app.launch()

# Create a summary function passing in the desired parameters.
def summarize(article, min_length, max_length):
    return f'{summarizer(article, max_length=max_length, min_length=min_length, do_sample=False)[0]["summary_text"]}'

# Create an instance of the Gradio Interface application function with parameters. 
app = gr.Interface(fn=summarize, title="Text Summarizer using Transformers",
                   inputs=[
                       gr.Textbox(lines=5, placeholder="Enter the original text to be summarized", label="Text Input Field", interactive=True),
                       gr.Slider(10, 30, value=10, step=10, label="Minimum number of words in Summarized Article", info="Choose between 10 and 30"),
                       gr.Slider(120, 150, value=120, step=10, label="Maximum number of words in Summarized Article", info="Choose between 120 and 150"),
                     ],
                     outputs=gr.Textbox(lines=10, label="Summarized Text Output", show_copy_button=True))

# Launch the app.
app.launch()



# Uncomment the following code if you are using Google Colab.
# ! pip install transformers
# ! pip install gradio

# Import transformers pipeline
from transformers import pipeline
# Import Gradio
import gradio as gr

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# Create a summary function passing in the desired parameters. 


# Create an instance of the Gradio Interface application function with parameters. 


# Launch the app
app.launch()

# Set the default maximum output length to 150 words.

# Launch the app
app.launch()

# Create a summary function passing in the desired parameters.


# Create an instance of the Gradio Interface application function with parameters. 


# Launch the app.
app.launch()



# Uncomment these lines if you are using Google Colab.
# ! pip install transformers
# ! pip install gradio

# Import transformers pipeline
from transformers import pipeline
# Import Gradio
import gradio as gr

# Initialize the pipeline to generate questions and answers using the distilbert-base-cased-distilled-squad model. 


# Create a function called `question_answer()` that takes two parameters, the text to search and a question.
# The function should return the question, answer, probability score, and the starting and ending index of the answer.


# Create the app with two Textbox components. 
# The first textbox will take the text to search the second will take the question.
# The output should show the question, answer, probability score, and the starting and ending index of the answer.


    
# Launch the app.
app.launch(show_error=True)



!pip install transformers

# Import the pipeline class from the transformers module. 
from transformers import pipeline
# Initialize the pipeline to generate questions and answers using the distilbert-base-cased-distilled-squad model. 
question_answerer = pipeline("question-answering", model='distilbert-base-cased-distilled-squad')

# Source: https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)
text = r"""
A transformer is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data. It is used primarily in the fields of natural language processing (NLP)[1] and computer vision (CV).[2]

Like recurrent neural networks (RNNs), transformers are designed to process sequential input data, such as natural language, with applications towards tasks such as translation and text summarization. However, unlike RNNs, transformers process the entire input all at once. The attention mechanism provides context for any position in the input sequence. For example, if the input data is a natural language sentence, the transformer does not have to process one word at a time. This allows for more parallelization than RNNs and therefore reduces training times.[1]

Transformers were introduced in 2017 by a team at Google Brain[1] and are increasingly becoming the model of choice for NLP problems,[3] replacing RNN models such as long short-term memory (LSTM). The additional training parallelization allows training on larger datasets. This led to the development of pretrained systems such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), which were trained with large language datasets, such as the Wikipedia Corpus and Common Crawl, and can be fine-tuned for specific tasks.[4][5]
"""


# Generate a list of questions.
questions = ["When were transformers first introduced?",
             "What are transformers better than?",
             "What are applications of transformers?"]

# Import Pandas 
import pandas as pd

# Create a function to generate the answers based on an input text.
def question_answer(questions, text):
    data = []
    for question in questions:
        result = question_answerer(question=question, context=text)
        data.append([question, result['answer'], result['score'], result['start'], result['end']])
    df = pd.DataFrame(data, columns=["Question", "Answer", "Score", "Starting Position", "Ending Position"])
    return df

# Call the question_answer function with the questions and text.
question_answer(questions, text)

# Check the output from one question.
question = "When were transformers first introduced?"
result = question_answerer(question=question, context=text)
result

# Use indexing to get the answer in the text to the first question.

