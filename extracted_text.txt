### Read CSV in as DataFrame

### Index Selection Using iloc

### Assignment Using iLoc

### Index Selection Using Loc

### Assignment Using Loc

### Read CSV in as DataFrame

### Index Selection Using iloc

### Assignment Using iLoc

### Index Selection Using Loc

### Assignment Using Loc

## Summary
There are more Batman comics than Superman comics in the dataset.



## Summary
There are more Batman comics than Superman comics in the dataset.



# Spring Cleaning!

Harold's stock data is a mess! Help him clean up his data before the auditors arrive!

### Load CSV data into Pandas using `read_csv`

### Identify the number of rows and columns (shape) in the DataFrame.

### Generate a sample of the data to visually ensure data has been loaded in correctly.

### Identify the number of records in the DataFrame, and compare it with the number of rows in the original file.

### Identify nulls records

### Drop Null Records

### Validate nulls have been dropped

### Default null `ebitda` values to 0. Then, validate no records are null for ebitda.

### Drop Duplicates

### Clean `price` Series by replacing `$`

### Confirm data type of `price`

### Cast `price` Series as float

# Spring Cleaning!

Harold's stock data is a mess! Help him clean up his data before the auditors arrive!

### Load CSV data into Pandas using `read_csv`

### Identify the number of rows and columns (shape) in the DataFrame.

### Generate a sample of the data to visually ensure data has been loaded in correctly.

### Identify the number of records in the DataFrame, and compare it with the number of rows in the original file.

### Identify nulls records

### Drop Null Records

### Validate nulls have been dropped

### Default null `ebitda` values to 0. Then, validate no records are null for ebitda.

### Drop Duplicates

### Clean `price` Series by replacing `$`

### Confirm data type of `price`

### Cast `price` Series as float and then validate using `dtype`

# Question: 
### Which utility's usage changed the most from 2013 to 2018?

# Summary
The number of passengers using the airport grew 28% in just 5 short years! Water consumption rose slightly at 4.9%, but despite the increase in airport traffic, Electricity and Gas usage were both down in 2018 compared to 2013, with Gas leading the charge with a 6.8% decline. This is a major victory in terms of emissions, and it also helps the bottom line due to cost reduction. We didn't research how the data was collected, and it is possible that the data is incomplete or biased in some unknown way. We could improve our analysis by digging deeper into the source of the data to better understand where it comes from.

# Question: 
### Which utility's usage changed the most from 2013 to 2018?

# Summary
{Write your summary here.}



# Question: 
### Which utility's usage changed the most from 2013 to 2018?

# Summary
The number of passengers using the airport grew 28% in just 5 short years! Water consumption rose slightly at 4.9%, but despite the increase in airport traffic, Electricity and Gas usage were both down in 2018 compared to 2013, with Gas leading the charge with a 6.8% decline. 

#### Import Libraries and Dependencies 

#### Read in Files 

#### Output of sample data

#### Concatenate data by rows using concat function and inner join

#### Concatenate data by columns using concat function and inner join

#### Concatenate data by rows using concat function and inner join and provide the stock as keys.

#### Combine the DataFrames horizontally using the `axis="columns"`.

#### Import Libraries and Dependencies 

#### Read in Files 

#### Output of sample data

#### Concatenate data by rows using concat function and inner join

#### Concatenate data by columns using concat function and inner join

#### Concatenate data by rows using concat function and inner join and provide the stock as keys.

#### Combine the DataFrames horizontally using the `axis="columns"`.

### Read in files

### Concatenate data by rows using `concat` function and `inner` join

### Concatenate data by rows using `concat` function and `inner` join and make each country a key.

### Display the customer and products DataFrames.

### Import Libraries and Dependencies

### Read in files

### Output sample of data

### Concatenate data by rows using `concat` function and `inner` join

### Concatenate data by rows using `concat` function and `inner` join and make each country a key.

### Display the customer and products DataFrames.

### Join the 2018 wheat data with the 2019 wheat data. 

### Join the data without setting the `index_col` parameter to "Country". 

### Join the 2018 wheat data with the 2019 wheat data. 

### Join the data without setting the `index_col` parameter to "Country". 

### Join 2016 to 2017. 

### Join 2018, 2019, and 2020 data to the 2016 and 2017 data.

### Drop all the columns with "year_" and sort the DataFrame.

### Join 2016 to 2017. 

### Join 2018, 2019, and 2020 data to the 2016 and 2017 data.

### Drop all the columns with "year_" and sort the DataFrame.

#### Import Libraries and Dependencies 

#### Read in Files 

#### Output of sample data

### Merge the Apple and Google data on the date.

### Merge all three DataFrames.

### Create a Summary DataFrame that has the best opening and closing, and the total volume in millions for each stock.


#### Import Libraries and Dependencies 

#### Read in Files 

#### Output of sample data

### Merge the Apple anbd Google data on the date.

### Merge all three DataFrames.

### Create a Summary DataFrame that has the best opening and closing, and the total volume in millions for each stock.


### Merge the 2018 wheat data with the 2019 wheat data. 

### Merge the 2018 wheat data with the 2019 wheat data. 

### Multiple Aggregations

### Grouping on Multiple Columns with Multiple Aggregations

### Flattening Multi-Indexed Columns to Single Columns

### Multiple Aggregations

### Grouping on Multiple Columns with Multiple Aggregations

### Flattening Multi-Indexed Columns to Single Columns

### Grouping and aggregating wiht multiple columns and functions.

### Grouping and aggregating wiht multiple columns and functions.

### Using the `pivot()` function.
---
`pd.pivot(data, columns, index=<a column>, values=<a column>` or `df.pivot(columns, index=<a column>, values=<a column>`


### Using the `pivot_table()` function.
---

- `pd.pivot_table(data, values=None, index=None, columns=None, aggfunc='mean', fill_value=None, margins=False, dropna=True, margins_name='All', observed=False, sort=True)`
- `pivot_table()` performs the mean aggregation by defualt

### Mutiple aggregations.

### Using the `pivot()` function.
---
`pd.pivot(data, columns, index=<a column>, values=<a column>` or `df.pivot(columns, index=<a column>, values=<a column>`


### Using the `pivot_table()` function.
---

- `pd.pivot_table(data, values=None, index=None, columns=None, aggfunc='mean', fill_value=None, margins=False, dropna=True, margins_name='All', observed=False, sort=True)`
- `pivot_table()` performs the mean aggregation by defualt

### Mutiple aggregations.

### Single Index (review)

### Multi-Indexing

### Multi-Index and Multi-Aggregations

### Single Index (review)

### Multi-Indexing

### Multi-Index and Multi-Aggregations

### Applying the `resample()` Function

### Applying the `melt()` Function

### Applying the `resample()` Function

### Applying the `melt()` Function

### Apply the `resample()` Function

### Apply the `melt()` Function

### Apply the `resample()` Function

### Apply the `melt()` Function

# Sourcing Data from HTML Tables with Pandas

We can use the `read_html` function in Pandas to automatically extract any tabular data from a page.

#### What we get in return is a list of dataframes for any tabular data that Pandas found.

#### We can slice off any of those dataframes that we want using normal indexing.

#### Fix column names

#### Drop a column

#### Reset an index

## Export DataFrame as CSV

# Sourcing Data from HTML Tables with Pandas

We can use the `read_html` function in Pandas to automatically extract any tabular data from a page.

#### What we get in return is a list of DataFrames for any tabular data that Pandas found.

#### We can slice off any of those dataframes that we want using normal indexing.

#### Fix column names

#### Drop a column

#### Reset an index

## Export DataFrame as CSV

# World Cup Stats

Use Pandas scraping to collect the 2019 FIFA World Cup Stats.

Use Pandas to scrape the following site, extract the "Tournament ranking" results table, clean the data, and export it to a CSV.

# World Cup Stats

Use Pandas scraping to collect the 2019 FIFA World Cup Stats.

Use Pandas to scrape the following site, extract the "Tournament ranking" results table, clean the data, and export it to a CSV.

### Execute GET request with URL

### Store response as variable

### Retrieve API output using `content` attribute

### Use `json` function from `json` library to format

### Format JSON with indents

### Identify country and GDP value second row

### Execute GET request with URL

### Store response as variable

### Retrieve API output using `content` attribute

### Use `json` function from `json` library to format

### Format JSON with indents

### Identify country and GDP value second row

### Import `requests` and `json` libraries

### Declare `request_urls`

### Execute `GET` request using `requests` library

### Get response `status code`

### Extract response `content`

### Convert output to JSON

### Format data with `json.dumps`

### Select value and store as variable

### Import `requests` and `json` libraries

### Declare `request_urls`

### Execute `GET` request using `requests` library

### Get response `status code`

### Extract response `content`

### Convert output to JSON

### Format data with `json.dumps`

### Select value and store as variable

### Execute the Numbers API for the number 42

### Execute the Numbers API for the number 8

### Execute the Numbers API for the number 42

### Execute the Numbers API for the number 8

# House of Requests

Use the `Deck of Cards` API to play a game of BlackJack with a partner. Parameterize the `Deck of Cards` API `request urls` in order to create the deck of cards, as well as draw cards for the game.

## Prep for the Game

### Parse JSON and extract `deck_id`

### Declare request urls to draw cards and shuffle deck

## Player 1 Turn

### Draw two cards

### Parse and extract `value` and `suit` elements from JSON output, for each card

### Decide whether to draw another card or skip to next player turn

### Manually calculate player 1 points and enter below

Player 1 points = 

## Player 2 / Dealer Turn

### Manually calculate player 2 points and enter below

Player 2 points =

# House of Requests

Use the `Deck of Cards` API to play a game of BlackJack with a partner. Parameterize the `Deck of Cards` API `request urls` in order to create the deck of cards, as well as draw cards for the game.

## Prep for the Game

### Parse JSON and extract `deck_id`

### Declare request urls to draw cards and shuffle deck

## Player 1 Turn

### Draw two cards

### Parse and extract `value` and `suit` elements from JSON output, for each card

### Decide whether to draw another card or skip to next player turn

### Manually calculate player 1 points and enter below

Player 1 points = 

## Player 2 / Dealer Turn

### Manually calculate player 2 points and enter below

Player 2 points =

### Instructor Demo: Environment Variables

### Instructor Demo: Environment Variables

### Use the `load_dotenv()` method from the `dotenv` package to load and export the environment variables

### Use the `os.getenv` function to retrieve the environment variable named `TMDB_API_KEY`. Store as a Python variable named `api_key`

### Use the `type` function to confirm the retrieval of the API key.

### Concatenate `request_url` with the `api_key` variable

### Execute GET request with API key

### Display JSON to screen using `json.dumps()`

### Bonus: Retrieve Movie Credits Data

### Use the `load_dotenv()` method from the `dotenv` package to load and export the environment variables

### Use the `os.getenv` function to retrieve the environment variable named `TMDB_API_KEY`. Store as a Python variable named `api_key`

### Use the `type` function to confirm the retrieval of the API key.

### Concatenate `request_url` with the `api_key` variable

### Execute GET request with API key

### Display JSON to screen using `json.dumps()`

### Bonus: Retrieve Movie Credits Data

# U.S. Census Demo

## Retrieve data from the U.S. Census using the Census library

References:

* Review the following page to review the Python library documentation: <https://github.com/CommerceDataService/census-wrapper>

* Review the following page to learn more about the data labels: <https://gist.github.com/afhaque/60558290d6efd892351c4b64e5c01e9b>

# U.S. Census Demo

## Retrieve data from the U.S. Census using the Census library

References:

* Review the following page to review the Python library documentation: <https://github.com/CommerceDataService/census-wrapper>

* Review the following page to learn more about the data labels: <https://gist.github.com/afhaque/60558290d6efd892351c4b64e5c01e9b>

# U.S. Census Retrieval

## Retrieve data from the U.S. Census using the Census library

References:

* Review the following page to review the Python library documentation: <https://github.com/CommerceDataService/census-wrapper>

* Review the following page to learn more about the data labels: <https://gist.github.com/afhaque/60558290d6efd892351c4b64e5c01e9b>

# U.S. Census Retrieval

## Retrieve data from the U.S. Census using the Census library

References:

* Review the following page to review the Python library documentation: <https://github.com/CommerceDataService/census-wrapper>

* Review the following page to learn more about the data labels: <https://gist.github.com/afhaque/60558290d6efd892351c4b64e5c01e9b>

# Explore Relationships in Data

## Load and Review Data

**After reviewing the dataset, what patterns and relationships are you interested in exploring? How might you use visualizations to gain insights on global happiness?**

YOUR ANSWER HERE

Answers will vary, but could include the following:

* How happiness varies by country
* How happiness varies by time
* If happiness tends to be higher in wealthier countries
* If countries with higher happiness levels are more likely to be generous

## Explore Happiness by Country with Bar Charts

**What do these charts tell you about global happiness?**

YOUR ANSWER HERE

Answers will vary, but could include the following observations:

* The maximum average happiness rating for any country is around 8 on a scale from 0 to 10
* Most of the countries in the dataset have a happiness rating in the middle of the scale
* The 15 happiest countries are mostly highly-developed countries in Europe and North America
* The 15 least happy countries are mostly less-developed, and several of the countries have had major conflicts or natural disasters

## Explore Changes Over Time with Line Charts

**What do you notice about the line graphs of happiness over time and wealth over time?**

YOUR ANSWER HERE

Answers will vary, but may include the following observations:

* The trends in the two plots look similar, which suggests there could be a correlation
* Both average global wealth and average global happiness have a noticeable decline in 2006
* Average global happiness has been about the same since 2008
* After 2006, both happiness and wealth seem to be increasing over time

## Explore Relationships with Scatter Plots

**What do you notice about the relationship between happiness and wealth? How does this relate to what you learned about how happiness and wealth vary over time?**

YOUR ANSWER HERE

Answers may vary, but should include the observation that happiness seems to increase as wealth increases. This suggests that there may be a relationship between happiness and wealth. This was also suggested by the previous charts, which showed that happiness and wealth had similar trends over time.

**What do you notice about the relationship between happiness and generosity?**

YOUR ANSWER HERE

Answers will vary, but may include the observation that there is no clear pattern or relationship. Interestingly, the most generous country does not have a high happiness rating. The countries with the highest happiness rating are only moderately happy.

**What do you notice about the relationship between wealth and generosity?**

YOUR ANSWER HERE

Answers may vary, but should note that there is no clear relationship between wealth and generosity. 

# Explore Relationships in Data

## Load and Review Data

**After reviewing the dataset, what patterns and relationships are you interested in exploring? How might you use visualizations to gain insights on global happiness?**

YOUR ANSWER HERE

Answers will vary, but could include the following:

* How happiness varies by country
* How happiness varies by time
* If happiness tends to be higher in wealthier countries
* If countries with higher happiness levels are more likely to be generous

## Explore Happiness by Country with Bar Charts

**What do these charts tell you about global happiness?**

YOUR ANSWER HERE

Answers will vary, but could include the following observations:

* The maximum average happiness rating for any country is around 8 on a scale from 0 to 10
* Most of the countries in the dataset have a happiness rating in the middle of the scale
* The 15 happiest countries are mostly highly-developed countries in Europe and North America
* The 15 least happy countries are mostly less-developed, and several of the countries have had major conflicts or natural disasters

## Explore Changes Over Time with Line Charts

**What do you notice about the line graphs of happiness over time and wealth over time?**

YOUR ANSWER HERE

Answers will vary, but may include the following observations:

* The trends in the two plots look similar, which suggests there could be a correlation
* Both average global wealth and average global happiness have a noticeable decline in 2006
* Average global happiness has been about the same since 2008
* After 2006, both happiness and wealth seem to be increasing over time

## Explore Relationships with Scatter Plots

**What do you notice about the relationship between happiness and wealth? How does this relate to what you learned about how happiness and wealth vary over time?**

YOUR ANSWER HERE

Answers may vary, but should include the observation that happiness seems to increase as wealth increases. This suggests that there may be a relationship between happiness and wealth. This was also suggested by the previous charts, which showed that happiness and wealth had similar trends over time.

**What do you notice about the relationship between happiness and generosity?**

YOUR ANSWER HERE

Answers will vary, but may include the observation that there is no clear pattern or relationship. Interestingly, the most generous country does not have a high happiness rating. The countries with the highest happiness rating are only moderately happy.

**What do you notice about the relationship between wealth and generosity?**

YOUR ANSWER HERE

Answers may vary, but should note that there is no clear relationship between wealth and generosity. 

# Introduction to PyPlot

## Plotting an Exponential Series

## Plotting Trigonometric Data

# Introduction to PyPlot

## Plotting an Exponential Series

## Plotting Trigonometric Data

# Configuring Line Plots

## Customizing the Sinusoidal Graph

### Data Markers

### Reference Lines

### Axis Labels

### Axis Limits

### Gridlines

### Legend

### Saving Plots



# Configuring Line Plots

## Customizing the Sinusoidal Graph

### Data Markers

### Reference Lines

### Axis Labels

### Axis Limits

### Gridlines

### Legend

### Saving Plots



### Using MatplotLib to Chart a DataFrame

### Using Pandas to Chart a DataFrame

### Using MatplotLib to Chart a DataFrame
Note that this is the same chart created in the previous activity.

### Using Pandas to Chart a DataFrame

# Library Usage

For this assignment, you will be taking Library Usage data from San Francisco and creating charts to determine which patron type checks out items from the library the most.

* Import your dependencies and then import your data into a pandas data frame from the CSV within the 'Data' folder
* Reduce the data to include only patrons who have checked out at least one item
* Split up your data into groups based upon the 'Patron Type Definition' column
* Chart your data using a bar graph, giving it both a title and labels for the axes

## Individual Branch Charts
You will now take the same base data frame before and create some code that will allow you to create individual bar charts for each library branch. For this part of the activity, we want you to chart the total 'Total Checkouts' of each library, sorted by patron type. If you are able to, try and come up with a method to do this without using loc or iloc to filter the original data frame! You can use loc to filter group data though.

Since there are quite a lot of patron types with minimal checkouts, the pie charts could look messy with overlapping text. You may also like to include a filter to limit the minimum number of total checkouts by patron group.

# Library Usage

For this assignment, you will be taking Library Usage data from San Francisco and creating charts to determine which patron type checks out items from the library the most.

* Import your dependencies and then import your data into a pandas data frame from the CSV within the 'Data' folder
* Reduce the data to include only patrons who have checked out at least one item
* Split up your data into groups based upon the 'Patron Type Definition' column
* Chart your data using a bar graph, giving it both a title and labels for the axes

## Individual Branch Charts
You will now take the same base data frame before and create some code that will allow you to create individual bar charts for each library branch. For this part of the activity, we want you to chart the total 'Total Checkouts' of each library, sorted by patron type. If you are able to, try and come up with a method to do this without using loc or iloc to filter the original data frame! You can use loc to filter group data though.

Since there are quite a lot of patron types with minimal checkouts, the pie charts could look messy with overlapping text. You may also like to include a filter to limit the minimum number of total checkouts by patron group.

# Traveling Companions: Part 1

# Traveling Companions: Part 1

# Traveling Companions: Part 2

### Part 2

# Traveling Companions: Part 2

### Part 2

# Traveling Companions: Part 3

### Part 2

### Part 3 - Charting Traveling Companions

* Create 3 variables, one for each country to chart.

* Create a variable for the type of traveling companion to compare.

* Store each country's percentage of travelers for the chosen traveling companion over time in 3 variables (one for each country)

* Create a line chart that will plot the comparison of each country's percentage of travelers with the chosen traveling companion from 2016 to 2018

# Traveling Companions: Part 3

### Part 2

### Part 3 - Charting Traveling Companions

* Create 3 variables, one for each country to chart.

* Create a variable for the type of traveling companion to compare.

* Store each country's percentage of travelers for the chosen traveling companion over time in 3 variables (one for each country)

* Create a line chart that will plot the comparison of each country's percentage of travelers with the chosen traveling companion from 2016 to 2018

# Stats Review

# Stats Review

# Box Plots

# Box Plots

# Temperature Outliers

# Temperature Outliers

# Compare different factors in the California housing dataset

# Compare different factors in the California housing dataset

# Demo: Using Pandas to Work with Time Data

# Demo: Analyzing Market Data Across Time

# Demo: Analyzing Market Data Across Time

A pandas `DatetimeIndex` object has several attributes that you can access to extract specific information about the dates. Here are some of them:

- `year`: The year of the date.
- `month`: The month of the date as January=1, December=12.
- `day`: The day of the month.
- `hour`: The hours of the datetime.
- `minute`: The minutes of the datetime.
- `second`: The seconds of the datetime.
- `microsecond`: The microseconds of the datetime.
- `nanosecond`: The nanoseconds of the datetime.
- `date`: Returns a numpy array of python datetime.date objects (only the date part).
- `time`: Returns a numpy array of python datetime.time objects (only the time part).
- `dayofweek`: The day of the week with Monday=0, Sunday=6.
- `dayofyear`: The ordinal day of the year.
- `weekofyear`: The week ordinal of the year.
- `quarter`: Quarter of the date: Jan-Mar = 1, Apr-Jun = 2, etc.
- `is_month_start`: Indicates whether the date is the first day of the month.
- `is_month_end`: Indicates whether the date is the last day of the month.
- `is_quarter_start`: Indicates whether the date is the first day of the quarter.
- `is_quarter_end`: Indicates whether the date is the last day of the quarter.
- `is_year_start`: Indicates whether the date is the first day of the year.
- `is_year_end`: Indicates whether the date is the last day of the year.
- `is_leap_year`: Indicates whether the date falls in a leap year.

Remember that these attributes return data for each date in the `DatetimeIndex`.

#### Step 2: Concatenate the `df_stock` DataFrame to the `df_trends` DataFrame, creating a single DataFrame named `df_apple`.

> **Rewind** Concatenation: Two DataFrames that have the same `datetime` index can be merged using `pd.concat`. The `axis=1` parameter means that we're bringing the columns of the two DataFrames together.

### Step 3: Create a plot of the concatenated DataFrame and analyze any trends and correlations. Do you observe any seasonal patterns?

> **Hint** You might find it useful to refer to the [Apple Events page](https://www.apple.com/apple-events/) and the [Timeline of Apple Inc. products](https://en.wikipedia.org/wiki/Timeline_of_Apple_Inc._products) to spotlight moments over time that could have triggered a particular pattern.

## Building Time Trends

### Step 1: Note that on September 10, 2019, Apple organized an event where it presented the new iPhone 11 family, the Apple Watch Series 5, and a new iPad. This event gained worldwide attention.

### Step 2: Closely examine the data from March 1, 2019 to January 31, 2020.

### Step 3: Create a plot and identify whether both time series indicate a common trend that might correspond to this narrative.

### Step 4: Before seeking any correlations between these time series, add columns to the `df_apple` DataFrame to analyze the impact of the Google Trends data on the weekly returns and stock volatility, as follows:

1. Use the Pandas `shift` function to add a new column that lags the Google Trends data by one period.

    > **Hint** Google Trends data is reported every week on Sunday, so you have a weekly period in this time series.

2. Use the Pandas `pct_change` function to add a new column that has the weekly price returns.

3. Compute the rolling stock volatility for Apple by using the following Pandas method:

    `df_apple["weekly_volatility"] = df_apple["close"].pct_change().rolling(window=4).std()`


### Step 5: Use the Pandas `corr` function to compute the correlations among the lagged Google Trends data, the price returns, and the stock volatility. Does any predictable relationship exist?

#### Step 2: Concatenate the `df_stock` DataFrame to the `df_trends` DataFrame, creating a single DataFrame named `df_apple`.

> **Rewind** Concatenation: Two DataFrames that have the same `datetime` index can be merged using `pd.concat`. The `axis=1` parameter means that we're bringing the columns of the two DataFrames together.

### Step 3: Create a plot of the concatenated DataFrame and analyze any trends and correlations. Do you observe any seasonal patterns?

> **Hint** You might find it useful to refer to the [Apple Events page](https://www.apple.com/apple-events/) and the [Timeline of Apple Inc. products](https://en.wikipedia.org/wiki/Timeline_of_Apple_Inc._products) to spotlight moments over time that could have triggered a particular pattern.

## Building Time Trends

### Step 1: Note that on September 10, 2019, Apple organized an event where it presented the new iPhone 11 family, the Apple Watch Series 5, and a new iPad. This event gained worldwide attention.

### Step 2: Closely examine the data from March 1, 2019 to January 31, 2020.

### Step 3: Create a plot and identify whether both time series indicate a common trend that might correspond to this narrative.

### Step 4: Before seeking any correlations between these time series, add columns to the `df_apple` DataFrame to analyze the impact of the Google Trends data on the weekly returns and stock volatility, as follows:

1. Use the Pandas `shift` function to add a new column that lags the Google Trends data by one period.

    > **Hint** Google Trends data is reported every week on Sunday, so you have a weekly period in this time series.

2. Use the Pandas `pct_change` function to add a new column that has the weekly price returns.

3. Compute the rolling stock volatility for Apple by using the following Pandas method:

    `df_apple["weekly_volatility"] = df_apple["close"].pct_change().rolling(window=4).std()`


### Step 5: Use the Pandas `corr` function to compute the correlations among the lagged Google Trends data, the price returns, and the stock volatility. Does any predictable relationship exist?

## Plot the Data

### Plotting the general trends

### Plotting the general trends

# Demo: Interpreting Prophet Forecasts for Decision Making

## Notebook Set Up

## Plot the Data

## Prepare the Data

## Create a Prophet Model

## Fit the Prophet Model

## Set Up for Predictions

## Build a Table of Predictions

## Plot the Forecast

## Read the Forecast Results

## Plot the Upper and Lower Bounds of the Forecast

## Break Down the Forecast

# Demo: Interpreting Prophet Forecasts for Decision Making

## Notebook Set Up

## Plot the Data

## Prepare the Data

## Create a Prophet Model

## Fit the Prophet Model

## Set Up for Predictions

## Build a Table of Predictions

## Plot the Forecast

## Read the Forecast Results

## Plot the Upper and Lower Bounds of the Forecast

## Break Down the Forecast

### Your turn. 

## Run the k-means model with 2 clusters

## Run the k-means model with 2 clusters

### Step 1: Read in the `customer-shopping-scaled.csv` file and create the DataFrame. Review the resulting DataFrame. Additionally, check the data types associated with the DataFrame.

### Step 2: Use the `encodeMethod` function that sets the `purchase` variable to 1 for "HotRestCafe" (hotel/restaurant/cafe ) purchases, and 2 for "retail" purchases.

### Step 3: Edit the "Method" column in the DataFrame by applying the `encodeMethod` function. 

### Step 4: Using this encoded DataFrame, initialize two K-means models: one with two clusters and another with three. For each model, be sure to follow each of the steps needed to identify the clusters and assign them to the data. 

* Initialize the KMeans model instance.
* Fit, or train, the model.
* Predict the model segments(clusters).

### Train the K-means algorithm where k=2

### Train the K-means algorithm where k=3

### Step 5: Once the models have been run, add each of the customer segment lists back into the `customers_df` DataFrame as new columns. 

### Step 6: Using Pandas plot, create scatter plots for each of the two customer segments. Be sure to build styled and formatted plots.

### Step 7: Answer the following question:

**Question**  Do you note any relevant differences between the two K-means models?

**Answer** There is a very little difference in the cluster assignment for frozen and grocery items. But, there is overlap with some data points in both clusters. However, the data seems to fit better with two clusters. 


### Step 1: Read in the `customer-shopping-scaled.csv` file and create the DataFrame. Review the resulting DataFrame. Additionally, check the data types associated with the DataFrame.

### Step 2: Use the `encodeMethod` function that sets the `purchase` variable to 1 for "HotRestCafe" (hotel/restaurant/cafe ) purchases, and 2 for "retail" purchases.

### Step 3: Edit the "Method" column in the DataFrame by applying the `encodeMethod` function. 

### Step 4: Using this encoded DataFrame, initialize two K-means models: one with two clusters and another with three. For each model, be sure to follow each of the steps needed to identify the clusters and assign them to the data. 

* Initialize the KMeans model instance.
* Fit, or train, the model.
* Predict the model segments(clusters).

### Train the K-means algorithm where k=2

### Train the K-means algorithm where k=3

### Step 5: Once the models have been run, add each of the customer segment lists back into the `customers_df` DataFrame as new columns. 

### Step 6: Using Pandas plot, create scatter plots for each of the two customer segments. Be sure to build styled and formatted plots.

### Step 7: Answer the following question:

**Question**  Do you note any relevant differences between the two K-means models?

**Answer** 


## Loading Scaled Customer Shopping Data into Pandas

## Implementing the Elbow Method

 The rate of decrease in inertia **begins** to slow down between k=3 to k=4, which means that our elbow point is at k=4.

## Loading Scaled Customer Shopping Data into Pandas

## Implementing the Elbow Method

 The rate of decrease in inertia **begins** to slow down between k=3 to k=4, which means that our elbow point is at k=4.

## Perform the following tasks for each of the two most likely values of `k`:

* Define a K-means model using `k` to define the clusters, fit the model, make predictions, and add the prediction values to a copy of the scaled DataFrame and call it `spread_predictions_df`.

* Plot the clusters. The x-axis should reflect the "hi_low_spread", and the y-axis should reflect the "close" price.

## Answer the following question
---
Considering the plot, what’s the best number of clusters to choose, or value of k? 

- From the scatter plots, it's a little hard to tell given the variability and quantity of the data, but it appears that the optimal value for k, the number of clusters, is 3.

## Perform the following tasks for each of the two most likely values of `k`:

* Define a K-means model using `k` to define the clusters, fit the model, make predictions, and add the prediction values to a copy of the scaled DataFrame and call it `spread_predictions_df`.

* Plot the clusters. The x-axis should reflect the "hi_low_spread", and the y-axis should reflect the "close" price.

## Answer the following question

* Considering the plot, what’s the best number of clusters to choose, or value of k? 

### Perform the following tasks for each of the two most likely values of `k`:

* Define a K-means model using `k` to define the clusters, fit the model, make predictions, and add the prediction values to a copy of the scaled DataFrame and call it `used_car_sales_predictions_df`.

* Plot the clusters. The x-axis should reflect home "selling_price", and the y-axis should reflect the "km_driven".

### Answer the following question

* Is the data segmented better into three or four clusters? Why? 

From the scatterplots, it appears that the optimal value for k, is probably 4. The lower kilometers driven matters more as the price of the used vehicle increases, whereas if the price is low the number of kilometers driven doesn't matter.

### Perform the following tasks for each of the two most likely values of `k`:

* Define a K-means model using `k` to define the clusters, fit the model, make predictions, and add the prediction values to a copy of the scaled DataFrame and call it `used_car_sales_predictions_df`.

* Plot the clusters. The x-axis should reflect home "selling_price", and the y-axis should reflect the "km_driven".

### Answer the following question

* Is the data segmented better into three or four clusters? Why? 

## Load the Credit Card Data into a Pandas DataFrame

## Data Preprocessing
---
###  Transform "education" column with get_dummies

### Transform "marriage" column with encoding function

## Data Scaling
---

### Apply the Standard Scaler to "limit_bal", "bill_amt", "pay_amt"

### Use the elbow method to find the best `k`.

##  Apply the KMeans Algorithm
---
### Use Kmeans to cluster data

## Load the Credit Card Data into a Pandas DataFrame

## Data Preprocessing
---
###  Transform "education" column with get_dummies

### Transform "marriage" column with encoding function

## Data Scaling
---

### Apply the Standard Scaler to "limit_bal", "bill_amt", "pay_amt"

### Use the elbow method to find the best `k`.

##  Apply the KMeans Algorithm
---
### Use Kmeans to cluster data

## Preprocess and Scale the Data

## Preprocess and Scale the Data

### Build the Dataset

### Fit and predict a K-Means Model

### Fit and Predict Birch and Agglomerative models

### Plot Model Predictions for Birch

### Estimate Scores for two Versions of the Birch Model

### Build the Dataset

### Fit and predict a K-Means Model

### Fit and Predict Birch and Agglomerative models

### Plot Model Predictions for Birch

### Estimate Scores for two Versions of the Birch Model

## Part 1: Create a Pandas DataFrame

## Part 2:  Preprocessing the Data
---
###  Transform "education" column with get_dummies

### Transform "marriage" column with encoding function

### Scale the Data
---
- Apply the Standard Scaler to "limit_bal", "bill_amt", "pay_amt"

## Part 3. Use the Elbow Method to determine the optimal number of clusters for KMeans.

## Part 4: Segment the data with K-means using the optimal number of clusters

## Part 5. Cluster the data using AgglomerativeClustering and Birch

Using your optimal number of clusters found above, additionally estimate clusters by using both `AgglomerativeClustering` and `Birch`. Save each of these models and their results for comparison.

## Part 6. Compare the cluster results from using Kmeans, AgglomerativeClustering, Birch

## Part 1: Create a Pandas DataFrame

## Part 2:  Preprocessing the Data
---
###  Transform "education" column with get_dummies

### Transform "marriage" column with encoding function

### Scale the Data
---
- Apply the Standard Scaler to "limit_bal", "bill_amt", "pay_amt"

## Part 3. Use the Elbow Method to determine the optimal number of clusters for KMeans.

## Part 4: Segment the data with K-means using the optimal number of clusters

## Part 5. Cluster the data using AgglomerativeClustering and Birch

Using your optimal number of clusters found above, additionally estimate clusters by using both `AgglomerativeClustering` and `Birch`. Save each of these models and their results for comparison.

## Part 6. Compare the cluster results from using Kmeans, AgglomerativeClustering, Birch

## Prepare the Data 

## Fit and Predict with KMeans


## Plot and Analyze the Results

* Based on this plot, which cluster of country appears to provide both the highest interest spread and currency return?

## Fit and Predict with the Birch Clustering Algorithm

* Based on this plot, which cluster of country appears to provide both the highest interest spread and currency return?

## Prepare the Data 

## Fit and Predict with KMeans

## Plot and Analyze the Results

* Based on this plot, which cluster of country appears to provide both the highest interest spread and currency return?

## Fit and Predict with the Birch Clustering Algorithm

* Based on this plot, which cluster of country appears to provide both the highest interest spread and currency return?

## Load the Data Into a Pandas DataFrame

## Normalize and Transform the Data

## Get the Variance of Each Component

## Creating the PCA DataFrame

## Determine the Optimal `k` Value

## Segmentation of the PCA data with K-means 

### Which features have the strongest influence on each component?
---

- "age" has the strongest influence on PCA1 at 99.9% and probably has the biggest influence on segmenting the data.
- "limit_bal", "bill_amt", and "pay_amt" have the strongest influence on PCA2 at 57%, 55%, and 60%. 
-  Based on these results, our original graph of using the "limit_bal" and "age" before applying PCA may be good enough to segment the data. Similar results would have been achieved for "bill_amt" and "age", and "pay_amt" and "age". 

## Load the Data Into a Pandas DataFrame

## Normalize and Transform the Data

## Get the Variance of Each Component

## Creating the PCA DataFrame

## Determine the Optimal `k` Value

## Segmentation of the PCA data with K-means 

### Which features have the strongest influence on each component?
---


### Read in the CSV file and prepare the Pandas DataFrame

### Step 1: Use PCA to reduce the dimensionality of the transformed customers DataFrame to 2 principal components

### Step 2: Using the `explained_variance_ratio_` function from PCA, calculate the percentage of the total variance that is captured by the two PCA variables.

**Question:** What is the explained variance ratio captured by the two PCA variables?
    
**Answer:** About 85% of the total variance is condensed into the 2 PCA variables.

### Step 3: Using the `customer_pca` data, create a Pandas DataFrame called customers_pca_df. The columns of the DataFrame should be called "PCA1" and "PCA2".

### Step 4: Using the `customers_pca_df` Dataframe, utilize the elbow method to determine the optimal value of k.

### Step 5: Segment the `customers_pca_df`  DataFrame using the K-means algorithm.

### Step 6: Segment the `customers_transformed_df` DataFrame with all factors using the K-means algorithm

### Step 7. Which features have the strongest influence on each component? And, plot the most influencial features for each component. 


**Answer:** 
- "feature_6" and "feature_9" have the strongest positive influence on PCA1. 
- "feature_7" and "feature_10" have the strongest positive influence on PCA2, whereas "feature_6" has the strong negative influence on PCA2.

### Step 8: Create a scatter plot of the most influential features for each component and customer segments.

### Step 9: What is the difference between the segmentation results of the PCA DataFrame and most influential features for each component? 

**Answer:** It appears that the customer segmentation information using the DataFrame with "feature_9" and "feature_10" yields similar results that the PCA analysis. 

### Read in the CSV file and prepare the Pandas DataFrame

### Step 1: Use PCA to reduce the dimensionality of the transformed customers DataFrame to 2 principal components

### Step 2: Using the `explained_variance_ratio_` function from PCA, calculate the percentage of the total variance that is captured by the two PCA variables.

**Question:** What is the explained variance ratio captured by the two PCA variables?
    
**Answer:** 

### Step 3: Using the `customer_pca` data, create a Pandas DataFrame called customers_pca_df. The columns of the DataFrame should be called "PCA1" and "PCA2".

### Step 4: Using the `customers_pca_df` Dataframe, utilize the elbow method to determine the optimal value of k.

### Step 5: Segment the `customers_pca_df`  DataFrame using the K-means algorithm.

### Step 6: Segment the `customers_transformed_df` DataFrame with all factors using the K-means algorithm

### Step 7. Which features have the strongest influence on each component? And, plot the most influencial features for each component. 


**Answer:** 


### Step 8: Create a scatter plot of the most influential features for each component and customer segments.

### Step 9: What is the difference between the segmentation results of the PCA DataFrame and most influential features for each component?

**Answer:** 

### Step 1:  Read in the `stock_data.csv` file and create a DataFrame.

### Step 2: Scale the `df_stocks` DataFrame and create a new DataFrame that contains the scaled data. 

### Step 3: Initialize the K-means model with three clusters and then fit the `df_stocks_scaled` DataFrame to the model.

### Step 4. Predict the clusters and then create a new DataFrame with the predicted clusters.

### Step 5: Create a scatter plot to visualize the "StockCluster" using  "MeanOpen" as the x-variable and "MeanPercentReturn" as the y-variable.  

### Step 6: Reduce the number of features to two principal components on the `df_stocks_scaled` DataFrame, and  calculate the explained variance ratio that results from the PCA data.

### Step 7: Use the calculate PCA DataFrame in Step 6 to create a new DataFrame called, `df_stocks_pca`, then add an additional column to the `df_stocks_pca` DataFrame that contains the tickers from the original `df_stocks` DataFrame.

### Step 8: Rerun the K-means algorithm on the `df_stocks_pca` DataFrame and create a scatter plot using the  "StockCluster" and the two principal components for the x- and y-axes. Be sure to style and format your plot.

**Question:** After visually analyzing the cluster analysis results, what is the impact of using fewer features to cluster the data using K-Means?

**Answer:** We can conclude that using less features we can more clearly identify three clusters.

### Step 9. Determine which features have the strongest influence on each componen, and plot the most influencial features for each component.

### Which features have the strongest influence on each component? 
--- 
**Answer:** 
- "MeanOpen", "MeanHigh", "MeanLow", and "MeanClose" have the strongest positive influence on PCA1. 
- "MeanVolume" has the strongest positive influence on PCA2.

### Step 10: Create a scatterplot of the most influential features for each principal component and stock cluster.

### Step 11: What is the difference between the segmentation results of the PCA DataFrame and most influential features for each component? 

**Answer:** It appears that the most influential features for each component are; "MeanHigh" and "MeanVolume", however, plotting these features does not yield similar results as plotting each principal component. Suggesting that more than one feature is influencing one or principal components. 

### Step 1:  Read in the `stock_data.csv` file and create a DataFrame.

### Step 2: Scale the `df_stocks` DataFrame and create a new DataFrame that contains the scaled data. 

### Step 3: Initialize the K-means model with three clusters and then fit the `df_stocks_scaled` DataFrame to the model.

### Step 4. Predict the clusters and then create a new DataFrame with the predicted clusters.

### Step 5: Create a scatter plot to visualize the "StockCluster" using  "MeanOpen" as the x-variable and "MeanPercentReturn" as the y-variable.  

### Step 6: Reduce the number of features to two principal components on the `df_stocks_scaled` DataFrame, and  calculate the explained variance ratio that results from the PCA data.

### Step 7: Use the calculate PCA DataFrame in Step 6 to create a new DataFrame called, `df_stocks_pca`, then add an additional column to the `df_stocks_pca` DataFrame that contains the tickers from the original `df_stocks` DataFrame.

### Step 8: Rerun the K-means algorithm on the `df_stocks_pca` DataFrame and create a scatter plot using the  "StockCluster" and the two principal components for the x- and y-axes. Be sure to style and format your plot.

**Question:** After visually analyzing the cluster analysis results, what is the impact of using fewer features to cluster the data using K-Means?

**Answer:**

### Step 9. Determine which features have the strongest influence on each componen, and plot the most influencial features for each component.

### Which features have the strongest influence on each component? 
--- 
**Answer:** 


### Step 10: Create a scatterplot of the most influential features for each principal component and stock cluster.

### Step 11: What is the difference between the segmentation results of the PCA DataFrame and most influential features for each component? 

**Answer:** 

# Demo: Linear Regression

## Data Loading and Visualization

## Data Preparation

## Building the Linear Regression Model

# Demo: Linear Regression

## Data Loading and Visualization

## Data Preparation

## Building the Linear Regression Model

## Load and Visualize the Sales Data

## Prepare the Data to Fit the Linear Regression Model

## Build the Linear Regression Model

## Plot the Best Fit Line for the Sales Prediction Model

## Make Manual Predictions

## Make Predictions Using the `predict` Function

## Load and Visualize the Sales Data

## Prepare the Data to Fit the Linear Regression Model

## Build the Linear Regression Model

## Plot the Best Fit Line for the Sales Prediction Model

## Make Manual Predictions

## Make Predictions Using the `predict` Function

# Demo: Linear Regression Model Evaluation

## Data Loading and Visualization

## Data Preparation

## Building the Linear Regression Model

## Make Predictions

## Linear Regression Model Assessment

# Demo: Linear Regression Model Evaluation

## Data Loading and Visualization

## Data Preparation

## Building the Linear Regression Model

## Make Predictions

## Linear Regression Model Assessment

## Load and Visualize the Sales Data

## Prepare the Data to Fit the Linear Regression Model

## Build the Linear Regression Model

## Make Predictions

## Linear Regression Model Assessment

## Load and Visualize the Sales Data

## Prepare the Data to Fit the Linear Regression Model

## Build the Linear Regression Model

## Make Predictions

## Linear Regression Model Assessment

## Load and Visualize the Electricity Data

## Prepare the Data to Fit the Linear Regression Model

## Build the Linear Regression Model

## Plot the Best Fit Line for the Electricity Generation Prediction Model

## Make Manual Predictions

## Make Predictions Using the `predict` Function

## Linear Regression Model Assessment

## Load and Visualize the Electricity Data

## Prepare the Data to Fit the Linear Regression Model

## Build the Linear Regression Model

## Plot the Best Fit Line for the Electricity Generation Prediction Model

## Make Manual Predictions

## Make Predictions Using the `predict` Function

## Linear Regression Model Assessment

# 1985 Auto Imports Database

1. Title: 1985 Auto Imports Database

2. Source Information:

   * Creator/Donor: Jeffrey C. Schlimmer (Jeffrey.Schlimmer@a.gp.cs.cmu.edu)
   
   * Date: 19 May 1987
   
   * Sources:
     1) 1985 Model Import Car and Truck Specifications, 1985 Ward's
        Automotive Yearbook.
     2) Personal Auto Manuals, Insurance Services Office, 160 Water
        Street, New York, NY 10038 
     3) Insurance Collision Report, Insurance Institute for Highway
        Safety, Watergate 600, Washington, DC 20037

3. Past Usage:

   * Kibler, D., Aha, D.W., \& Albert, M. (1989).  Instance-based prediction
      of real-valued attributes.  {\it Computational Intelligence}, {\it 5},
      51--57.
      
    * Predicted price of car using all numeric and Boolean attributes
     
    * Method: an instance-based learning (IBL) algorithm derived from a
	    localized k-nearest neighbor algorithm.  Compared with a
	    linear regression prediction...so all instances
	    with missing attribute values were discarded.  This resulted with
	    a training set of 159 instances, which was also used as a test
	    set (minus the actual instance during testing).
        
	 * Results: Percent Average Deviation Error of Prediction from Actual
     
	    * 11.84% for the IBL algorithm
        
	    * 14.12% for the resulting linear regression equation

4. Relevant Information:
   * Description
      This data set consists of three types of entities: (a) the
      specification of an auto in terms of various characteristics, (b)
      its assigned insurance risk rating, (c) its normalized losses in use
      as compared to other cars.  The second rating corresponds to the
      degree to which the auto is more risky than its price indicates.
      Cars are initially assigned a risk factor symbol associated with its
      price.   Then, if it is more risky (or less), this symbol is
      adjusted by moving it up (or down) the scale.  Actuarians call this
      process "symboling".  A value of +3 indicates that the auto is
      risky, -3 that it is probably pretty safe.

      The third factor is the relative average loss payment per insured
      vehicle year.  This value is normalized for all autos within a
      particular size classification (two-door small, station wagons,
      sports/speciality, etc...), and represents the average loss per car
      per year.

   * Note: Several of the attributes in the database could be used as a
            "class" attribute.

5. Number of Instances: 205

6. Number of Attributes: 26 total

   ```text
   -- 15 continuous
   -- 1 integer
   -- 10 nominal
   ```

7. Attribute Information:     

```text

     Attribute:                Attribute Range:
     ------------------        -----------------------------------------------
  1. symboling:                -3, -2, -1, 0, 1, 2, 3.
  2. normalized-losses:        continuous from 65 to 256.
  3. make:                     alfa-romero, audi, bmw, chevrolet, dodge, honda,
                               isuzu, jaguar, mazda, mercedes-benz, mercury,
                               mitsubishi, nissan, peugot, plymouth, porsche,
                               renault, saab, subaru, toyota, volkswagen, volvo
  4. fuel-type:                diesel, gas.
  5. aspiration:               std, turbo.
  6. num-of-doors:             four, two.
  7. body-style:               hardtop, wagon, sedan, hatchback, convertible.
  8. drive-wheels:             4wd, fwd, rwd.
  9. engine-location:          front, rear.
 10. wheel-base:               continuous from 86.6 120.9.
 11. length:                   continuous from 141.1 to 208.1.
 12. width:                    continuous from 60.3 to 72.3.
 13. height:                   continuous from 47.8 to 59.8.
 14. curb-weight:              continuous from 1488 to 4066.
 15. engine-type:              dohc, dohcv, l, ohc, ohcf, ohcv, rotor.
 16. num-of-cylinders:         eight, five, four, six, three, twelve, two.
 17. engine-size:              continuous from 61 to 326.
 18. fuel-system:              1bbl, 2bbl, 4bbl, idi, mfi, mpfi, spdi, spfi.
 19. bore:                     continuous from 2.54 to 3.94.
 20. stroke:                   continuous from 2.07 to 4.17.
 21. compression-ratio:        continuous from 7 to 23.
 22. horsepower:               continuous from 48 to 288.
 23. peak-rpm:                 continuous from 4150 to 6600.
 24. city-mpg:                 continuous from 13 to 49.
 25. highway-mpg:              continuous from 16 to 54.
 26. price:                    continuous from 5118 to 45400.
```

8. Missing Attribute Values: (denoted by "?")

   ```text
   Attribute #:   Number of instances missing a value:
   2.             41
   6.             2
   19.            4
   20.            4
   22.            2
   23.            2
   26.            4
   ```

## Remaining categorical features and their values:
```text
make                  alfa-romero, audi, bmw, chevrolet, dodge, honda,
                      isuzu, jaguar, mazda, mercedes-benz, mercury,
                      mitsubishi, nissan, peugot, plymouth, porsche,
                      renault, saab, subaru, toyota, volkswagen, volvo
fuel-type             diesel, gas
aspiration            std, turbo
body-style            hardtop, wagon, sedan, hatchback, convertible
drive-wheels          4wd, fwd, rwd
engine-location       front, rear
engine-type           dohc, dohcv, l, ohc, ohcf, ohcv, rotor
fuel-system           1bbl, 2bbl, 4bbl, idi, mfi, mpfi, spdi, spfi
```

## Pandas encoding methods

## Scikit-learn encoding methods

# 1985 Auto Imports Database

1. Title: 1985 Auto Imports Database

2. Source Information:

   * Creator/Donor: Jeffrey C. Schlimmer (Jeffrey.Schlimmer@a.gp.cs.cmu.edu)
   
   * Date: 19 May 1987
   
   * Sources:
     1) 1985 Model Import Car and Truck Specifications, 1985 Ward's
        Automotive Yearbook.
     2) Personal Auto Manuals, Insurance Services Office, 160 Water
        Street, New York, NY 10038 
     3) Insurance Collision Report, Insurance Institute for Highway
        Safety, Watergate 600, Washington, DC 20037

3. Past Usage:

   * Kibler, D., Aha, D.W., \& Albert, M. (1989).  Instance-based prediction
      of real-valued attributes.  {\it Computational Intelligence}, {\it 5},
      51--57.
      
    * Predicted price of car using all numeric and Boolean attributes
     
    * Method: an instance-based learning (IBL) algorithm derived from a
	    localized k-nearest neighbor algorithm.  Compared with a
	    linear regression prediction...so all instances
	    with missing attribute values were discarded.  This resulted with
	    a training set of 159 instances, which was also used as a test
	    set (minus the actual instance during testing).
        
	 * Results: Percent Average Deviation Error of Prediction from Actual
     
	    * 11.84% for the IBL algorithm
        
	    * 14.12% for the resulting linear regression equation

4. Relevant Information:
   * Description
      This data set consists of three types of entities: (a) the
      specification of an auto in terms of various characteristics, (b)
      its assigned insurance risk rating, (c) its normalized losses in use
      as compared to other cars.  The second rating corresponds to the
      degree to which the auto is more risky than its price indicates.
      Cars are initially assigned a risk factor symbol associated with its
      price.   Then, if it is more risky (or less), this symbol is
      adjusted by moving it up (or down) the scale.  Actuarians call this
      process "symboling".  A value of +3 indicates that the auto is
      risky, -3 that it is probably pretty safe.

      The third factor is the relative average loss payment per insured
      vehicle year.  This value is normalized for all autos within a
      particular size classification (two-door small, station wagons,
      sports/speciality, etc...), and represents the average loss per car
      per year.

   * Note: Several of the attributes in the database could be used as a
            "class" attribute.

5. Number of Instances: 205

6. Number of Attributes: 26 total

   ```text
   -- 15 continuous
   -- 1 integer
   -- 10 nominal
   ```

7. Attribute Information:     

```text

     Attribute:                Attribute Range:
     ------------------        -----------------------------------------------
  1. symboling:                -3, -2, -1, 0, 1, 2, 3.
  2. normalized-losses:        continuous from 65 to 256.
  3. make:                     alfa-romero, audi, bmw, chevrolet, dodge, honda,
                               isuzu, jaguar, mazda, mercedes-benz, mercury,
                               mitsubishi, nissan, peugot, plymouth, porsche,
                               renault, saab, subaru, toyota, volkswagen, volvo
  4. fuel-type:                diesel, gas.
  5. aspiration:               std, turbo.
  6. num-of-doors:             four, two.
  7. body-style:               hardtop, wagon, sedan, hatchback, convertible.
  8. drive-wheels:             4wd, fwd, rwd.
  9. engine-location:          front, rear.
 10. wheel-base:               continuous from 86.6 120.9.
 11. length:                   continuous from 141.1 to 208.1.
 12. width:                    continuous from 60.3 to 72.3.
 13. height:                   continuous from 47.8 to 59.8.
 14. curb-weight:              continuous from 1488 to 4066.
 15. engine-type:              dohc, dohcv, l, ohc, ohcf, ohcv, rotor.
 16. num-of-cylinders:         eight, five, four, six, three, twelve, two.
 17. engine-size:              continuous from 61 to 326.
 18. fuel-system:              1bbl, 2bbl, 4bbl, idi, mfi, mpfi, spdi, spfi.
 19. bore:                     continuous from 2.54 to 3.94.
 20. stroke:                   continuous from 2.07 to 4.17.
 21. compression-ratio:        continuous from 7 to 23.
 22. horsepower:               continuous from 48 to 288.
 23. peak-rpm:                 continuous from 4150 to 6600.
 24. city-mpg:                 continuous from 13 to 49.
 25. highway-mpg:              continuous from 16 to 54.
 26. price:                    continuous from 5118 to 45400.
```

8. Missing Attribute Values: (denoted by "?")

   ```text
   Attribute #:   Number of instances missing a value:
   2.             41
   6.             2
   19.            4
   20.            4
   22.            2
   23.            2
   26.            4
   ```

## Remaining categorical features and their values:
```text
make                  alfa-romero, audi, bmw, chevrolet, dodge, honda,
                      isuzu, jaguar, mazda, mercedes-benz, mercury,
                      mitsubishi, nissan, peugot, plymouth, porsche,
                      renault, saab, subaru, toyota, volkswagen, volvo
fuel-type             diesel, gas
aspiration            std, turbo
body-style            hardtop, wagon, sedan, hatchback, convertible
drive-wheels          4wd, fwd, rwd
engine-location       front, rear
engine-type           dohc, dohcv, l, ohc, ohcf, ohcv, rotor
fuel-system           1bbl, 2bbl, 4bbl, idi, mfi, mpfi, spdi, spfi
```

## Pandas encoding methods

## Scikit-learn encoding methods

## Use Feature Selection for X

## Use Feature Selection for X

## Split into training and testing sets

## Train the model

## Evaluate the model

## Split into training and testing sets

## Train the model

## Evaluate the model

## Visualize the Data to Find Any Linear Trends

## Visualize the Data to Find Any Linear Trends

## Split the amenities into separate columns

The amenities column may contain multiple amenities, separated by a comma. Each amenity should have its own column with binary values where 0 means the property does not have the amenity and 1 means that it does.

## Remaining categorical features and their values

```text
category         'housing/rent/apartment', 'housing/rent/home',
                 'housing/rent/short_term'
fee              'No'
has_photo        'Thumbnail', 'Yes', 'No'
pets_allowed     'None', 'Cats,Dogs', 'Cats', 'Dogs'
price_type       'Monthly', 'Weekly', 'Monthly|Weekly'
cityname         1575 cities in dataset
state            52 states in dataset
source           'RentLingo', 'Listanza', 'ListedBuy', 'RentDigs.com', 'GoSection8',
                 'RealRentals', 'RENTOCULAR', 'rentbits', 'Home Rentals',
                 'Real Estate Agent', 'RENTCafé', 'tenantcloud'
```

## Pandas encoding methods

## Scikit-learn encoding methods

## Split into Training and Testing Data

## Split the amenities into separate columns

The amenities column may contain multiple amenities, separated by a comma. Each amenity should have its own column with binary values where 0 means the property does not have the amenity and 1 means that it does.

## Remaining categorical features and their values

```text
category         'housing/rent/apartment', 'housing/rent/home',
                 'housing/rent/short_term'
fee              'No'
has_photo        'Thumbnail', 'Yes', 'No'
pets_allowed     'None', 'Cats,Dogs', 'Cats', 'Dogs'
price_type       'Monthly', 'Weekly', 'Monthly|Weekly'
cityname         1575 cities in dataset
state            52 states in dataset
source           'RentLingo', 'Listanza', 'ListedBuy', 'RentDigs.com', 'GoSection8',
                 'RealRentals', 'RENTOCULAR', 'rentbits', 'Home Rentals',
                 'Real Estate Agent', 'RENTCafé', 'tenantcloud'
```

## Pandas encoding methods

## Scikit-learn encoding methods

## Split into Training and Testing Data

## Split into training and testing sets

## Train the models

## Evaluate the model

## Split into training and testing sets

## Train the models

## Evaluate the model

### Use cross validation to select alpha

### Use cross validation to select alpha

### Perform ridge regression

### Compare performance with a linear regression model

### Lasso regression

### Assess the lasso regression MSE and compare to ridge regression

### Perform ridge regression

### Compare performance with a linear regression model

### Lasso regression

### Assess the lasso regression MSE and compare to ridge regression

## Split into training and testing sets

## Train the models

## Evaluate the model

## Split into training and testing sets

## Train the models

## Evaluate the model

# Prepare the Data

# Split the data into training and testing sets

# Model and Fit the Data to a Logistic Regression

# Predict the Testing Labels

# Prepare the Data

# Split the data into training and testing sets

# Model and Fit the Data to a Logistic Regression

# Predict the Testing Labels

# Predicting Malware

## Reference:

Mathur, A. (2022). [NATICUSdroid (Android Permissions) Dataset](https://archive.ics.uci.edu/dataset/722/naticusdroid+android+permissions+dataset). UCI Machine Learning Repository.

## Prepare the Data

## Split the data into training and testing sets

## Model and Fit the Data to a Logistic Regression

## Predict the Testing Labels

## Calculate the Performance Metrics

**Question:** For this dataset, how well did the model predict actual malware?

**Answer:** For this test data: Accuracy looks extremely good: approximately 96% of the apps in the test data were accurately categorized by the model.

# Predicting Malware

## Reference:

Mathur, A. (2022). [NATICUSdroid (Android Permissions) Dataset](https://archive.ics.uci.edu/dataset/722/naticusdroid+android+permissions+dataset). UCI Machine Learning Repository.

## Prepare the Data

## Split the data into training and testing sets

## Model and Fit the Data to a Logistic Regression

## Predict the Testing Labels

## Calculate the Performance Metrics

**Question:** For this dataset, how well did the model predict actual malware?

**Answer:** 

**Which scalar instance produces a better accuracy score on the scaled testing data?**

Answer: `MinMaxScalar()`

**Which scalar instance produces a better accuracy score on the scaled testing data?**

Answer: 

# Dataset:  occupancy.csv

Source: Accurate occupancy detection of an office room from light, temperature, humidity and CO2 measurements using statistical learning models. Luis M. Candanedo, VÃ©ronique Feldheim. Energy and Buildings. Volume 112, 15 January 2016, Pages 28-39.

Description: Experimental data used for binary classification (room occupancy) from Temperature,Humidity,Light and CO2. Ground-truth occupancy was obtained from time stamped pictures that were taken every minute.

Variables/Columns

- Temperature, in Celsius
- Relative Humidity %
- Light in Lux
- CO2 in ppm
- Humidity Ratio, Derived quantity from temperature and relative humidity, in kgwater-vapor/kg-air
- Occupancy 0 or 1 
    - 0 for not occupied
    - 1 for occupied 

## Split the data into training and testing sets

## Model and Fit to a Support Vector Machine

## Predict the Testing Labels

## Evaluate the Model

# Dataset:  occupancy.csv

Source: Accurate occupancy detection of an office room from light, temperature, humidity and CO2 measurements using statistical learning models. Luis M. Candanedo, VÃ©ronique Feldheim. Energy and Buildings. Volume 112, 15 January 2016, Pages 28-39.

Description: Experimental data used for binary classification (room occupancy) from Temperature,Humidity,Light and CO2. Ground-truth occupancy was obtained from time stamped pictures that were taken every minute.

Variables/Columns

- Temperature, in Celsius
- Relative Humidity %
- Light in Lux
- CO2 in ppm
- Humidity Ratio, Derived quantity from temperature and relative humidity, in kgwater-vapor/kg-air
- Occupancy 0 or 1 
    - 0 for not occupied
    - 1 for occupied 

## Split the data into training and testing sets

## Model and Fit to a Support Vector Machine

## Predict the Testing Labels

## Evaluate the Model

## Split the data into training and testing sets

## Model and Fit to a Support Vector Machine

## Predict the Testing Labels

## Evaluate the Model

### Compare the results:

**Logistic Regression Accuracy Score:** 0.9605891176871676

## Split the data into training and testing sets

## Model and Fit to a Support Vector Machine

## Predict the Testing Labels

## Evaluate the Model

### Compare the results:

**Logistic Regression Accuracy Score:** 0.9605891176871676

# Dataset:  glass.csv

Source: Vina Spiehler, Ph.D., DABFT Diagnostic Products Corporation

Description: Research conducted to test the rule-based system BEAGLE to determine whether a glass was a type of "float" glass or not. 6 types of glass are defined in terms of their oxide content. The study of classification of types of glass was motivated by criminological investigation. At the scene of the crime, the glass left can be used as evidence, but only if it is correctly identified!

Variables/Columns

- rI: refractive index
- na: Sodium 
- mg: Magnesium
- al: Aluminum
- si: Silicon
- k: Potassium
- ca: Calcium
- ba: Barium
- fe: Iron
- glass: (class attribute)
    - 1: building_windows_float_processed
    - 2: building_windows_non_float_processed
    - 3: vehicle_windows_float_processed
    - 4: vehicle_windows_non_float_processed (none in this database)
    - 5: containers
    - 6: tableware
    - 7: headlamps

# Dataset:  glass.csv

Source: Vina Spiehler, Ph.D., DABFT Diagnostic Products Corporation

Description: Research conducted to test the rule-based system BEAGLE to determine whether a glass was a type of "float" glass or not. 6 types of glass are defined in terms of their oxide content. The study of classification of types of glass was motivated by criminological investigation. At the scene of the crime, the glass left can be used as evidence, but only if it is correctly identified!

Variables/Columns

- rI: refractive index
- na: Sodium 
- mg: Magnesium
- al: Aluminum
- si: Silicon
- k: Potassium
- ca: Calcium
- ba: Barium
- fe: Iron
- glass: (class attribute)
    - 1: building_windows_float_processed
    - 2: building_windows_non_float_processed
    - 3: vehicle_windows_float_processed
    - 4: vehicle_windows_non_float_processed (none in this database)
    - 5: containers
    - 6: tableware
    - 7: headlamps

# K-nearest neighbors

# K-nearest neighbors

## Loading and Preprocessing Crowdfunding Data

Load the `crowdfunding-data.csv` in a pandas DataFrame called `df_crowdfunding`.

Define the features set, by copying the `df_crowdfunding` DataFrame and dropping the `outcome` column.

Create the target vector by assigning the values of the `outcome` column from the `df_crowdfunding` DataFrame.

Split the data into training and testing sets.

Use the `StandardScaler` to scale the features data, remember that only `X_train` and `X_test` DataFrames should be scaled.

## Fitting the Decision Tree Model

Once data is scaled, create a decision tree instance and train it with the training data (`X_train_scaled` and `y_train`).

## Making Predictions Using the Tree Model

Validate the trained model, by predicting crowdfunding success using the testing data (`X_test_scaled`).

## Model Evaluation

Evaluate model's results, by using `sklearn` to calculate the accuracy score.

## Visualizing the Decision Tree

In this section, you should create a visual representation of the decision tree using `pydotplus`. Show the graph on the notebook, and also save it in `PDF` and `PNG` formats.

## Loading and Preprocessing Crowdfunding Data

Load the `crowdfunding-data.csv` in a pandas DataFrame called `df_crowdfunding`.

Define the features set, by copying the `df_crowdfunding` DataFrame and dropping the `outcome` column.

Create the target vector by assigning the values of the `outcome` column from the `df_crowdfunding` DataFrame.

Split the data into training and testing sets.

Use the `StandardScaler` to scale the features data, remember that only `X_train` and `X_testing` DataFrames should be scaled.

## Fitting the Decision Tree Model

Once data is scaled, create a decision tree instance and train it with the training data (`X_train_scaled` and `y_train`).

## Making Predictions Using the Tree Model

Validate the trained model, by predicting malware apps using the testing data (`X_test_scaled`).

## Model Evaluation

Evaluate model's results, by using `sklearn` to calculate the accuracy score.

## Visualizing the Decision Tree

In this section, you should create a visual representation of the decision tree using `pydotplus`. Show the graph on the notebook, and also save it in `PDF` and `PNG` formats.

## Loading and Preprocessing Crowdfunding Data

Load the `app-data.csv` in a pandas DataFrame called `app_data`.

Define the features set, by copying the `df_crowdfunding` DataFrame and dropping the `outcome` column.

Create the target vector by assigning the values of the `Result` column from the `app_data` DataFrame.

Split the data into training and testing sets.

Use the `StandardScaler` to scale the features data, remember that only `X_train` and `X_test` DataFrames should be scaled.

## Fitting the Decision Tree Model

Once data is scaled, create a decision tree instance and train it with the training data (`X_train_scaled` and `y_train`).

## Making Predictions Using the Tree Model

Validate the trained model, by predicting crowdfunding success using the testing data (`X_test_scaled`).

## Model Evaluation

Evaluate model's results, by using `sklearn` to calculate the accuracy score.

## Visualizing the Decision Tree

In this section, you should create a visual representation of the decision tree using `pydotplus`. Show the graph on the notebook, and also save it in `PDF` and `PNG` formats.

## Loading and Preprocessing Crowdfunding Data

Load the `app-data.csv` in a pandas DataFrame called `app_data`.

Define the features set, by copying the `df_crowdfunding` DataFrame and dropping the `outcome` column.

Create the target vector by assigning the values of the `Result` column from the `app_data` DataFrame.

Split the data into training and testing sets.

Use the `StandardScaler` to scale the features data, remember that only `X_train` and `X_test` DataFrames should be scaled.

## Fitting the Decision Tree Model

Once data is scaled, create a decision tree instance and train it with the training data (`X_train_scaled` and `y_train`).

## Making Predictions Using the Tree Model

Validate the trained model, by predicting crowdfunding success using the testing data (`X_test_scaled`).

## Model Evaluation

Evaluate model's results, by using `sklearn` to calculate the accuracy score.

## Visualizing the Decision Tree

In this section, you should create a visual representation of the decision tree using `pydotplus`. Show the graph on the notebook, and also save it in `PDF` and `PNG` formats.

# Dataset:  covtype.csv

Source: Remote Sensing and GIS Program, Department of Forest Sciences, College of Natural Resources, Colorado State University

Description: Predicting forest cover type from cartographic variables only (no remotely sensed data). The actual forest cover type for a given observation (30 x 30 meter cell) was determined from US Forest Service (USFS) Region 2 Resource Information System (RIS) data. Independent variables were derived from data originally obtained from US Geological Survey (USGS) and USFS data. Data is in raw form (not scaled) and contains binary (0 or 1) columns of data for qualitative independent variables (wilderness areas and soil types).

This study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices.

Variables/Columns

- Elevation: Elevation in meters
- Aspect: Aspect in degrees azimuth
- Slope: Slope in degrees
- Horizontal_Distance_To_Hydrology: Horz Dist to nearest surface water features
- Vertical_Distance_To_Hydrology: Vert Dist to nearest surface water features
- Horizontal_Distance_To_Roadways: Horz Dist to nearest roadway
- Hillshade_9am: Hillshade index at 9am, summer solstice
- Hillshade_Noon: Hillshade index at noon, summer soltice
- Hillshade_3pm: Hillshade index at 3pm, summer solstice
- Horizontal_Distance_To_Fire_Points: Horz Dist to nearest wildfire ignition points
- Wilderness_Area: 0 (absence) or 1 (presence)
- Cover_Type: (2 types) Forest Cover Type designation
    - 1: Spruce/Fir
    - 2: Lodgepole Pine

# Dataset:  covtype.csv

Source: Remote Sensing and GIS Program, Department of Forest Sciences, College of Natural Resources, Colorado State University

Description: Predicting forest cover type from cartographic variables only (no remotely sensed data). The actual forest cover type for a given observation (30 x 30 meter cell) was determined from US Forest Service (USFS) Region 2 Resource Information System (RIS) data. Independent variables were derived from data originally obtained from US Geological Survey (USGS) and USFS data. Data is in raw form (not scaled) and contains binary (0 or 1) columns of data for qualitative independent variables (wilderness areas and soil types).

This study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices.

Variables/Columns

- Elevation: Elevation in meters
- Aspect: Aspect in degrees azimuth
- Slope: Slope in degrees
- Horizontal_Distance_To_Hydrology: Horz Dist to nearest surface water features
- Vertical_Distance_To_Hydrology: Vert Dist to nearest surface water features
- Horizontal_Distance_To_Roadways: Horz Dist to nearest roadway
- Hillshade_9am: Hillshade index at 9am, summer solstice
- Hillshade_Noon: Hillshade index at noon, summer soltice
- Hillshade_3pm: Hillshade index at 3pm, summer solstice
- Horizontal_Distance_To_Fire_Points: Horz Dist to nearest wildfire ignition points
- Wilderness_Area: 0 (absence) or 1 (presence)
- Cover_Type: (2 types) Forest Cover Type designation
    - 1: Spruce/Fir
    - 2: Lodgepole Pine

## Loading and Preprocessing Malware Apps Data

Load the `app-data.csv` in a pandas DataFrame called `df_apps`

Define the features set, by copying the `df_apps` DataFrame and dropping the `Result` column.

Create the target vector by assigning the values of the `Result` column from the `df_apps` DataFrame.

Split the data into training and testing sets.

## Fitting the Random Forest Model

Create a random forest instance and train it with the training data (`X_train` and `y_train`), define `n_estimators=128` and `random_state=78`.

## Making Predictions Using the Random Forest Model

Validate the trained model by malware apps using the testing data (`X_test`).

## Model Evaluation

Evaluate model's results, by using `sklearn` to calculate the accuracy score.

## Feature Importance

In this section, you are asked to fetch the features' importance from the random forest model and display the top 10 most important features.

## Analysis Questions

Finally, analyze the model's evaluation results and answer the following questions.

* **Question:** Would you trust this model to detect malware? 

    * **Sample Answer:** Yes. The model's accuracy is good at predicting malware because of the high accuracy. 

* **Question:** Out of the following models, which one had the highest accuracy score: logistic regression, SVM, KNN, decision tree, or random forest?

    * **Sample Answer:** Random forest performed marginally better (about 0.007) than the other models, which all performed in a similar range. Other performance metrics should be calculated to determine the best model.

## Loading and Preprocessing Malware Apps Data

Load the `app-data.csv` in a pandas DataFrame called `df_apps`

Define the features set, by copying the `df_apps` DataFrame and dropping the `Result` column.

Create the target vector by assigning the values of the `Result` column from the `df_apps` DataFrame.

Split the data into training and testing sets.

## Fitting the Random Forest Model

Create a random forest instance and train it with the training data (`X_train` and `y_train`), define `n_estimators=128` and `random_state=78`.

## Making Predictions Using the Random Forest Model

Validate the trained model by malware apps using the testing data (`X_test`).

## Model Evaluation

Evaluate model's results, by using `sklearn` to calculate the accuracy score.

## Feature Importance

In this section, you are asked to fetch the features' importance from the random forest model and display the top 10 most important features.

## Analysis Questions

Finally, analyze the model's evaluation results and answer the following questions.

* **Question:** Would you trust this model to detect malware? 

    * **Answer:** 

* **Question:** Out of the following models, which one had the highest accuracy score: logistic regression, SVM, KNN, decision tree, or random forest?

    * **Answer:**

# Dataset:  covtype.csv

Source: Remote Sensing and GIS Program, Department of Forest Sciences, College of Natural Resources, Colorado State University

Description: Predicting forest cover type from cartographic variables only (no remotely sensed data). The actual forest cover type for a given observation (30 x 30 meter cell) was determined from US Forest Service (USFS) Region 2 Resource Information System (RIS) data. Independent variables were derived from data originally obtained from US Geological Survey (USGS) and USFS data. Data is in raw form (not scaled) and contains binary (0 or 1) columns of data for qualitative independent variables (wilderness areas and soil types).

This study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices.

Variables/Columns

- Elevation: Elevation in meters
- Aspect: Aspect in degrees azimuth
- Slope: Slope in degrees
- Horizontal_Distance_To_Hydrology: Horz Dist to nearest surface water features
- Vertical_Distance_To_Hydrology: Vert Dist to nearest surface water features
- Horizontal_Distance_To_Roadways: Horz Dist to nearest roadway
- Hillshade_9am: Hillshade index at 9am, summer solstice
- Hillshade_Noon: Hillshade index at noon, summer soltice
- Hillshade_3pm: Hillshade index at 3pm, summer solstice
- Horizontal_Distance_To_Fire_Points: Horz Dist to nearest wildfire ignition points
- Wilderness_Area: 0 (absence) or 1 (presence)
- Cover_Type: (2 types) Forest Cover Type designation
    - 1: Spruce/Fir
    - 2: Lodgepole Pine

# Dataset:  covtype.csv

Source: Remote Sensing and GIS Program, Department of Forest Sciences, College of Natural Resources, Colorado State University

Description: Predicting forest cover type from cartographic variables only (no remotely sensed data). The actual forest cover type for a given observation (30 x 30 meter cell) was determined from US Forest Service (USFS) Region 2 Resource Information System (RIS) data. Independent variables were derived from data originally obtained from US Geological Survey (USGS) and USFS data. Data is in raw form (not scaled) and contains binary (0 or 1) columns of data for qualitative independent variables (wilderness areas and soil types).

This study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices.

Variables/Columns

- Elevation: Elevation in meters
- Aspect: Aspect in degrees azimuth
- Slope: Slope in degrees
- Horizontal_Distance_To_Hydrology: Horz Dist to nearest surface water features
- Vertical_Distance_To_Hydrology: Vert Dist to nearest surface water features
- Horizontal_Distance_To_Roadways: Horz Dist to nearest roadway
- Hillshade_9am: Hillshade index at 9am, summer solstice
- Hillshade_Noon: Hillshade index at noon, summer soltice
- Hillshade_3pm: Hillshade index at 3pm, summer solstice
- Horizontal_Distance_To_Fire_Points: Horz Dist to nearest wildfire ignition points
- Wilderness_Area: 0 (absence) or 1 (presence)
- Cover_Type: (2 types) Forest Cover Type designation
    - 1: Spruce/Fir
    - 2: Lodgepole Pine

# Tic-tac-toe endgame

1. Title: Tic-Tac-Toe Endgame database

2. Source Information
   -- Creator: David W. Aha (aha@cs.jhu.edu)
   -- Donor: David W. Aha (aha@cs.jhu.edu)
   -- Date: 19 August 1991
 
3. Known Past Usage: 
   1. Matheus,~C.~J., \& Rendell,~L.~A. (1989).  Constructive
      induction on decision trees.  In {\it Proceedings of the
      Eleventh International Joint Conference on Artificial Intelligence} 
      (pp. 645--650).  Detroit, MI: Morgan Kaufmann.
      -- CITRE was applied to 100-instance training and 200-instance test
         sets.  In a study using various amounts of domain-specific
         knowledge, its highest average accuracy was 76.7% (using the
         final decision tree created for testing).

   2. Matheus,~C.~J. (1990). Adding domain knowledge to SBL through
      feature construction.  In {\it Proceedings of the Eighth National
      Conference on Artificial Intelligence} (pp. 803--808). 
      Boston, MA: AAAI Press.
      -- Similar experiments with CITRE, includes learning curves up
         to 500-instance training sets but used _all_ instances in the
         database for testing.  Accuracies reached above 90%, but specific
         values are not given (see Chris's dissertation for more details).

   3. Aha,~D.~W. (1991). Incremental constructive induction: An instance-based
      approach.  In {\it Proceedings of the Eighth International Workshop
      on Machine Learning} (pp. 117--121).  Evanston, ILL: Morgan Kaufmann.
      -- Used 70% for training, 30% of the instances for testing, evaluated
         over 10 trials.  Results reported for six algorithms:
         -- NewID:   84.0%
         -- CN2:     98.1%  
         -- MBRtalk: 88.4%
         -- IB1:     98.1% 
         -- IB3:     82.0%
         -- IB3-CI:  99.1%
      -- Results also reported when adding an additional 10 irrelevant 
         ternary-valued attributes; similar _relative_ results except that
         IB1's performance degraded more quickly than the others.

4. Relevant Information:

   This database encodes the complete set of possible board configurations
   at the end of tic-tac-toe games, where "x" is assumed to have played
   first.  The target concept is "win for x" (i.e., true when "x" has one
   of 8 possible ways to create a "three-in-a-row").  

   Interestingly, this raw database gives a stripped-down decision tree
   algorithm (e.g., ID3) fits.  However, the rule-based CN2 algorithm, the
   simple IB1 instance-based learning algorithm, and the CITRE 
   feature-constructing decision tree algorithm perform well on it.

5. Number of Instances: 958 (legal tic-tac-toe endgame boards)

6. Number of Attributes: 9, each corresponding to one tic-tac-toe square

7. Attribute Information: (x=player x has taken, o=player o has taken, b=blank)

    1. top-left-square: {x,o,b}
    2. top-middle-square: {x,o,b}
    3. top-right-square: {x,o,b}
    4. middle-left-square: {x,o,b}
    5. middle-middle-square: {x,o,b}
    6. middle-right-square: {x,o,b}
    7. bottom-left-square: {x,o,b}
    8. bottom-middle-square: {x,o,b}
    9. bottom-right-square: {x,o,b}
   10. Class: {positive,negative}

8. Missing Attribute Values: None

9. Class Distribution: About 65.3% are positive (i.e., wins for "x")

## Preprocess the data

## Model and Fit to a Logistic Regression Classifier

## Model and Fit to a Support Vector Machine

## Model and Fit to a KNN model

## Model and Fit to a Decision Tree Classifier

## Model and Fit to a Random Forest Classifier

# Tic-tac-toe endgame

1. Title: Tic-Tac-Toe Endgame database

2. Source Information
   -- Creator: David W. Aha (aha@cs.jhu.edu)
   -- Donor: David W. Aha (aha@cs.jhu.edu)
   -- Date: 19 August 1991
 
3. Known Past Usage: 
   1. Matheus,~C.~J., \& Rendell,~L.~A. (1989).  Constructive
      induction on decision trees.  In {\it Proceedings of the
      Eleventh International Joint Conference on Artificial Intelligence} 
      (pp. 645--650).  Detroit, MI: Morgan Kaufmann.
      -- CITRE was applied to 100-instance training and 200-instance test
         sets.  In a study using various amounts of domain-specific
         knowledge, its highest average accuracy was 76.7% (using the
         final decision tree created for testing).

   2. Matheus,~C.~J. (1990). Adding domain knowledge to SBL through
      feature construction.  In {\it Proceedings of the Eighth National
      Conference on Artificial Intelligence} (pp. 803--808). 
      Boston, MA: AAAI Press.
      -- Similar experiments with CITRE, includes learning curves up
         to 500-instance training sets but used _all_ instances in the
         database for testing.  Accuracies reached above 90%, but specific
         values are not given (see Chris's dissertation for more details).

   3. Aha,~D.~W. (1991). Incremental constructive induction: An instance-based
      approach.  In {\it Proceedings of the Eighth International Workshop
      on Machine Learning} (pp. 117--121).  Evanston, ILL: Morgan Kaufmann.
      -- Used 70% for training, 30% of the instances for testing, evaluated
         over 10 trials.  Results reported for six algorithms:
         -- NewID:   84.0%
         -- CN2:     98.1%  
         -- MBRtalk: 88.4%
         -- IB1:     98.1% 
         -- IB3:     82.0%
         -- IB3-CI:  99.1%
      -- Results also reported when adding an additional 10 irrelevant 
         ternary-valued attributes; similar _relative_ results except that
         IB1's performance degraded more quickly than the others.

4. Relevant Information:

   This database encodes the complete set of possible board configurations
   at the end of tic-tac-toe games, where "x" is assumed to have played
   first.  The target concept is "win for x" (i.e., true when "x" has one
   of 8 possible ways to create a "three-in-a-row").  

   Interestingly, this raw database gives a stripped-down decision tree
   algorithm (e.g., ID3) fits.  However, the rule-based CN2 algorithm, the
   simple IB1 instance-based learning algorithm, and the CITRE 
   feature-constructing decision tree algorithm perform well on it.

5. Number of Instances: 958 (legal tic-tac-toe endgame boards)

6. Number of Attributes: 9, each corresponding to one tic-tac-toe square

7. Attribute Information: (x=player x has taken, o=player o has taken, b=blank)

    1. top-left-square: {x,o,b}
    2. top-middle-square: {x,o,b}
    3. top-right-square: {x,o,b}
    4. middle-left-square: {x,o,b}
    5. middle-middle-square: {x,o,b}
    6. middle-right-square: {x,o,b}
    7. bottom-left-square: {x,o,b}
    8. bottom-middle-square: {x,o,b}
    9. bottom-right-square: {x,o,b}
   10. Class: {positive,negative}

8. Missing Attribute Values: None

9. Class Distribution: About 65.3% are positive (i.e., wins for "x")

## Preprocess the data

## Model and Fit to a Logistic Regression Classifier

## Model and Fit to a Support Vector Machine

## Model and Fit to a KNN model

## Model and Fit to a Decision Tree Classifier

## Model and Fit to a Random Forest Classifier

# Car Evaluation

1. Title: Car Evaluation Database

2. Sources:
   (a) Creator: Marko Bohanec
   (b) Donors: Marko Bohanec   (marko.bohanec@ijs.si)
               Blaz Zupan      (blaz.zupan@ijs.si)
   (c) Date: June, 1997

3. Past Usage:

   The hierarchical decision model, from which this dataset is derived, was first presented in M. Bohanec and V. Rajkovic: Knowledge acquisition and explanation for multi-attribute decision making. In 8th Intl Workshop on Expert Systems and their Applications, Avignon, France. pages 59-78, 1988.

   Within machine-learning, this dataset was used for the evaluation of HINT (Hierarchy INduction Tool), which was proved to be able to completely reconstruct the original hierarchical model. This, together with a comparison with C4.5, is presented in B. Zupan, M. Bohanec, I. Bratko, J. Demsar: Machine learning by
   function decomposition. ICML-97, Nashville, TN. 1997 (to appear)

4. Relevant Information Paragraph:

   Car Evaluation Database was derived from a simple hierarchical decision model originally developed for the demonstration of DEX (M. Bohanec, V. Rajkovic: Expert system for decision making. Sistemica 1(1), pp. 145-157, 1990.). The model evaluates cars according to the following concept structure:

   ```text
   CAR                      car acceptability
   . PRICE                  overall price
   . . buying               buying price
   . . maint                price of the maintenance
   . TECH                   technical characteristics
   . . COMFORT              comfort
   . . . doors              number of doors
   . . . persons            capacity in terms of persons to carry
   . . . lug_boot           the size of luggage boot
   . . safety               estimated safety of the car
   ```

   Input attributes are printed in lowercase. Besides the target concept (CAR), the model includes three intermediate concepts:  PRICE, TECH, COMFORT. Every concept is in the original model related to its lower level descendants by a set of examples (for these examples sets see http://www-ai.ijs.si/BlazZupan/car.html).

   The Car Evaluation Database contains examples with the structural information removed, i.e., directly relates CAR to the six input attributes: buying, maint, doors, persons, lug_boot, safety.

   Because of known underlying concept structure, this database may be particularly useful for testing constructive induction and structure discovery methods.

5. Number of Instances: 1728
   (instances completely cover the attribute space)

6. Number of Attributes: 6

7. Attribute Values:

   ```text
   buying       v-high, high, med, low
   maint        v-high, high, med, low
   doors        2, 3, 4, 5-more
   persons      2, 4, more
   lug_boot     small, med, big
   safety       low, med, high
   ```

8. Missing Attribute Values: none

9. Class Distribution (number of instances per class)

   ```text
   class      N          N[%]
   -----------------------------
   unacc     1210     (70.023 %) 
   acc        384     (22.222 %) 
   good        69     ( 3.993 %) 
   v-good      65     ( 3.762 %) 
   ```

## Preprocess the data

## Model and Fit to a Logistic Regression Classifier

## Model and Fit to a Support Vector Machine

## Model and Fit to a KNN Model

## Model and Fit to a Decision Tree Classifier

## Model and Fit to a Random Forest Classifier

# Car Evaluation

1. Title: Car Evaluation Database

2. Sources:
   (a) Creator: Marko Bohanec
   (b) Donors: Marko Bohanec   (marko.bohanec@ijs.si)
               Blaz Zupan      (blaz.zupan@ijs.si)
   (c) Date: June, 1997

3. Past Usage:

   The hierarchical decision model, from which this dataset is derived, was first presented in M. Bohanec and V. Rajkovic: Knowledge acquisition and explanation for multi-attribute decision making. In 8th Intl Workshop on Expert Systems and their Applications, Avignon, France. pages 59-78, 1988.

   Within machine-learning, this dataset was used for the evaluation of HINT (Hierarchy INduction Tool), which was proved to be able to completely reconstruct the original hierarchical model. This, together with a comparison with C4.5, is presented in B. Zupan, M. Bohanec, I. Bratko, J. Demsar: Machine learning by
   function decomposition. ICML-97, Nashville, TN. 1997 (to appear)

4. Relevant Information Paragraph:

   Car Evaluation Database was derived from a simple hierarchical decision model originally developed for the demonstration of DEX (M. Bohanec, V. Rajkovic: Expert system for decision making. Sistemica 1(1), pp. 145-157, 1990.). The model evaluates cars according to the following concept structure:

   ```text
   CAR                      car acceptability
   . PRICE                  overall price
   . . buying               buying price
   . . maint                price of the maintenance
   . TECH                   technical characteristics
   . . COMFORT              comfort
   . . . doors              number of doors
   . . . persons            capacity in terms of persons to carry
   . . . lug_boot           the size of luggage boot
   . . safety               estimated safety of the car
   ```

   Input attributes are printed in lowercase. Besides the target concept (CAR), the model includes three intermediate concepts:  PRICE, TECH, COMFORT. Every concept is in the original model related to its lower level descendants by a set of examples (for these examples sets see http://www-ai.ijs.si/BlazZupan/car.html).

   The Car Evaluation Database contains examples with the structural information removed, i.e., directly relates CAR to the six input attributes: buying, maint, doors, persons, lug_boot, safety.

   Because of known underlying concept structure, this database may be particularly useful for testing constructive induction and structure discovery methods.

5. Number of Instances: 1728
   (instances completely cover the attribute space)

6. Number of Attributes: 6

7. Attribute Values:

   ```text
   buying       v-high, high, med, low
   maint        v-high, high, med, low
   doors        2, 3, 4, 5-more
   persons      2, 4, more
   lug_boot     small, med, big
   safety       low, med, high
   ```

8. Missing Attribute Values: none

9. Class Distribution (number of instances per class)

   ```text
   class      N          N[%]
   -----------------------------
   unacc     1210     (70.023 %) 
   acc        384     (22.222 %) 
   good        69     ( 3.993 %) 
   v-good      65     ( 3.762 %) 
   ```

## Preprocess the data

## Model and Fit to a Logistic Regression Classifier

## Model and Fit to a Support Vector Machine

## Model and Fit to a KNN Model

## Model and Fit to a Decision Tree Classifier

## Model and Fit to a Random Forest Classifier

# Customer Churn

## Model and Fit to a Logistic Regression Classifier

## Model and Fit to a Support Vector Machine

## Model and Fit to a KNN Model

## Model and Fit to a Decision Tree Classifier

## Model and Fit to a Random Forest Classifier

# Letter Recognition

## Model and Fit to a Logistic Regression Classifier

## Model and Fit to a Support Vector Machine

## Model and Fit to a KNN Model

## Model and Fit to a Decision Tree Classifier

## Model and Fit to a Random Forest Classifier

## Model and Fit to a Gradient Boosting Classifier

## Model and Fit to an Adaptive Boosting Classifier

# Website Phishing

## Preprocess the data

## Model and Fit to a Logistic Regression Classifier

## Model and Fit to a Support Vector Machine

## Model and Fit to a KNN Model

## Model and Fit to a Decision Tree Classifier

## Model and Fit to a Random Forest Classifier

# Sports Article Objectivity

## Model and Fit to a Logistic Regression Classifier

## Model and Fit to a Support Vector Machine

## Model and Fit to a KNN Model

## Model and Fit to a Decision Tree Classifier

## Model and Fit to a Random Forest Classifier

## Model and Fit to a Gradient Boosting Classifier

## Model and Fit to an Adaptive Boosting Classifier

# Orthopaedic Patients

## Model and Fit to a Logistic Regression Classifier

## Model and Fit to a Support Vector Machine

## Model and Fit to a KNN Model

## Model and Fit to a Decision Tree Classifier

## Model and Fit to a Random Forest Classifier

# Part 1: Crowdfunding

# Part 2: Start Up Success

# Part 1: Crowdfunding

# Part 2: Start Up Success

# Missing Values

# Categorical Variables

# Missing Values

# Categorical Variables

# Missing Values

# Categorical Variables

## Interpretations
What were the best settings for the hyperparameters that were tested? How much improvement was made by tuning those hyperparameters?

**Answer:** The best hyper parameter combination was {'weights': 'distance', 'n_neighbors': 19, 'leaf_size': 243}, and they improved the overall accuracy of the model from 0.87 to 0.89. If Recall of the positive class was the metric of interest however, the adjusted hyperparameters performed worse. 

## Interpretations
What were the best settings for the hyperparameters that were tested? How much improvement was made by tuning those hyperparameters?

# Random Resampling

## Generating the features and targets dataset

---

## Random Undersampling
---
We need to import the RandomUnderSampler function from the imbalance learn module, `imblearn.under_sampling` as follows: 
* `pip install imbalanced-learn`.

---

## Random Oversampling

# Random Resampling

## Generating the features and targets dataset

---

## Random Undersampling
---
We need to import the RandomUnderSampler function from the imbalance learn module, `imblearn.under_sampling` as follows: 
* `pip install imbalanced-learn`.

---

## Random Oversampling

## Prepare the Data

---

## RandomForestClassifier

### Create and fit a `RandomForestClassifier` to the **scaled** training data.

---

## Random Undersampler

### Import `RandomUnderSampler` from `imblearn`.

###  Create and fit a `RandomForestClassifier` to the **undersampled** training data.

---

## Random Oversampler

###  Import `RandomOverSampler` from `imblearn`.

###  Create and fit a `RandomForestClassifier` to the **oversampled** training data.

---

## Cluster Centroids

---

## SMOTE

---

## SMOTEENN

## More Loans

In this activity you will pratice using random and SMOTE oversampling in combination with logistic regression to predict whether or not someone is likely to default on their credit card loans in a given month given demographic information. 

ln_balance_limit is the log of the maximum balance they can have on the card; 1 is female, 0 male for sex; the education is denoted: 1 = graduate school; 2 = university; 3 = high school; 4 = others; 1 is married and 0 single for marriage; default_next_month is whether the person defaults in the following month (1 yes, 0 no).

### Random Oversampling

### SMOTE Oversampling

## Prepare the Data

---

## RandomForestClassifier

### Create and fit a `RandomForestClassifier` to the **scaled** training data.

---

## Random Undersampler

### Import `RandomUnderSampler` from `imblearn`.

###  Create and fit a `RandomForestClassifier` to the **undersampled** training data.

---

## Random Oversampler

###  Import `RandomOverSampler` from `imblearn`.

###  Create and fit a `RandomForestClassifier` to the **oversampled** training data.

---

## Cluster Centroids

---

## SMOTE

---

## SMOTEENN

# Missing Values

# Missing Values

# Categorical Variables

# Missing Values

# Missing Values

## Saving the Trained Model

## Loading a Trained Model

## Saving the Trained Model

## Loading a Trained Model

## Save the Model

## Load the Model

# Sports Article Objectivity

## Model and Fit to a Logistic Regression Classifier

## Model and Fit to a Support Vector Machine

## Model and Fit to a KNN Model

## Model and Fit to a Decision Tree Classifier

## Model and Fit to a Random Forest Classifier

## Model and Fit to a Gradient Boosting Classifier

## Model and Fit to an Adaptive Boosting Classifier

## Save the Model

## Load the Model

# Sports Article Objectivity

## Model and Fit to a Logistic Regression Classifier

## Model and Fit to a Support Vector Machine

## Model and Fit to a KNN Model

## Model and Fit to a Decision Tree Classifier

## Model and Fit to a Random Forest Classifier

## Model and Fit to a Gradient Boosting Classifier

## Model and Fit to an Adaptive Boosting Classifier

## Prepare the Data

## Train the Model

## Save Model

## Retrieve Model

## Test Model Recommendations

## Prepare the Data

## Train the Model

## Save Model

## Retrieve Model

## Test Model Recommendations

## Prepare the Data

## Prepare the Data

## Prepare the Data

## Prepare the Data

## Prepare the Data

## Prepare the Data

# Gathering URLs
The image data can be stored in many different ways, but unfortunately it isn't really feasible to store a bunch of images in a CSV. This means that the steps to import a bunch of images will depend wholly on the way those images are stored, and where the metadata about those images resides. In our case, the image metadata IS stored conveniently in a CSV! Lets import it.

Note that the metadata contains only one column; the filename of each image. We can combine this with the base_url to build full urls for each image.

Importing 624 images will take a while, likely more than 4 minutes. When we do the import, it will be important to add print statements to monitor progress, and also to check our work with only a few images to start.

# Storing Pickles
This process took a long time; it would be nice if we didn't have to RELOAD all the images every time we want to use them. The "pickle" module allows us to store Python objects as local files so that we can open them back up whenever we need them, even in other projects. In Google Colab, we'll also have to start by "mounting" Google Drive. Lets try this with the list of images we just made.

# Gathering URLs
The image data can be stored in many different ways, but unfortunately it isn't really feasible to store a bunch of images in a CSV. This means that the steps to import a bunch of images will depend wholly on the way those images are stored, and where the metadata about those images resides. In our case, the image metadata IS stored conveniently in a CSV! Lets import it.

Note that the metadata contains only one column; the filename of each image. We can combine this with the base_url to build full urls for each image.

# Storing Pickles
This process took a long time; it would be nice if we didn't have to RELOAD all the images every time we want to use them. The "pickle" module allows us to store Python objects as local files so that we can open them back up whenever we need them, even in other projects. In Google Colab, we'll also have to start by "mounting" Google Drive. Lets try this with the list of images we just made.

Note the shape of the data: an array of arrays, where each internal array represents one row of pixels. The numbers indicate how bright each pixel is on a scale of 0 to 255 where 255 is full brightness and zero is black. These numbers are what our model will use to make predictions! Many image models expect floating point values, so its common practice to convert from integers to floating point before other preprocessing steps. Note that from this point forward, we will not be able to visualize the images without using some sort of plotting library; we've converted the images to numbers!

Note the shape of the data: an array of arrays, where each internal array represents one row of pixels. The numbers indicate how bright each pixel is on a scale of 0 to 255 where 255 is full brightness and zero is black. These numbers are what our model will use to make predictions! Many image models expect floating point values, so its common practice to convert from integers to floating point before other preprocessing steps. Note that from this point forward, we will not be able to visualize the images without using some sort of plotting library; we've converted the images to numbers!

# Part X: Labels

We've done a good bit of work preprocessing the data but... how are we going to do classification? Supervised learning requires labels for our data, but these pixel values don't contain any secret information that will help us classify images. Different datasets will handle this problem differently, but for our dataset, lets re-examine our image filenames.

Note how each filename is broken into four parts. You can read more about the data here https://archive.ics.uci.edu/dataset/124/cmu+face+images, but these filenames contain metadata in the following format: `<userid>_<pose>_<expression>_<eyes>.png`

For our first model, lets see if we can determine whether the person in the picture has sunglasses on. To do this, we'll need to extract the label we want from the filenames. To do this, we can make use of Pandas split function https://pandas.pydata.org/docs/reference/api/pandas.Series.str.split.html

# Part X: Labels

We've done a good bit of work preprocessing the data but... how are we going to do classification? Supervised learning requires labels for our data, but these pixel values don't contain any secret information that will help us classify images. Different datasets will handle this problem differently, but for our dataset, lets re-examine our image filenames.

Note how each filename is broken into four parts. You can read more about the data here https://archive.ics.uci.edu/dataset/124/cmu+face+images, but these filenames contain metadata in the following format: `<userid>_<pose>_<expression>_<eyes>.png`

For our first model, lets see if we can determine whether the person in the picture has sunglasses on. To do this, we'll need to extract the label we want from the filenames. To do this, we can make use of Pandas split function https://pandas.pydata.org/docs/reference/api/pandas.Series.str.split.html

This is merely an example of a CNN model; note that we only have one Conv2D layer and one MaxPooling layer. To create the first layer (Conv2D) we must pass the shape of our images. The first value is the height of the image, the second is the width, and the third is the number of channels the image has for color. For RGB images, there are three channels; red, green, and blue. For grayscale images (like our faces data) there is only 1 channel.

Convolutional Layer (Conv2D):

layers.Conv2D(32, (3, 3), activation='relu', input_shape=(250, 250, 3))
This layer applies convolution to the input images. The settings are as follows:
32: This represents the number of filters (or kernels) that will be used in the convolution. It means there are 32 different filters to learn different patterns.
(3, 3): This specifies the size of the convolutional kernel. A kernel of size (3, 3) means the convolution operation will use a 3x3 filter.
activation='relu': Rectified Linear Unit (ReLU) is used as the activation function, introducing non-linearity to the model.
input_shape=(250, 250, 3): This defines the shape of the input data. In this case, it's set to (250, 250, 3), indicating a 250x250 image with three color channels (RGB).
MaxPooling Layer (MaxPooling2D):

layers.MaxPooling2D((2, 2))
This layer performs max pooling, which reduces the spatial dimensions of the input volume. The settings are:
(2, 2): This specifies the size of the pooling window. A window of (2, 2) means the operation will take the maximum value over a 2x2 window.
Flatten Layer (Flatten):

layers.Flatten()
This layer flattens the output from the previous layer into a one-dimensional array. It prepares the data for the subsequent fully connected layers.
Dense Layer (Dense):

layers.Dense(64, activation='relu')
This fully connected layer has 64 neurons. The settings are:
64: Number of neurons or units in the layer.
activation='relu': ReLU is again used as the activation function.
Output Layer (Dense):

layers.Dense(2, activation='sigmoid')
This is the output layer with two neurons, suitable for binary classification tasks. The settings are:
2: Two output neurons, one for each class in binary classification.
activation='sigmoid': Sigmoid activation is used for binary classification to produce probabilities.
These settings are typical choices for a simple CNN architecture. Adjustments might be made based on the specific characteristics of the dataset and the objectives of the model.

In the end, our validation score is 92% accuracy, which is pretty good! What might we do to improve this model though?

This is merely an example of a CNN model; note that we only have one Conv2D layer and one MaxPooling layer. In the end, our validation score is 92% accuracy, which is pretty good! What might we do to improve this model though?

Before we augment an image, we must preprocess it like we have before

In addition to our regular preprocessing, the image generator requires a "batch" dimension and a "channels" dimension. RGB images already have channel dimensions, but a grayscale image must have it added.

We could also easily apply these augmentations to the entire training set to make our model more robust; to do this, we will not display all the images. Displaying the images is nice to verify your work, but it is not a necessary step.

First, we import our preprocessed data and split it into training and testing sets

Next we build our data generator; make sure to select only parameters that will work with our data. An image of a face might appear facing either left or right, but in our data it would never appear upside down, so flipping the images vertically would not make sense!

Now we can loop through the entire training set and add 5 new images based on every original image. Note that we only do this for the training dataset; this is to help our model learn to identify the original images correctly! Adding "fake" images to the testing set would skew our results.

Before we augment an image, we must preprocess it like we have before

In addition to our regular preprocessing, the image generator requires a "batch" dimension and a "channels" dimension. RGB images already have channel dimensions, but a grayscale image must have it added.

We could also easily apply these augmentations to the entire training set to make our model more robust; to do this, we will not display all the images. Displaying the images is nice to verify your work, but it is not a necessary step.

First, we import our preprocessed data and split it into training and testing sets

Next we build our data generator; make sure to select only parameters that will work with our data. An image of a face might appear facing either left or right, but in our data it would never appear upside down, so flipping the images vertically would not make sense!

Now we can loop through the entire training set and add 5 new images based on every original image. Note that we only do this for the training dataset; this is to help our model learn to identify the original images correctly! Adding "fake" images to the testing set would skew our results.

Before we augment an image, we must preprocess it like we have before

In addition to our regular preprocessing, the image generator requires a "batch" dimension and a "channels" dimension. RGB images already have channel dimensions, but a grayscale image must have it added.

Before we augment an image, we must preprocess it like we have before

In addition to our regular preprocessing, the image generator requires a "batch" dimension and a "channels" dimension. RGB images already have channel dimensions, but a grayscale image must have it added.

# Part 1: Importing Data

## Part 2: Preprocessing

# Part 3: Labels

# Part 4: Augmentation

# Part 5: Creating the Model

# Part 1: Importing Data

## Part 2: Preprocessing

# Part 3: Labels

# Part 4: Augmentation

# Part 5: Creating the Model

To fit the model to the data, we specify X_train as normal, but pass a dictionary for the y_data.

We have two choices; should the y columns of quality and color all be predicted in a single layer? It might be easier to separate these into two layers, both having the sigmoid activation function. To start, lets preprocess these layers using labelencoder and onehotencoder.

Now that the data is processed, start by creating the shared portion of the model. We start with the input layer, but note that because we are note using the "sequential" model from keras, we must specify where each new layer attaches to the model. We do this by placing the name of the preceding layer in parentheses at the end of the line creating the new layer.

Note how shared_layer1 is created as a Dense layer, and then (input_layer) specifies that shared_layer1 will come directly after the input_layer. In the following line, shared_layer2 is created to follow shared_layer1.

So far, we've created a sequential set on layers, one following the other. Now we will create two branches for our two output layers. To do this, we create two layers and specify the same "preceding layer" for each. Note how both the quality_output layer and the color_output layer connect to the model via shared_layer2.

When creating these layers, we determine that sigmoid is best for quality and for color. That said, there are arguments to be made for either!

Now we can pull the model together. We only need to specify the input and output layers and Keras will infer the rest. To compile the model, we can specify unique metrics and loss functions for each output layer, but for this dataset we have chosen 'binary_crossentropy' as the loss function and 'accuracy' as the metric for both output layers.

To fit the model to the data, we specify X_train as normal, but pass a dictionary for the y_data.

# 1. Import the Data
Import the individual image files; this will take some time!

# 2. Preprocessing Images
Resize, convert to floating point, and normalize. If you'd like a challenge, research mean subtraction or standard deviation scaling!

Lets look at our filename data and prepare it for the model.

Now that we've split our y data into separate columns, lets preprocess each y column and determine how many output layers to create.

# 4. Augmenting the Image Files
When augmenting, it is important to think through whether any particular augmentation will invalidate a label. For instance, in this dataset, the "pose" label is dependent on the direction the subject is facing. If we flip an image horizontally, the "pose" label may be incorrect on the augmented file! Lets choose carefully which augmentations we can apply to this set.

# 5. Preparing y Data for Output Layers
Now that we have all the y data formatted correctly, we need to divide it back into sets of columns that can be predicted by a single layer. For instance, all the userid columns should be together. It is perfectly reasonable to gather all onehotencoded outputs for the original columns back together and use 'sigmoid' as the activation function for all the output layers. This will result in one y variable for "userid", one for "expression", one for "pose" and one for "eyes".

Could any of these be combined? Would there be an advantage to using softmax instead of sigmoid somewhere?

Specifically for the userid column, what if we later use this model to predict an image of a new person? With sigmoid, all predictions would be forced to add to 1 even if the model was certain that the new image didn't belong to any of the original faces. With softmax, our model would be allowed to show a confidence near zero for every userid.

# 7. Export Pickle Files
At this stage, the data has been manipulated and cleaned an extensive amount. We're happy with how the data looks, so this is a good opportunity to create a "checkpoint". Lets save all our data variables in a pickle file so we don't have to repeat our preprocessing the next time we try to work with our model!

# 1. Import the Data
Import the individual image files; this will take some time!

# 2. Preprocessing Images
Resize, convert to floating point, and normalize. If you'd like a challenge, research mean subtraction or standard deviation scaling!

Lets look at our filename data and prepare it for the model.

Now that we've split our y data into separate columns, lets preprocess each y column and determine how many output layers to create.

# 4. Augmenting the Image Files
When augmenting, it is important to think through whether any particular augmentation will invalidate a label. For instance, in this dataset, the "pose" label is dependent on the direction the subject is facing. If we flip an image horizontally, the "pose" label may be incorrect on the augmented file! Lets choose carefully which augmentations we can apply to this set.

# 5. Preparing y Data for Output Layers
Now that we have all the y data formatted correctly, we need to divide it back into sets of columns that can be predicted by a single layer. For instance, all the userid columns should be together. It is perfectly reasonable to gather all onehotencoded outputs for the original columns back together and use 'sigmoid' as the activation function for all the output layers. This will result in one y variable for "userid", one for "expression", one for "pose" and one for "eyes".

Could any of these be combined? Would there be an advantage to using softmax instead of sigmoid somewhere?

Specifically for the userid column, what if we later use this model to predict an image of a new person? With sigmoid, all predictions would be forced to add to 1 even if the model was certain that the new image didn't belong to any of the original faces. With softmax, our model would be allowed to show a confidence near zero for every userid.

# 7. Export Pickle Files
At this stage, the data has been manipulated and cleaned an extensive amount. We're happy with how the data looks, so this is a good opportunity to create a "checkpoint". Lets save all our data variables in a pickle file so we don't have to repeat our preprocessing the next time we try to work with our model!

# 1. Import the Preprocessed Data
Import the pickle file with the preprocessed images.

# 2. Build the Model

# 1. Import the Preprocessed Data
Import the pickle file with the preprocessed images.

# 2. Build the Model

## The NLTK Reuters corpus

## Tokenizing with Python `split()`

## NLTK tokenization

**Question: What differences are there between using Python to split the sentence and tokenizer functions?**
- Python keeps the escape characeter (\n)
- NLTK tokenizer doesn't keep the escape character and period, and splits the parentheses around "CPA". 

## The NLTK Reuters corpus

## Tokenizing with Python `split()`

## NLTK tokenization

**Question: What differences are there between using Python to split the sentence and tokenizer functions?**
- Python: 
- NLTK tokenizer:  

## NLTK Stopwords

## Getting Rid of Non-Alpha Characters using Regular Expressions

## NLTK Stopwords

## Getting Rid of Non-Alpha Characters using Regular Expressions

# Lemmatization

# Stemming

## Porter Stemmer

## Snowball Stemmer

# Lemmatization

# Stemming

## Porter Stemmer

## Snowball Stemmer

# N-Gram Counter

## Frequency Analysis: Word Counts

## Frequency Analysis: N-gram Counts

# N-Gram Counter

## Frequency Analysis: Word Counts

## Frequency Analysis: N-gram Counts

## Count the occurrence of each word in the text.

## Calculate the TF-IDF score from a Corpus of Documents.

## Count the occurrence of each word in the text.

## Calculate the TF-IDF score from a Corpus of Documents.

## Get all the Articles About Money

## Calculate the TF-IDF Weights

## How many documents contains a specific word or group of words?

 ### Question 1: How many articles talk about Yen?

### Question 2: How many articles talk about Japan or Banks?

 ### Question 3: How many articles talk about England or Dealers?

## Get all the Articles About Money

## Calculate the TF-IDF Weights

## How many documents contains a specific word or group of words?

 ### Question 1: How many articles talk about Yen?

### Question 2: How many articles talk about Japan or Banks?

 ### Question 3: How many articles talk about England or Dealers?

## Split the data into train & test sets:

## For efficiency,  build a Pipeline with the vectorizer and SVM model. 

## Test the classifier and display results

Using the text of the messages, our model performed exceedingly well; it correctly predicted spam **98.9%** of the time!<br>
Now let's apply what we've learned to a text classification project involving positive and negative movie reviews.

## Split the data into train & test sets:

## For efficiency,  build a Pipeline with the vectorizer and SVM model. 

## Test the classifier and display results

Using the text of the messages, our model performed exceedingly well; it correctly predicted spam **98.9%** of the time!<br>
Now let's apply what we've learned to a text classification project involving positive and negative movie reviews.

## Split the data into train & test sets:

## For efficiency,  build a Pipeline with the vectorizer and SVM model. 

## Test the classifier and display results

## Split the data into training & testing data sets.

## Run predictions and analyze the results.

### Feed a review into the model's `predict()` method

## Repeat the analysis with the `english` stopwords. 

Now let's repeat the process above and see if the removal of stopwords improves or impairs our score.

Our score didn't change that much. We went from 74.2 % without filtering stopwords to 75.6% after adding a stopword filter to our pipeline. Keep in mind that 748 movie reviews is a relatively small dataset. The real gain from stripping stopwords is improved processing speed; depending on the size of the corpus, it might save hours.

### Feed the previous review into the model's `predict()` method.

**Question:** Did the review change? 

**Answer:** No.

**Question:** If so, why do you think it changed? 

## Repeat the analysis using the following custom stopwords. 

**Question:** Did the review change? 

**Answer:** Yes.

**Question:** If so, why do you think it changed? 

**Answer:** There are many words in the stopword list that may influence the classification of movie reviews.Using a custom or domain specific custom stopword list can help improve the algorithm.

## Split the data into train & test sets:

## Run predictions and analyze the results.

### Feed a review into the model's `predict()` method

## Repeat the analysis with the `english` stopwords. 

Now let's repeat the process above and see if the removal of stopwords improves or impairs our score.

### Feed the previous review into the model's `predict()` method.

**Question:** Did the review change? 

**Answer:**

**Question:** If so, why do you think it changed? 

## Repeat the analysis using the following custom stopwords. 

**Question:** Did the review change? 

**Answer:** 

**Question:** If so, why do you think it changed? 

**Answer:** 

##  Retrieve the documents IDs and text of the U.S. presidential inaugural addresses

## The Most Frequent Adjectives from each Inaugural Address

## Most Common Adjectives Used in Inaugural Addresses

## Analyze Adjectives Over Time

## The Most Common Adjectives Describing America

##  Retrieve the documents IDs and text of the U.S. presidential inaugural addresses

## The Most Frequent Adjectives from each Inaugural Address

## Most Common Adjectives Used in Inaugural Addresses

## Analyze Adjectives Over Time

## The Most Common Adjectives Describing America

## Preprocess the Text

## Process the Text to Tokens and Counts.

## LDA

## Using `argsort()`
---
- `argsort()` returns index positions from least to greatest.

### Taking our best guess at the topics.
---
- TOPIC 1: **Travel**
- TOPIC 2: **Sports**
- TOPIC 3: **Food**
- TOPIC 4: **Politics**
- TOPIC 5: **Business**
- TOPIC 6: **Entertainment**
- TOPIC 7: **Technology**

### Assigning the Topic to the Headline

This means that our model thinks that the first article belongs to topic "2".

## Preprocess the Text

## Process the Text to Tokens and Counts.

## LDA

## Using `argsort()`
---
- `argsort()` returns index positions from least to greatest.

### Taking our best guess at the topics.
---
- TOPIC 1: **Travel**
- TOPIC 2: **Sports**
- TOPIC 3: **Food**
- TOPIC 4: **Politics**
- TOPIC 5: **Business**
- TOPIC 6: **Entertainment**
- TOPIC 7: **Technology**

### Assigning the Topic to the Headline

This means that our model thinks that the first article belongs to topic "2".

## Preprocessing

## Process the Text to Tokens and Counts.

## LDA

## Get the Top 15 Words Per Topic

### **Question:** What is the label for each topic? 
---
- TOPIC 1: Entertainment
- TOPIC 2: Sports
- TOPIC 3: Business
- TOPIC 4: Politics
- TOPIC 5: Technology

## Assign the Topics and Labels to the News Summaries

**Question:** Did LDA do a good job at assigning the appropriate topic to the news summaries? 

**Answer:** Yes. Most of the news summaries look like they have been appropriately assigned the correct topic and topic label.

## Preprocessing

## Process the Text to Tokens and Counts.

## LDA

## What are the Top 15 Words Per Topic.

### **Question:** What is the label for each topic? 
---
- TOPIC 1: 
- TOPIC 2:
- TOPIC 3: 
- TOPIC 4: 
- TOPIC 5: 

## Assign the Topics to the News Summaries

**Question:** Did LDA do a good job at assigning the appropriate topic to the news summaries? 

**Answer:**

## Preprocess the Text

## Create a TF-IDF matrix from our documents.

## Applying NMF

### Taking our best guess at the topics.
---
- TOPIC 1: **Entertainment**
- TOPIC 2: **Technology**
- TOPIC 3: **Food and Drink**
- TOPIC 4: **Politics**
- TOPIC 5: **Business**
- TOPIC 6: **Sports**
- TOPIC 7: **Travel**

## Assigning the Topic to the Headline

## Preprocess the Text

## Create a TF-IDF matrix from our documents.

## Applying NMF

### Taking our best guess at the topics.
---
- TOPIC 1: 
- TOPIC 2: 
- TOPIC 3: 
- TOPIC 4: 
- TOPIC 5: 
- TOPIC 6: 
- TOPIC 7: 

## Assigning the Topic to the Headline

## Preprocessing

## Create a TF-IDF matrix from our documents.

## Applying NMF

## Get the Top 15 Words Per Topic

### **Question:** What is the label for each topic? 
---
- TOPIC 1: Business
- TOPIC 2: Entertainment
- TOPIC 3: Politics
- TOPIC 4: Sports
- TOPIC 5: Technology

## Assign the Topics and Labels to the News Summaries

## Preprocessing

## Create a TF-IDF matrix from our documents.

## Applying NMF

## Get the Top 15 Words Per Topic

### **Question:** What is the label for each topic? 
---
- TOPIC 1: 
- TOPIC 2: 
- TOPIC 3: 
- TOPIC 4: 
- TOPIC 5: 

## Assign the Topics and Labels to the News Summaries

## Tokenize and Clean Text

The next step is to determine how long of token sequence we want to predict the next token. 
This also affects our model's accuracy. 
Hiakus are three token sequences and lyrics are seven token sequences. 
We want to create 25 token sequences and then predict the next token, number 26.

## Create Sequences of Tokens

## Perform Tokenization with Keras

## Convert the List of Sequences to Arrays.

## Create input sequences and one-hot encode the target variable.

## Creating a LSTM  Model

## Training the Model

## Generating New Text

## Test: Grab a random seed sequence

- **The next 25 words aren't that accurate.**

## Explore Generating Text

## Check the fist four chapters of Moby Dick to determine the accuracy of the text.
---
"Seeing, now, that there were no curtains to the window, and that the
street being very narrow, the house opposite commanded a plain view
into the room, and <font color='blue'>observing more and more the indecorous figure that
Queequeg made, staving about with little else but his hat and boots
on; I begged him as well as I could, to accelerate his toilet
somewhat, and particularly to get into his pantaloons as soon as
possible.</font>"

**Question: How would we gain better accuracy for the next 50 words?**

- Increase or decrease the length of the sequence? 
- Decrease the batch size? 

## Tokenize and Clean Text

## Create Sequences of Tokens

## Perform Tokenization with Keras

## Convert the List of Sequences to Arrays.

## Create input sequences and one-hot encode the target variable.

## Creating a LSTM  Model

## Training the Model

## Generating New Text

## Test: Grab a random seed sequence

- **The next 25 words aren't that accurate.**

## Explore Generating Text

**Question: How would we gain better accuracy for the next 50 words?**

- Increase or decrease the length of the sequence? 
- Decrease the batch size? 

## Tokenize and Clean Text

## Create Sequences of Tokens

## Perform Tokenization with Keras

## Convert the List of Sequences to Arrays.

## Create input sequences and one-hot encode the target variable.

## Creating a LSTM  Model

## Training the Model

## Generating New Text

## Tokenize and Clean Text

## Create Sequences of Tokens

## Perform Tokenization with Keras

## Convert the List of Sequences to Arrays.

## Create input sequences and one-hot encode the target variable.

## Creating a LSTM  Model

## Training the Model

## Generating New Text

### Using scikit-learn's `CountVectorizer` demonstrate how a BoW is created.

### Use scikit-learn's `CountVectorizer` demonstrate how a BoW is created.

## NLTK tokenization

## Tokenizing using spaCy

## Tokenize the first sentence using bert-base-uncased.

## NLTK tokenization

## Tokenizing using spaCy

## Tokenize the first sentence using bert-base-uncased.

**Question:** What category is the new headline? 

**Answer:** "Travel".

**Question:** Why did you choose this category?

**Answer:** The headline "Top 10 Hacks for Traveling Like a Pro" is most similar to a "Technology" headline, however we should classify it as "Travel" since the second and third best similarity scores are "Travel". 

**Question:** What category is the new headline?

**Answer:**  

**Question:** Why did you choose this category?

**Answer:** 

## Preprocess the Data and Get Vector Embeddings.

## Get the Top Five Cosine Similarities For Each Unclassified Text Message

---

1. Calculate the cosine similarities between each unclassified message and all unclassified messages.
2. Sort the cosine similarities.
3. Get the top 5 similarities for each unclassified message.

## Determine the Classification of Each Unclassified Text Message.
---

- Print out each unclassified message and the top 5 similar unclassified messages along with their labels and similarity scores.

### Answer the following questions.
---
**Question 1:** Did the similarity scores for the "unclassified" text messages agree with the label given in the CSV file? Why or why not? 

**Answer 1:** For a majority of the "unclassified" text messages the top similarity scores were correct. There were a few "unclassified" text messages that might be mislabeled as "ham" instead of "spam" using this method.

**Question 2:** What other method would you use to confirm the classification of the text messages?

**Possible Answer 2:** Use more spam data in order to get better similarities.

**Possible Answer 2:** Use a LinearSVC unsupervised learning model to train the 100 text messages on the model and use the model to predict the classification of the "unclassified" text messages.

## Preprocess the Data and Get Vector Embeddings.

## Get the Top Five Cosine Similarities For Each Unclassified Text Message

---

1. Calculate the cosine similarities between each unclassified message and all classified messages.
2. Sort the cosine similarities.
3. Get the top 5 similarities for each unclassified message.

## Determine the Classification of Each Unclassified Text Message.
---

- Print out each unclassified message and the top 5 similar unclassified messages along with their labels and similarity scores.

### Answer the following questions.
---
**Question 1:** Did the similarity scores for the "unclassified" text messages agree with the label given in the CSV file? Why or why not? 

**Answer 1:** 

**Question 2:** What other method would you use to confirm the classification of the text messages?

**Answer 2:** 

### Use Transformers `AutoTokenizer` and `TFAutoModelForSeq2SeqLM` to Translate Each News Headline.

### Use the Transformer Pipeline to Translate Each Headline.

### Use Transformers `AutoTokenizer` and `TFAutoModelForSeq2SeqLM` to Translate Each News Headline.

### Use the Transformer Pipeline to Translate Each Headline.

**Question:** What was the best model? Why?

**Answer:** The best model was "EleutherAI/gpt-neo-2.7B" because it gave the answer that made the most sense.

**Question:** What was the best model? Why?

**Answer:**

### Modify the code so we can interact with the application.

### Modify the code so we can interact with the application.

**The text to paste:**

Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.[2]
s
Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.[3][4][5]

Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog.[6][7]

The adjective "deep" in deep learning refers to the use of multiple layers in the network. Early work showed that a linear perceptron cannot be a universal classifier, but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can. Deep learning is a modern variation that is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability.

**The text to paste:**

Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.[2]
s
Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.[3][4][5]

Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog.[6][7]

The adjective "deep" in deep learning refers to the use of multiple layers in the network. Early work showed that a linear perceptron cannot be a universal classifier, but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can. Deep learning is a modern variation that is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability.

**The text to paste:**

A transformer is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data. It is used primarily in the fields of natural language processing (NLP)[1] and computer vision (CV).[2]

Like recurrent neural networks (RNNs), transformers are designed to process sequential input data, such as natural language, with applications towards tasks such as translation and text summarization. However, unlike RNNs, transformers process the entire input all at once. The attention mechanism provides context for any position in the input sequence. For example, if the input data is a natural language sentence, the transformer does not have to process one word at a time. This allows for more parallelization than RNNs and therefore reduces training times.[1]

Transformers were introduced in 2017 by a team at Google Brain[1] and are increasingly becoming the model of choice for NLP problems,[3] replacing RNN models such as long short-term memory (LSTM). The additional training parallelization allows training on larger datasets. This led to the development of pretrained systems such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), which were trained with large language datasets, such as the Wikipedia Corpus and Common Crawl, and can be fine-tuned for specific tasks.[4][5]

**Questions to ask.**
1. Who introduced transformers?
2. Why is parallelization important?
3. What pretrained systems were developed from parallelization?

